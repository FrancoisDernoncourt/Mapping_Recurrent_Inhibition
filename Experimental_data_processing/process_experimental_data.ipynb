{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies that are not included in the environment\n",
    "# Please make sur to have selected the right Python environment/kernel (mapping_RI_env) before running these commands\n",
    "%pip install factor_analyzer\n",
    "%pip install seaborn\n",
    "%pip install PyWavelets\n",
    "%pip install cmasher\n",
    "%pip install fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import h5py\n",
    "from h5py import string_dtype\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy.io import loadmat\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, windows\n",
    "from scipy.signal import filtfilt, windows, detrend\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.signal import csd, detrend\n",
    "import pickle\n",
    "from scipy.signal import welch\n",
    "from scipy.fft import fft, fftfreq\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import re\n",
    "import gc  # Import Python's garbage collector\n",
    "import itertools\n",
    "from scipy.optimize import least_squares\n",
    "import csv\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stats\n",
    "import matplotlib as gridspec\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from IPython.display import display\n",
    "from dataclasses import dataclass\n",
    "import numpy.ma as ma\n",
    "import pywt\n",
    "from scipy.signal import butter, sosfiltfilt, windows\n",
    "from sklearn.decomposition import NMF\n",
    "import cmasher as cmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*LOKY_MAX_CPU_COUNT.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*force_all_finite.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"No rotation will be performed when the number of factors equals 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_files = \"C://Users//franc//Documents//Mapping_Recurrent_Inhibition_minimal_data_to_run_scripts//one_example_experimental_MUedit_file//\"\n",
    "# ^ Replace this path with the path to the folder containing the .mat files to analyze.\n",
    "# Running it on all files in the folder is quite long, so do not hesitate to create a subfolder with only one or two files to test the script first.\n",
    "export_processed_spike_trains_as_hdf5_file_for_new_analysis = True\n",
    "\n",
    "files_to_analyze_experimental_data = [f for f in os.listdir(path_of_files) if f.endswith('.mat')]\n",
    "input_fsamp = 2048 # for experimental data\n",
    "# Assuming grids of 13x5\n",
    "# For nb of grids, check parameters a bit below\n",
    "ISI_thresh_for_discontinuity = 1.0 # 0.4 # in s\n",
    "single_or_double_diff_EMG = 'single' # 'single' or 'double' ; returns monopolar EMG if anything else\n",
    "display_figures_inline = True # If false, no figure will be displayed when running the script.\n",
    "# Seeting display_figures_inline = True is recommended if \"select_plateaus_manually = True\" because when this variable is set to True, all figures appear in new windows (which is quickly overwhelming)\n",
    "\n",
    "# Force/torque processing and plateau selection parameters\n",
    "force_is_3D = False # set to \"True\" to recover all force components (when recorded with a 3D force sensor)\n",
    "select_plateaus_manually = False # Select plateaus manually (or not)\n",
    "force_lowpass_cutoff = 5 # 100 # Hz # 0.33\n",
    "force_differential_epsilon_up_or_down = 0.0001 # 0.0001 (not used if select_plateaus_manually = True)\n",
    "if select_plateaus_manually:\n",
    "    %matplotlib qt\n",
    "    # Use an interactive backend\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    # Use an inline backend\n",
    "\n",
    "# Initialize a dictionary with lists as default values\n",
    "conditions_per_subject = defaultdict(list)\n",
    "\n",
    "# Process filenames\n",
    "for filename in files_to_analyze_experimental_data:\n",
    "    subject, condition = filename.replace('.mat', '').split('_', 1)  # Split subject and condition\n",
    "    conditions_per_subject[subject].append(condition)\n",
    "\n",
    "# Convert defaultdict to a regular dictionary\n",
    "conditions_per_subject = dict(conditions_per_subject)\n",
    "\n",
    "# # # # CHOOSE NUMBER OF MUs TO RANDOMLY SELECT FOR ANALYSES IN THE CASE OF SIMULATED DATA\n",
    "select_all_MNs_from_simulation = True # if False, a random sample of MUs will be selected\n",
    "sim_nb_MUs_to_select_per_pool = 50 # Both continuous and/or discontinuous MUs may get selected\n",
    "sim_ignore_discontinuous_MUs = True # if True, only continuous MUs will be selected for the analyses\n",
    "\n",
    "# # # # # CHOOSE WHICH ANALYSES TO PERFORM # # # # # \n",
    "# # # # # All the analyses below correspond to analyses on experimental data that are NOT included in the paper - the main purpose of this script is to process experimental spike trains data and to create a usable .hd5 file for the \"analyzer.py\" script. They remain available here.\n",
    "perform_analysis_dimensionality = False # True\n",
    "perform_analysis_frequency_components_dimensionality = False\n",
    "perform_analysis_coherence = False # True\n",
    "perform_analysis_CST_PSD = False\n",
    "perform_analysis_CST_torque_correlation = False # False\n",
    "perform_analysis_recurrent_inhibition = False # True\n",
    "\n",
    "# # # # # PARAMETERS FOR FORCE PREPROCESSING # # # # #\n",
    "polynomial_degree_for_detrending = 1\n",
    "force_smoothing_window_length = 1 # in s\n",
    "\n",
    "# # # # # PARAMETERS FOR DIMENSIONALITY AND COH ANALYSIS # # # # # \n",
    "nb_plateaus_to_select = {}\n",
    "nb_plateaus_to_select['10percent'] = 10\n",
    "nb_plateaus_to_select['40percent'] = 20\n",
    "nb_plateaus_to_select['flexion'] = 5 # only used for the 'fingers\" file\n",
    "nb_plateaus_to_select['abduction'] = 5 # only used for the 'fingers\" file\n",
    "# Minimum proportion of MUs to keep in the plateau nb selection - if no optimal selection can select enough MUs, then one less plateau is selected and the process repeats\n",
    "min_proportion_of_MUs_to_keep = 0.5 # 0.33\n",
    "\n",
    "# --------------------------------------\n",
    "#       Factor analysis\n",
    "# --------------------------------------\n",
    "max_nb_of_factors = 6  # e.g., 6, but can be anything you need\n",
    "surrogate_iter_nb = 10 # 1\n",
    "# Setting the smoothing kernel for the smoothed discharge rates\n",
    "Wind_s = 0.4  # hanning window duration. 0.4 for 2.5hz low-pass, 0.2 for 5hz low-pass\n",
    "HanningW = 2 / round(input_fsamp * Wind_s) * windows.hann(round(input_fsamp * Wind_s))  # unitary area\n",
    "fa_on_first_pool_only_for_sim = True\n",
    "display_Rsquared_for_muscle_pair = False\n",
    "\n",
    "# --------------------------------------\n",
    "#       Coherence\n",
    "# --------------------------------------\n",
    "coh_window_length = 1 # in s\n",
    "coh_windows_overlap = 0.5 # % relative to coh_window_length\n",
    "# frequency resolution will be = 1/coh_window_length. If coh_window_length = 10s for instance, resolution will be 0.1hz\n",
    "upsampling_frequency_resolution = 1 # 5 # for the 'nfft' parameter of scipy.csd(). Interpolates the coherence values over the frequencies, to smooth the figure a bit.\n",
    "# For example, if the resolution defined by coh_window_length is 1hz, and if upsampling_frequency_resolution = 2, the resolution will be upsampled to 0.5hz\n",
    "max_freq = 150 # maximum frequency that is kept\n",
    "COH_calc_max_iteration_nb_per_group_size = 100 # More iteration for smaller group sizes, because the value obtained is very dependent upon the exact neurons selected, especially when only a few MNs are used to create the CST\n",
    "\n",
    "# --------------------------------------\n",
    "#       CST-torque correlation\n",
    "# --------------------------------------\n",
    "low_pass_freqs_to_try = [2.5] # [1, 2.5, 5, 10, 50]  # in Hz\n",
    "arbitrary_lowpass_freq = 2.5   # must be in low_pass_freqs_to_try\n",
    "edges_to_remove = (1/np.min(low_pass_freqs_to_try)) + 0.5  # seconds\n",
    "edges_to_remove_samples = int(edges_to_remove * input_fsamp)\n",
    "cross_correl_max_lag = int(0.6*input_fsamp)  # in samples - 0.6s is 300ms each side\n",
    "# Force low-pass filter cutoff (Hz)\n",
    "force_lowpass_cutoff = 2.5 # 2.5  \n",
    "# Display choice: 'all' or list of muscles you want to show\n",
    "CST_torque_corr_display_only_selected_muscles = 'all' # 'all' # ['VL']\n",
    "\n",
    "#\n",
    "# Experimental data parameters\n",
    "nb_grids_origin = {}\n",
    "# Make sure the names match with the filename conditions\n",
    "nb_grids_origin['Quadriceps'] = 6\n",
    "nb_grids_origin['TA'] = 4\n",
    "nb_grids_origin['TricepsSurae'] = 6\n",
    "nb_grids_origin['Fingers'] = 2\n",
    "nb_grids_origin['FDI'] = 1\n",
    "TA_grids = [0,1,2,3]\n",
    "TA_color = 'green'\n",
    "SOL_grids = [0,1]\n",
    "SOL_color = 'blue'\n",
    "GM_color = 'purple'\n",
    "GM_grids = [2,3,4,5]\n",
    "VM_grids = [0,1]\n",
    "VL_grids = [2,3,4,5]\n",
    "VM_color = 'orange'\n",
    "VL_color = 'red'\n",
    "FDI_grids = [0]\n",
    "APB_grids = [1]\n",
    "FDI_color = 'turquoise'\n",
    "APB_color = 'pink'\n",
    "NA_color = 'gray'\n",
    "\n",
    "# Define the colors for the grids\n",
    "grid_colormap = plt.colormaps.get_cmap('plasma')\n",
    "# Generate 6 evenly spaced values between 0 and 1\n",
    "n_colors = max(nb_grids_origin.values())\n",
    "indices = np.linspace(0, 1, n_colors)\n",
    "# Generate colors\n",
    "grid_colors = [grid_colormap(index) for index in indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def mix_colors(color1, color2):\n",
    "    \"\"\"\n",
    "    Mix two colors and return their average as a hex string.\n",
    "    \n",
    "    :param color1: First color (name or hex string).\n",
    "    :param color2: Second color (name or hex string).\n",
    "    :return: Mixed color as a hex string.\n",
    "    \"\"\"\n",
    "    # Convert colors to RGB tuples in [0,1]\n",
    "    rgb1 = mcolors.to_rgb(color1)\n",
    "    rgb2 = mcolors.to_rgb(color2)\n",
    "    \n",
    "    # Compute the average of the two RGB tuples.\n",
    "    mixed_rgb = [(c1 + c2) / 2 for c1, c2 in zip(rgb1, rgb2)]\n",
    "    \n",
    "    # Convert the averaged RGB back to a hex string.\n",
    "    return mcolors.to_hex(mixed_rgb)\n",
    "\n",
    "def modify_color(color, rgb_factor=0.5, rgb_constant=0.0):\n",
    "    if type(rgb_factor) == float:\n",
    "        rgb_factor = [rgb_factor] * 3\n",
    "    if type(rgb_factor) == float:\n",
    "        rgb_constant = [rgb_constant] * 3\n",
    "\n",
    "    # Convert the color to an RGB triplet (values between 0 and 1)\n",
    "    rgb = np.array(mcolors.to_rgb(color))  \n",
    "\n",
    "    # Apply the modification (e.g., divide by 2 to darken)\n",
    "    modified_rgb = np.clip(rgb * rgb_factor + rgb_constant, 0, 1)  # Ensure values stay between 0 and 1\n",
    "\n",
    "    # Convert back to a hex string\n",
    "    return mcolors.to_hex(modified_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recursively navigate and load data from the .mat file\n",
    "def load_file_h5py(file_path):\n",
    "    mat_data = {}\n",
    "\n",
    "    def resolve_reference(reference):\n",
    "        \"\"\"Helper function to resolve HDF5 references.\"\"\"\n",
    "        if isinstance(reference, h5py.Reference):\n",
    "            return f[reference][()]  # Dereference and load the actual data\n",
    "        return reference  # Otherwise, return the original data\n",
    "\n",
    "    def flatten_and_dereference(nested_array):\n",
    "        \"\"\"\n",
    "        Flatten any nested object arrays and resolve references.\n",
    "        Recursively dereference any references stored in the array.\n",
    "        Returns a list of lists instead of a NumPy array to avoid broadcasting errors.\n",
    "        \"\"\"\n",
    "        if isinstance(nested_array, np.ndarray) and nested_array.dtype == 'O':  # Check for object (cell array)\n",
    "            dereferenced = []\n",
    "            for item in nested_array:\n",
    "                if isinstance(item, h5py.Reference):\n",
    "                    dereferenced.append(resolve_reference(item))\n",
    "                elif isinstance(item, np.ndarray) and item.dtype == 'O':\n",
    "                    dereferenced.append(flatten_and_dereference(item))  # Recursively flatten/dereference\n",
    "                elif isinstance(item, np.ndarray):\n",
    "                    dereferenced.append(item)  # Keep normal NumPy arrays\n",
    "                else:\n",
    "                    dereferenced.append(item)\n",
    "            return dereferenced  # Return as a Python list, NOT a NumPy array\n",
    "        return nested_array\n",
    "\n",
    "    def process_spike_times(spike_times):\n",
    "        \"\"\"\n",
    "        Process spike_times and return a dictionary with grid numbers as keys and lists of arrays of spike times as values.\n",
    "        Also, ensure that empty arrays are represented as array([], dtype=float64).\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure spike_times is always a 2D structure (list of lists)\n",
    "        if isinstance(spike_times, np.ndarray) and spike_times.ndim == 1:  # If only 1 row (1xM), reshape to 2D\n",
    "            spike_times = spike_times.reshape(1, -1)\n",
    "\n",
    "        spike_dict = {}\n",
    "\n",
    "        for i in range(len(spike_times)):  # Iterate over grids\n",
    "            spike_dict[i] = []  # Initialize list for each grid\n",
    "            for j in range(len(spike_times[i])):  # Iterate over motor units\n",
    "                spike_array = spike_times[i][j]\n",
    "                if isinstance(spike_array, np.ndarray):\n",
    "                    # Replace array([0., 0.]) with empty array if it contains [0., 0.]\n",
    "                    if np.array_equal(spike_array, np.array([0., 0.])):\n",
    "                        spike_dict[i].append(np.array([], dtype=np.float64))\n",
    "                    else:\n",
    "                        # Convert to 1D array (flatten) and ensure dtype is float64\n",
    "                        spike_dict[i].append(np.ravel(spike_array).astype(np.float64))\n",
    "                else:\n",
    "                    # Ensure it's an empty array for invalid or empty data\n",
    "                    spike_dict[i].append(np.array([], dtype=np.float64))\n",
    "\n",
    "        return spike_dict\n",
    "    \n",
    "    def process_channels_mask(channels_mask):\n",
    "        \"\"\"\n",
    "        Process channels_mask to ensure it is an array of uint8 arrays (one per grid).\n",
    "        \"\"\"\n",
    "        mask_list = []\n",
    "\n",
    "        for i in range(len(channels_mask)):  # Iterate over grids\n",
    "            mask_array = channels_mask[i]\n",
    "\n",
    "            # Ensure the mask is treated as a 1D array and cast to uint8\n",
    "            if isinstance(mask_array, np.ndarray):\n",
    "                mask_array = mask_array.flatten().astype(np.uint8)\n",
    "            mask_list.append(mask_array)\n",
    "\n",
    "        return np.array(mask_list, dtype=object)  # Return as object array of uint8 arrays\n",
    "\n",
    "\n",
    "    # Open the .mat file and access the necessary fields\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Accessing 'signal' struct fields\n",
    "        signal = f['signal']\n",
    "        emg_data = signal['data'][()]  # Extract the full EMG data without slicing\n",
    "        force_data = signal['path'][()]      # Load force data\n",
    "        target_data = signal['target'][()]   # Load target data\n",
    "        channels_mask = signal['EMGmask'][()]  # Load EMG mask as ndarray\n",
    "\n",
    "        # Accessing 'edition' struct field for spike times\n",
    "        edition = f['edition']\n",
    "        spike_times = edition['Dischargetimes'][()]\n",
    "\n",
    "        # Flatten and dereference spike_times and channels_mask\n",
    "        spike_times = flatten_and_dereference(spike_times)\n",
    "\n",
    "        # Convert list of lists into a NumPy object array before transposing\n",
    "        spike_times = np.array(spike_times, dtype=object)\n",
    "        # Ensure spike_times is always 2D (fixes issue when only 1 grid is present)\n",
    "        if spike_times.ndim == 1:\n",
    "            spike_times = spike_times.reshape(1, -1)\n",
    "        # Now it's safe to transpose\n",
    "        spike_times = process_spike_times(spike_times.T)\n",
    "        \n",
    "        # Process channels_mask to ensure each grid has its own uint8 array\n",
    "        channels_mask = flatten_and_dereference(channels_mask)\n",
    "        channels_mask = process_channels_mask(channels_mask)\n",
    "\n",
    "        # Store the results in the dictionary for easy access\n",
    "        mat_data['EMG'] = emg_data.T\n",
    "\n",
    "        if mat_data['EMG'].shape[0] > (64 * nb_grids):\n",
    "            mat_data['force'] = mat_data['EMG'][(64 * nb_grids), :]\n",
    "            if force_is_3D:\n",
    "                # plt.figure()\n",
    "                # plt.title(f\"{current_subject} - {current_condition} - 3D force\")\n",
    "                mat_data['force_3D'] = np.array([\n",
    "                    mat_data['EMG'][(64 * nb_grids), :],\n",
    "                    mat_data['EMG'][(64 * nb_grids)+1, :],\n",
    "                    mat_data['EMG'][(64 * nb_grids)+2, :]])\n",
    "                # plt.plot(mat_data['force_3D'][0,:],label=\"Force Z\")\n",
    "                # plt.plot(mat_data['force_3D'][1,:],label=\"Force X\")\n",
    "                # plt.plot(mat_data['force_3D'][2,:],label=\"Force Y\")\n",
    "                # mat_data['force_3D_magnitude'] = np.sqrt(np.sum(np.array(mat_data['force_3D'])**2,axis=0))\n",
    "                # plt.plot(mat_data['force_3D_magnitude'],label=\"3D force magnitude\",color=\"k\")\n",
    "                # plt.xlabel(\"Time (samples)\")\n",
    "                # plt.ylabel(\"Force (raw values)\")\n",
    "                # plt.legend()\n",
    "                # if display_figures_inline:\n",
    "                #     plt.show()\n",
    "                # else:\n",
    "                #     plt.close()\n",
    "        else:\n",
    "            mat_data['force'] = np.squeeze(target_data)\n",
    "\n",
    "        # Check if force is reversed\n",
    "        mean_center_force = np.mean(mat_data['force'])\n",
    "        if np.abs(np.min(mean_center_force)) > np.abs(np.max(mean_center_force)):\n",
    "            mat_data['force'] = -mat_data['force']\n",
    "\n",
    "        mat_data['target'] = np.squeeze(target_data)\n",
    "        mat_data['EMG'] = mat_data['EMG'][np.arange((64 * nb_grids)), :]  # Extracting EMG data (first (64*nb_grids) rows)\n",
    "        mat_data['channels_mask'] = channels_mask  # Store channels_mask as object array of uint8 arrays\n",
    "        mat_data['spike_times'] = spike_times  # Store spike_times as a dict\n",
    "\n",
    "    # Clean up large variables to free memory\n",
    "    gc.collect()\n",
    "\n",
    "    return mat_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine whether to use h5py or scipy based on the file format\n",
    "def load_file(file_path):\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "    try:\n",
    "        print(\"Trying to load as an HDF5 format\")\n",
    "        # Try to open as an HDF5-based v7.3 file\n",
    "        return load_file_h5py(file_path)\n",
    "    except OSError as e:\n",
    "        print(\"Error loading file as HDF5 format\")\n",
    "        print(f\"OSError: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_emg(emg_data, channels_mask):\n",
    "    \"\"\"\n",
    "    Reorganizes the EMG numpy array to reflect the spatial organization of electrodes, sets certain channels to NaN,\n",
    "    and computes the differential EMG.\n",
    "\n",
    "    Parameters:\n",
    "    emg_data (np.ndarray): The EMG data array with shape ((64*nb_grids), N) where (64*nb_grids) is the number of channels.\n",
    "    channels_mask (np.ndarray): A mask indicating which channels should be set to NaN (1 = NaN).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 4D array of shape (N, 13-M, 5, N-1) representing the processed EMG for N grids, with 13-1 rows for differential EMG, or 13-2 rows for double differential EMG.\n",
    "    \"\"\"\n",
    "    # Get the number of samples from the EMG data\n",
    "    num_samples = emg_data.shape[1]  # N samples\n",
    "    \n",
    "    # Initialize a 4D array to store the reorganized data: (6 grids, 13 rows, 5 columns, N samples)\n",
    "    num_grids = nb_grids\n",
    "    grid_rows = 13\n",
    "    grid_cols = 5\n",
    "    \n",
    "    # Output array to store the reorganized data\n",
    "    reorganized_emg = np.zeros((num_grids, grid_rows, grid_cols, num_samples))\n",
    "    \n",
    "    # Define the channel mapping for one grid based on the raster scan pattern\n",
    "    channel_map = []\n",
    "    for col in range(grid_cols):\n",
    "        if col % 2 == 0:\n",
    "            # Even columns: top to bottom (excluding the first row, which is missing)\n",
    "            channel_map.extend([(row, col) for row in range(grid_rows-1, 0, -1)])  # Rows 12 -> 1\n",
    "        else:\n",
    "            # Odd columns: bottom to top (excluding the first row, which is missing)\n",
    "            channel_map.extend([(row, col) for row in range(1, grid_rows)])  # Rows 1 -> 12\n",
    "\n",
    "    # Ensure channels_mask is a proper NumPy array\n",
    "    channels_mask = np.array(channels_mask, dtype=object)\n",
    "    \n",
    "    # Reorganize data for each grid and apply the channel_mask\n",
    "    for grid in range(num_grids):\n",
    "        for channel, (row, col) in enumerate(channel_map):\n",
    "            # Place the EMG data into the correct location in the 4D array\n",
    "            channel_data = emg_data[grid * 64 + channel, :]\n",
    "            \n",
    "            mask_array = np.array(channels_mask[grid]).flatten()  # Ensure it's a 1D array\n",
    "\n",
    "            if channel < len(mask_array):  # Prevent index out of bounds\n",
    "                mask_value = mask_array[channel]  \n",
    "            else:\n",
    "                mask_value = 0  # Default to 0 (not masked) if out of bounds\n",
    "\n",
    "            if np.isscalar(mask_value):  # Ensure it's a scalar\n",
    "                mask_value = int(mask_value)  # Convert to integer if necessary\n",
    "            else:\n",
    "                mask_value = int(mask_value[0])  # Extract scalar value if it's an array\n",
    "\n",
    "            if mask_value == 1:\n",
    "                reorganized_emg[grid, row, col, :] = np.nan\n",
    "            else:\n",
    "                reorganized_emg[grid, row, col, :] = channel_data\n",
    "\n",
    "        # Set the missing first channel in each grid to NaN\n",
    "        reorganized_emg[grid, 0, 0, :] = np.nan\n",
    "\n",
    "    # Compute differential EMG (difference along the columns, reducing the number of rows by 1)\n",
    "    if single_or_double_diff_EMG == 'single':\n",
    "        differential_emg = np.diff(reorganized_emg, axis=1)\n",
    "    elif single_or_double_diff_EMG == 'double':\n",
    "        differential_emg = np.diff(reorganized_emg, axis=1)\n",
    "        differential_emg = np.diff(differential_emg, axis=1)\n",
    "        # use double differential EMG ==== commented out means we are using single differential only\n",
    "        # Apply the Laplacian kernel along the rows (axis=1), equivalent to using diff() twice\n",
    "        # double_diff_kernel = np.array([1, -2, 1])\n",
    "        # differential_emg = convolve1d(reorganized_emg, double_diff_kernel, axis=1, mode='constant', cval=np.nan)\n",
    "    else:\n",
    "        print(\"'single_or_double_diff_EMG' is not set to 'single', nor 'double': returning monopolar EMG\")\n",
    "        differential_emg = reorganized_emg\n",
    "        \n",
    "    return differential_emg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realign_spike_times(emg_data, spike_times, sampling_rate=2048, display_muap=False, filename = 'unknown file'):\n",
    "    \"\"\"\n",
    "    Estimate the MUAP onset time using spike-triggered averaging and realign spike times.\n",
    "    \n",
    "    Parameters:\n",
    "    emg_data (np.ndarray): The reorganized EMG array of shape (6, 11, 5, N).\n",
    "    spike_times (dict): A dictionary with keys corresponding to grid numbers and motor unit numbers,\n",
    "                        containing arrays of spike times.\n",
    "    sampling_rate (int): The sampling rate of the EMG data (default: 2048 Hz).\n",
    "    display_muap (bool): Whether to display the MUAP window, spike times, threshold, and selected channel.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary of realigned spike times.\n",
    "    \"\"\"\n",
    "    # Spike-triggered averaging window\n",
    "    pre_spike_window = int(40 / 1000 * sampling_rate)  # 40 ms in samples\n",
    "    post_spike_window = int(40 / 1000 * sampling_rate)  # 40 ms in samples (after spike)\n",
    "\n",
    "    realigned_spike_times = {}\n",
    "    \n",
    "    # Colors for plotting based on grid index\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "    if single_or_double_diff_EMG == 'double':\n",
    "        nb_of_rows_temp = 11\n",
    "    elif single_or_double_diff_EMG == 'single':\n",
    "        nb_of_rows_temp = 12\n",
    "    else:\n",
    "        nb_of_rows_temp = 13\n",
    "\n",
    "    for grid in data[current_subject][current_condition]['spike_times'].keys():\n",
    "        realigned_spike_times[grid] = []\n",
    "        \n",
    "        for mu_idx, spikes in enumerate(spike_times[grid]):\n",
    "            # Initialize spike-triggered average\n",
    "            spike_triggered_avg = np.zeros((nb_of_rows_temp, 5, pre_spike_window + post_spike_window))\n",
    "\n",
    "            # Count valid spikes\n",
    "            valid_spike_count = 0\n",
    "            \n",
    "            # Perform spike-triggered averaging\n",
    "            for spike in spikes:\n",
    "                spike_idx = int(spike)  # Convert spike time to integer index\n",
    "                if spike_idx - pre_spike_window >= 0 and spike_idx + post_spike_window < emg_data.shape[-1]:\n",
    "                    spike_window = emg_data[grid, :, :, spike_idx - pre_spike_window:spike_idx + post_spike_window]\n",
    "                    spike_triggered_avg += spike_window\n",
    "                    valid_spike_count += 1\n",
    "\n",
    "            if valid_spike_count > 0:\n",
    "                spike_triggered_avg /= valid_spike_count  # Average the signal\n",
    "\n",
    "                # Find the best channel based on peak-to-peak amplitude ratio\n",
    "                best_channel_row, best_channel_col = None, None\n",
    "                max_ratio = -np.inf\n",
    "\n",
    "                for row in range(nb_of_rows_temp):\n",
    "                    for col in range(5):\n",
    "                        # Compute peak-to-peak amplitude for this channel\n",
    "                        signal = spike_triggered_avg[row, col, :]\n",
    "                        peak_to_peak = np.ptp(signal)\n",
    "\n",
    "                        # Compute noise level in the [40, 20] ms window before the peak and [20, 40] ms after the peak\n",
    "                        noise_window = signal[0:int(pre_spike_window * 0.5)]  # 40 to 20 ms before spike\n",
    "                        noise_window = np.append(noise_window, signal[int(pre_spike_window + post_spike_window * 0.5):])  # 20 to 40 ms after spike\n",
    "                        noise_std = np.std(noise_window)\n",
    "                        noise_mean = np.mean(noise_window)\n",
    "\n",
    "                        # Ratio of signal amplitude to noise\n",
    "                        if noise_std > 0:\n",
    "                            ratio = peak_to_peak / noise_std\n",
    "                        else:\n",
    "                            ratio = 0  # Avoid division by zero\n",
    "\n",
    "                        # Track the best channel (largest signal-to-noise ratio)\n",
    "                        if ratio > max_ratio:\n",
    "                            max_ratio = ratio\n",
    "                            best_channel_row, best_channel_col = row, col\n",
    "\n",
    "                # Detect the onset of the MUAP for the best channel\n",
    "                best_channel_signal = spike_triggered_avg[best_channel_row, best_channel_col, :]\n",
    "                noise_window = best_channel_signal[0:int(pre_spike_window * 0.5)]  # 40 to 20 ms before spike and [20, 40] ms after the spike\n",
    "                noise_window = np.append(noise_window, best_channel_signal[int(pre_spike_window + post_spike_window * 0.5):])  # 20 to 40 ms after spike\n",
    "                basal_mean = np.mean(noise_window)\n",
    "                basal_std = np.std(noise_window)\n",
    "\n",
    "                # Threshold for MUAP onset (6 times the standard deviation above the mean basal activity)\n",
    "                threshold_high = basal_mean + 6 * basal_std\n",
    "                threshold_low = basal_mean - 6 * basal_std\n",
    "\n",
    "                # Find the first time point where the MUAP crosses the threshold\n",
    "                onset_idx_high = np.argmax(best_channel_signal > threshold_high)\n",
    "                onset_idx_low = np.argmax(best_channel_signal < threshold_low)\n",
    "                onset_idx = np.min([onset_idx_high, onset_idx_low])  # Take the first occurrence of either condition\n",
    "\n",
    "                # Adjust spike times by the onset time\n",
    "                realigned_spikes = spikes - (pre_spike_window - onset_idx)  # Shift spikes to the onset\n",
    "\n",
    "                realigned_spike_times[grid].append(realigned_spikes)\n",
    "\n",
    "                # Display the MUAP if requested\n",
    "                if display_muap:\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    time_axis = np.arange(-pre_spike_window, post_spike_window) / sampling_rate * 1000  # in ms\n",
    "                    plt.plot(time_axis, best_channel_signal, label=\"MUAP\", color=colors[grid])\n",
    "                    plt.axvline(x=0, color='k', linestyle='--', label=\"Original Spike Time\")\n",
    "                    plt.axvline(x=(onset_idx - pre_spike_window) / sampling_rate * 1000, color='r', linestyle='--', label=\"Realigned Spike Time\")\n",
    "                    plt.axhline(y=threshold_high, color='g', linestyle='-', label=\"Threshold (6 x std)\")\n",
    "                    plt.axhline(y=threshold_low, color='g', linestyle='-')\n",
    "\n",
    "                    # Show the noise window (-40 to -20 ms and +20ms to +40ms) as a shaded area\n",
    "                    noise_start = -40  # in ms\n",
    "                    noise_end = -20  # in ms\n",
    "                    plt.axvspan(noise_start, noise_end, color='gray', alpha=0.3)\n",
    "                    noise_start = 20  # in ms\n",
    "                    noise_end = 40  # in ms\n",
    "                    plt.axvspan(noise_start, noise_end, color='gray', alpha=0.3, label=\"Noise Window (-40 to -20ms & +20ms to +40ms)\")\n",
    "\n",
    "                    # Plot title with MU and grid/channel information\n",
    "                    plt.title(f\"{filename} \\n MUAP for MU#{mu_idx}, Grid#{grid} - Best Channel: Grid {grid}, Row {best_channel_row}, Col {best_channel_col}\")\n",
    "                    plt.xlabel(\"Time (ms)\")\n",
    "                    plt.ylabel(\"Amplitude\")\n",
    "                    plt.legend(loc=\"upper right\")\n",
    "                    if display_figures_inline:\n",
    "                        plt.show()\n",
    "                    else:\n",
    "                        plt.close()\n",
    "\n",
    "            else:\n",
    "                realigned_spike_times[grid].append(spikes)  # No valid spikes, keep original\n",
    "\n",
    "    return realigned_spike_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_filter(input_signal, fsamp, freq_cutoff, filter_order):\n",
    "    nyquist = 0.5 * fsamp\n",
    "    normal_cutoff = freq_cutoff / nyquist\n",
    "    b, a = butter(filter_order, normal_cutoff, btype='low', analog=False)\n",
    "    filtered_signal = filtfilt(b, a, input_signal)\n",
    "    return filtered_signal\n",
    "\n",
    "def normalize_force_and_identify_plateaus(force_temp, input_fsamp, display_plot=False, condition=1, filename=\"filename not specified\"):\n",
    "    \"\"\"\n",
    "    Normalize the force signal and extract plateau samples.\n",
    "    \"\"\"\n",
    "    ### Detrend the force signal (there is drifting in some conditions)\n",
    "    force_temp_mean = np.mean(force_temp)\n",
    "    t = np.arange(len(force_temp)) / input_fsamp\n",
    "    force_temp_polyfit = np.polynomial.Polynomial.fit(t, force_temp, deg=polynomial_degree_for_detrending)  # fit degree N polynomial\n",
    "    force_temp -= force_temp_polyfit(t) # subtract the polynomial fit from the force signal\n",
    "    force_temp += force_temp_mean\n",
    "    ### Normalize the force signal\n",
    "    if force_is_3D:\n",
    "        force_temp_3D = force_temp\n",
    "        force_temp = force_temp[0,:]\n",
    "        std_ratios_relative_to_first_force_component = [1,\n",
    "                np.std(force_temp_3D[1,:])/np.std(force_temp_3D[0,:]),\n",
    "                np.std(force_temp_3D[2,:])/np.std(force_temp_3D[0,:])]\n",
    "    epsilon_for_meaningful_rampup_or_down = force_differential_epsilon_up_or_down\n",
    "    force_temp = force_temp / np.std(force_temp)\n",
    "    force_smoothed_diff = np.gradient(lowpass_filter(force_temp, input_fsamp, freq_cutoff=force_lowpass_cutoff, filter_order=4))\n",
    "    force_smoothed_diff_positive = np.full_like(force_smoothed_diff, np.nan)\n",
    "    positive_indices = np.where(force_smoothed_diff>=+epsilon_for_meaningful_rampup_or_down)[0]\n",
    "    force_smoothed_diff_positive[positive_indices] = force_smoothed_diff[positive_indices]\n",
    "    count_positive = len(positive_indices)\n",
    "    force_smoothed_diff_negative = np.full_like(force_smoothed_diff, np.nan)\n",
    "    negative_indices = np.where(force_smoothed_diff<-epsilon_for_meaningful_rampup_or_down)[0]\n",
    "    force_smoothed_diff_negative[negative_indices] = force_smoothed_diff[negative_indices]\n",
    "    count_negative = len(negative_indices)\n",
    "    plt.figure()\n",
    "    plt.plot(force_smoothed_diff, color=\"gray\")\n",
    "    plt.plot(force_smoothed_diff_positive, color=\"C1\", linewidth = 3, alpha = 0.75,\n",
    "             label=f\"{count_positive} samples where derivative is >= {epsilon_for_meaningful_rampup_or_down:.4f}\")\n",
    "    plt.plot(force_smoothed_diff_negative, color=\"C0\", linewidth = 3, alpha = 0.75,\n",
    "             label=f\"{count_negative} samples where derivative is < {epsilon_for_meaningful_rampup_or_down:.4f}\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    if display_figures_inline:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    if count_negative > count_positive:\n",
    "        force_temp = force_temp * (-1)\n",
    "    # Min-max normalize force_temp between 0 and 1 relative to percentile low and high\n",
    "    percentile_low = np.percentile(force_temp, 5)\n",
    "    percentile_high = np.percentile(force_temp, 95)\n",
    "    force_temp[:input_fsamp*2] = percentile_low\n",
    "    force_temp[-input_fsamp*2:] = percentile_low\n",
    "    force_temp = (force_temp - percentile_low) / (percentile_high - percentile_low)\n",
    "\n",
    "    plt.figure()\n",
    "    if force_is_3D:\n",
    "        plt.title(f\"{current_subject} - {current_condition} - 3D force\")\n",
    "        for force_direction in range(force_temp_3D.shape[0]):\n",
    "            if force_direction==0:\n",
    "                force_temp_3D[0,:] = force_temp\n",
    "                plt.plot(force_temp_3D[force_direction,:],label=f\"Force direction {force_direction}\")\n",
    "                continue\n",
    "            force_direction_temp = force_temp_3D[force_direction,:]\n",
    "            force_direction_temp = force_direction_temp / np.std(force_temp_3D[force_direction,:])\n",
    "            force_direction_temp = force_direction_temp * std_ratios_relative_to_first_force_component[force_direction]\n",
    "            if count_negative > count_positive:\n",
    "                force_direction_temp = force_direction_temp * (-1)\n",
    "            # Normalize 3D force components relative to main force component (1st channel)\n",
    "            percentile_low = np.percentile(force_direction_temp, 5)\n",
    "            force_direction_temp[:input_fsamp*2] = percentile_low\n",
    "            force_direction_temp[-input_fsamp*2:] = percentile_low\n",
    "            force_direction_temp = (force_direction_temp - percentile_low) / (percentile_high - percentile_low)\n",
    "            force_temp_3D[force_direction,:] = force_direction_temp\n",
    "            plt.plot(force_temp_3D[force_direction,:],label=f\"Force direction {force_direction}\")\n",
    "        force_amplitude_3D = np.sqrt(np.sum(force_temp_3D**2,axis=0))\n",
    "        plt.plot(force_amplitude_3D,color='k',label=f\"Magnitude of 3D force vector\")\n",
    "    else:\n",
    "        plt.title(f\"{current_subject} - {current_condition} - force\")\n",
    "        plt.plot(force_temp)\n",
    "    plt.xlabel(\"Time (samples)\")\n",
    "    plt.ylabel(\"Force (max standardized to 1, baseline to 0)\")\n",
    "    plt.legend()\n",
    "    if os.path.exists(current_folder):\n",
    "        plt.savefig(f'{current_folder}//{filename[0:-4]}_force_raw.png')\n",
    "    if display_figures_inline:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    # Smooth the force using a 1-second moving average\n",
    "    smoothing_window_force = int(np.round(input_fsamp * force_smoothing_window_length))  # 1 second window\n",
    "    force_smoothed = np.convolve(force_temp, np.ones(smoothing_window_force) / smoothing_window_force, mode='same')\n",
    "\n",
    "    # Compute force_diff (derivative of the smoothed force), using a 3-second moving average\n",
    "    force_diff = np.diff(force_smoothed)  # First, compute the derivative\n",
    "    smoothing_window_diff = int(np.round(input_fsamp * 3))  # 3 seconds window for moving average\n",
    "    force_diff_smoothed = np.convolve(force_diff, np.ones(smoothing_window_diff) / smoothing_window_diff, mode='same')\n",
    "\n",
    "    # Pad force_diff_smoothed to match the length of force_temp\n",
    "    force_diff_smoothed = np.pad(force_diff_smoothed, (1, 0), mode='constant', constant_values=0)\n",
    "\n",
    "    # Normalize the smoothed force_diff for easier threshold calculation\n",
    "    force_diff_smoothed = force_diff_smoothed / max(force_diff_smoothed)\n",
    "\n",
    "    # Time axis for plotting/intersections\n",
    "    time_axis = np.arange(len(force_temp))\n",
    "\n",
    "    if select_plateaus_manually:\n",
    "        valid_peaks, valid_troughs = select_plateaus_manually_with_user_interacton(\n",
    "            force_smoothed, time_axis)\n",
    "\n",
    "    else:\n",
    "        # Compute thresholds for peak and trough detection\n",
    "        std_diff = np.std(force_diff_smoothed)\n",
    "        thresh_for_peak = 0.5 * std_diff\n",
    "        thresh_for_trough = -0.5 * std_diff\n",
    "\n",
    "        # Function to detect intersections\n",
    "        def find_intersections(x, y, threshold, direction='down'):\n",
    "            intersections = []\n",
    "            for i in range(1, len(y)):\n",
    "                if (y[i - 1] > threshold and y[i] <= threshold) or (y[i - 1] < threshold and y[i] >= threshold):\n",
    "                    if direction == 'down' and y[i] < y[i - 1]:\n",
    "                        intersections.append(x[i])\n",
    "            return intersections\n",
    "\n",
    "        # Detect intersections (downward slope for peaks and troughs)\n",
    "        peak_intersections = find_intersections(time_axis, force_diff_smoothed, thresh_for_peak, direction='down')\n",
    "        trough_intersections = find_intersections(time_axis, force_diff_smoothed, thresh_for_trough, direction='down')\n",
    "        # Make sure they are sorted\n",
    "        peak_intersections = sorted(peak_intersections)\n",
    "        trough_intersections = sorted(trough_intersections)\n",
    "\n",
    "        # Define minimum plateau duration (in seconds)\n",
    "        min_plateau_duration_sec = 5\n",
    "        min_plateau_samples = int(min_plateau_duration_sec * input_fsamp)\n",
    "        # ---------------------------\n",
    "        # Combine and sort events\n",
    "        # ---------------------------\n",
    "        # Label each index as 'peak' or 'trough'\n",
    "        events = [(p, 'peak') for p in peak_intersections] + [(t, 'trough') for t in trough_intersections]\n",
    "        # Sort by time index\n",
    "        events.sort(key=lambda x: x[0])  # sorts by the first element in tuple\n",
    "        valid_peaks = []\n",
    "        valid_troughs = []\n",
    "        # -----------------------------------------\n",
    "        # Walk through the sorted list of events\n",
    "        # -----------------------------------------\n",
    "        i = 0\n",
    "        while i < len(events):\n",
    "            current_time, current_type = events[i]\n",
    "            # If it's a peak, we look ahead to see if the next event is a trough\n",
    "            if current_type == 'peak':\n",
    "                # Check if there *is* a next event\n",
    "                if i + 1 < len(events):\n",
    "                    next_time, next_type = events[i + 1]\n",
    "                    # The next event must be a trough\n",
    "                    if next_type == 'trough':\n",
    "                        # Check minimum time difference\n",
    "                        if (next_time - current_time) >= min_plateau_samples:\n",
    "                            # We have a valid peak-trough pair\n",
    "                            valid_peaks.append(current_time)\n",
    "                            valid_troughs.append(next_time)\n",
    "                            # Skip the next event (we've already paired it)\n",
    "                            i += 2\n",
    "                            continue\n",
    "                        else:\n",
    "                            # The trough is too close => remove both from consideration\n",
    "                            # i.e., skip them\n",
    "                            i += 2\n",
    "                            continue\n",
    "                    else:\n",
    "                        # The next event is another peak => remove the current peak\n",
    "                        i += 1\n",
    "                        continue\n",
    "                else:\n",
    "                    # No next event => remove this peak\n",
    "                    i += 1\n",
    "                    continue\n",
    "            else:\n",
    "                # Current event is a trough that isn't directly paired with a valid peak\n",
    "                # => skip it\n",
    "                i += 1\n",
    "    # End of 'if no manual selection of plateaus'\n",
    "\n",
    "    # At this point, valid_peaks and valid_troughs only contain\n",
    "    # properly paired peakâ€“trough events, with adequate time between them.\n",
    "    # ------------------------------------------------------------------\n",
    "    # Overwrite original peak_intersections and trough_intersections\n",
    "    peak_intersections = valid_peaks\n",
    "    trough_intersections = valid_troughs\n",
    "\n",
    "    # Normalization of the force data based on peaks and troughs (median during rest = 0, median during plateaus = 1)\n",
    "    def rest_VS_plateau_force_normalization_and_plateau_samples_identification(force, peaks, troughs, fs=None):\n",
    "        \"\"\"\n",
    "        Normalize the force signal so that:\n",
    "        1) The median 'rest' force => 0\n",
    "        2) The median 'plateau' force => 1\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        normalized_force : np.ndarray\n",
    "            The force data mapped so that the rest median is 0 and the plateau median is 1.\n",
    "        plateau_samples : list\n",
    "            Indices of the samples considered part of the plateaus.\n",
    "        rest_samples : list\n",
    "            Indices of the samples considered part of the rest intervals.\n",
    "        plateau_median : float\n",
    "            The median force value across all plateau intervals.\n",
    "        rest_force : float\n",
    "            The median force value across all rest intervals.\n",
    "        \"\"\"\n",
    "        # 1) Identify plateau intervals\n",
    "        plateau_values = []\n",
    "        plateau_samples = []\n",
    "        plateau_samples_at_each_plateau = []\n",
    "        # Go through each peak -> trough interval\n",
    "        for i in range(len(peaks)):\n",
    "            start_idx = peaks[i]\n",
    "            # Use matching trough or go to end of signal\n",
    "            if i < len(troughs):\n",
    "                end_idx = troughs[i]\n",
    "            else:\n",
    "                end_idx = len(force) - 1\n",
    "            plateau_values.extend(force[start_idx:end_idx])\n",
    "            plateau_samples.extend(range(start_idx, end_idx))\n",
    "            plateau_samples_at_each_plateau.append(range(start_idx, end_idx))\n",
    "        # 2) Define rest samples as everything outside the plateau samples\n",
    "        all_indices = set(range(len(force)))\n",
    "        plateau_indices = set(plateau_samples)\n",
    "        rest_indices = list(all_indices - plateau_indices)\n",
    "        # 3) Compute medians\n",
    "        #    - If we have no plateau samples, default to 1 to avoid division by zero\n",
    "        plateau_median = np.median(plateau_values) if len(plateau_values) > 0 else 1.0\n",
    "        #    - If we have no rest samples, default to 0\n",
    "        rest_values = force[rest_indices] if len(rest_indices) > 0 else np.array([0])\n",
    "        # here, \"rest_values\" actually contains the ramps. So taking the lowest 1% of the values instead (not the minimum value because of potential artifacts)\n",
    "        rest_force = np.percentile(rest_values, 1) # np.median(rest_values)\n",
    "        # 4) Apply a linear transform:\n",
    "        #    rest_force -> 0, plateau_median -> 1\n",
    "        denom = plateau_median - rest_force\n",
    "        if np.isclose(denom, 0):\n",
    "            # Edge case: if rest_force == plateau_median, we can't do normal scaling\n",
    "            # Fallback: just zero-center around rest_force\n",
    "            print(\"Warning: rest_force and plateau_median are the same. \"\n",
    "                \"Forcing zero-centering. Plateaus will also be at ~0.\")\n",
    "            normalized_force = force - rest_force\n",
    "        else:\n",
    "            normalized_force = (force - rest_force) / denom\n",
    "        return (normalized_force,\n",
    "                plateau_samples,\n",
    "                plateau_samples_at_each_plateau,\n",
    "                rest_indices,\n",
    "                plateau_median,\n",
    "                rest_force)\n",
    "\n",
    "    # Normalize the force data and get the plateau samples\n",
    "    (normalized_force, plateau_samples, plateau_samples_at_each_plateau,\n",
    "        _, _, _) = rest_VS_plateau_force_normalization_and_plateau_samples_identification(\n",
    "        force_temp, peak_intersections, trough_intersections)\n",
    "    \n",
    "    # Compute FFT of the force signal for each plateau and for the entire signal\n",
    "    # --- controls for shared frequency grid ---\n",
    "    bins_per_hz = 4           # \"at most N bins per Hz\"\n",
    "    fmax = 15.0               # up to 10 Hz (will be clipped to Nyquist)\n",
    "    fmax = min(fmax, input_fsamp/2.0)\n",
    "    n_bins = int(np.floor(bins_per_hz * fmax))\n",
    "    freq_edges = np.linspace(0.0, fmax, n_bins + 1)\n",
    "    freq_centers = 0.5 * (freq_edges[:-1] + freq_edges[1:])\n",
    "\n",
    "    def one_sided_psd_binned(x, fs, edges):\n",
    "        \"\"\"Return (bin_centers, PSD averaged within bins).\n",
    "        PSD units: (signal_units^2 / Hz); comparable across different lengths.\"\"\"\n",
    "        x = np.asarray(x)\n",
    "        if x.size < 2:\n",
    "            return freq_centers, np.full(edges.size - 1, np.nan)\n",
    "\n",
    "        # de-mean to remove offset\n",
    "        x = x - np.mean(x)\n",
    "\n",
    "        N = x.size\n",
    "        X = np.fft.rfft(x)\n",
    "        # Periodogram scaling for PSD (linear): |X|^2 / (fs * N), then one-sided\n",
    "        Pxx = (np.abs(X) ** 2) / (fs * N)\n",
    "        if N % 2 == 0:  # even N => Nyquist bin present at the end\n",
    "            if Pxx.size > 2:\n",
    "                Pxx[1:-1] *= 2.0\n",
    "        else:\n",
    "            if Pxx.size > 1:\n",
    "                Pxx[1:] *= 2.0\n",
    "\n",
    "        f = np.fft.rfftfreq(N, d=1.0/fs)\n",
    "\n",
    "        # Restrict to chosen band\n",
    "        m = (f >= edges[0]) & (f < edges[-1])\n",
    "        f = f[m]\n",
    "        Pxx = Pxx[m]\n",
    "\n",
    "        # Bin by averaging the PSD within each frequency bin\n",
    "        sums, _ = np.histogram(f, bins=edges, weights=Pxx)\n",
    "        counts, _ = np.histogram(f, bins=edges)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            binned = sums / counts\n",
    "        binned[counts == 0] = np.nan  # no samples fell into that bin\n",
    "\n",
    "        centers = 0.5 * (edges[:-1] + edges[1:])\n",
    "        return centers, binned\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # Compute PSD (binned) for each plateau & total + coefficient of variation of force during each plateau\n",
    "    # --------------------------------------------\n",
    "    psd_each_plateau = []\n",
    "    force_coeff_of_variation_each_plateau = []\n",
    "    force_std_each_plateau = []\n",
    "    force_unfiltered = force_temp * condition\n",
    "\n",
    "    for plateau_range in plateau_samples_at_each_plateau:\n",
    "        if len(plateau_range) > 1:\n",
    "            plateau_signal = force_unfiltered[plateau_range]\n",
    "            _, psd_binned = one_sided_psd_binned(plateau_signal, input_fsamp, freq_edges)\n",
    "            psd_each_plateau.append(psd_binned)\n",
    "            force_std_each_plateau.append(np.std(plateau_signal))\n",
    "            force_coeff_of_variation_each_plateau.append(\n",
    "                np.std(plateau_signal) / np.mean(plateau_signal)\n",
    "            )\n",
    "        else:\n",
    "            psd_each_plateau.append(None)\n",
    "            force_std_each_plateau.append(None)\n",
    "            force_coeff_of_variation_each_plateau.append(None)\n",
    "\n",
    "    # \"Total\" PSD over the concatenated/selected samples\n",
    "    _, psd_total_binned = one_sided_psd_binned(force_unfiltered[plateau_samples], input_fsamp, freq_edges)\n",
    "\n",
    "    # Mean PSD across plateaus (ignore missing/empty bins with NaNs)\n",
    "    valid_psds = [p for p in psd_each_plateau if p is not None]\n",
    "    if len(valid_psds):\n",
    "        mean_plateau_psd = np.nanmean(np.vstack(valid_psds), axis=0)\n",
    "    else:\n",
    "        mean_plateau_psd = np.full_like(psd_total_binned, np.nan)\n",
    "\n",
    "    # Scale the normalized force by the condition value\n",
    "    scaled_force = normalized_force * condition\n",
    "\n",
    "    # Plot the force if required\n",
    "    if display_plot:\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "        # Plot the original and smoothed force, with peaks and troughs\n",
    "        axs[0].plot(time_axis, force_temp, color=\"black\", label='force_raw')\n",
    "        axs[0].plot(time_axis, force_smoothed, color=\"C1\", label='force_smoothed')\n",
    "        if not select_plateaus_manually:\n",
    "            axs[0].plot(time_axis, force_diff_smoothed, color=\"C5\", label='force_smoothed_diff')\n",
    "            axs[0].axhline(y=thresh_for_peak, color=\"C2\", label='thresh_for_peak')\n",
    "            axs[0].axhline(y=thresh_for_trough, color=\"C2\", label='thresh_for_trough')\n",
    "            axs[0].scatter(peak_intersections, [thresh_for_peak] * len(peak_intersections), color='red', label='Peak Intersections')\n",
    "            axs[0].scatter(trough_intersections, [thresh_for_trough] * len(trough_intersections), color='blue', label='Trough Intersections')\n",
    "            axs[0].set_title(\"Original and Smoothed Force with Diff\")\n",
    "\n",
    "        # Plot the normalized force\n",
    "        axs[1].plot(time_axis, scaled_force, color=\"C1\", label='force_normalized (scaled)', zorder=1)\n",
    "        axs[1].scatter(time_axis[plateau_samples], scaled_force[plateau_samples], s=1, color='red',\n",
    "                    label='Plateau Samples', marker='o', zorder=2)\n",
    "        axs[1].legend()\n",
    "        axs[1].set_title(\"Normalized Force\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.title(f'{filename}')\n",
    "        if os.path.exists(current_folder):\n",
    "            plt.savefig(f'{current_folder}//{filename[0:-4]}_force.png')\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "        # Second figure, with 2 subplots (top, bottom)\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 6))\n",
    "\n",
    "        # 1) Top subplot: force during each plateau\n",
    "        for i, plateau_range in enumerate(plateau_samples_at_each_plateau):\n",
    "            if len(plateau_range) > 1:\n",
    "                plateau_signal = force_unfiltered[plateau_range]\n",
    "                ax1.plot(plateau_range, plateau_signal, color='black', linewidth=0.7)\n",
    "                mid_point = plateau_range[len(plateau_range)//2]\n",
    "                max_val = np.max(plateau_signal)\n",
    "                ax1.text(mid_point, max_val, f'STD={force_std_each_plateau[i]:.2f}% MVC\\nCV={force_coeff_of_variation_each_plateau[i]:.3f}',\n",
    "                        fontsize=8, ha='center', va='bottom')\n",
    "                ax1.set_title(\"Force during each plateau\")\n",
    "                ax1.set_xlabel(\"Time (samples)\")\n",
    "                ax1.set_ylabel(\"Force (% MVC)\")\n",
    "\n",
    "        # 2) Bottom subplot: PSD in dB for each plateau, total, and mean (up to fmax)\n",
    "        eps = 1e-20  # to avoid log(0)\n",
    "        for i, psd_binned in enumerate(psd_each_plateau):\n",
    "            if psd_binned is not None:\n",
    "                ax2.plot(freq_centers, 10*np.log10(psd_binned + eps), color='purple', alpha=0.3)\n",
    "\n",
    "        ax2.plot(freq_centers, 10*np.log10(psd_total_binned + eps), linewidth=2,\n",
    "                label='Total PSD (dB)', color='black')\n",
    "        ax2.plot(freq_centers, 10*np.log10(mean_plateau_psd + eps), linestyle='-',\n",
    "                linewidth=2, label='Mean plateau PSD (dB)', color='purple')\n",
    "\n",
    "        ax2.set_xlim(0, fmax)\n",
    "        ax2.set_title(\"Power Spectral Density (binned, dB)\")\n",
    "        ax2.set_xlabel(\"Frequency (Hz)\")\n",
    "        ax2.set_ylabel(\"PSD (dB re (%%MVC)^2/Hz)\")\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.title(f'{filename}')\n",
    "        if os.path.exists(current_folder):\n",
    "            plt.savefig(f'{current_folder}//{filename[0:-4]}_force_variations_and_psd.png')\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    # Keep linear PSD for downstream use\n",
    "    force_psd_total = psd_total_binned\n",
    "    force_psd_each_plateau = mean_plateau_psd\n",
    "\n",
    "    # Return the normalized force, plateau samples, and info about force\n",
    "    return scaled_force, plateau_samples, plateau_samples_at_each_plateau, force_psd_each_plateau, force_psd_total, force_std_each_plateau, force_coeff_of_variation_each_plateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recruitment_thresholds_and_firing_rates(normalized_force, spike_times,\n",
    "                                                    plateau_samples_at_each_plateau,\n",
    "                                                    time_window_to_check_for_ramp=7,  # seconds (used for ramp duration)\n",
    "                                                    lowpass_filter_force_freq=1,    # Hz cutoff for force filtering\n",
    "                                                    positive_fraction_threshold=0.5,  # fraction required in ramp window\n",
    "                                                    derivative_epsilon=0.05,          # threshold for normalized derivative\n",
    "                                                    threshold_for_MU_to_be_continuous=0.5,  # max gap in s for continuity (MFR)\n",
    "                                                    input_fsamp=2048,\n",
    "                                                    display_plot=False, filename='filename not specified'):\n",
    "    \"\"\"\n",
    "    Compute recruitment thresholds and mean firing rates for motor units (MUs) based on spike times,\n",
    "    normalized force, and pre-determined plateau intervals.\n",
    "    \n",
    "    Procedure:\n",
    "      1. Lowpass filter normalized force (cutoff: lowpass_filter_force_freq Hz) and compute its derivative.\n",
    "         Normalize the derivative (zero mean, unit std).\n",
    "      2. For each plateau:\n",
    "           - Check that at least time_window_to_check_for_ramp seconds of data exist prior to plateau onset.\n",
    "           - In the ramp window (from plateau_onset - ramp_duration_samples to plateau_onset),\n",
    "             compute the fraction of samples for which the normalized derivative > derivative_epsilon.\n",
    "             If that fraction is below positive_fraction_threshold, then the ramp is invalid and the\n",
    "             recruitment threshold and mean firing rate (MFR) for that plateau are set to NaN.\n",
    "      3. For a valid ramp:\n",
    "           - Compute the recruitment threshold using a candidate window for threshold defined as:\n",
    "                 candidate_start_thresh = max(0, plateau_onset - ramp_duration_samples)\n",
    "                 candidate_end = plateau_end\n",
    "             Then, within that window, slide a window of 5 candidate spikes. If a window is found where the time\n",
    "             difference between the first and the 5th spike is <= input_fsamp samples (â‰¤ 1 s),\n",
    "             then the recruitment threshold is the filtered force at the time of the first spike in that window and\n",
    "             its time is recorded; otherwise, assign NaN.\n",
    "           - Compute the MFR using the candidate window for firing rate defined as:\n",
    "                 if first plateau: candidate_start_mfr = plateau_onset,\n",
    "                 else: candidate_start_mfr = previous plateauâ€™s end,\n",
    "                 candidate_end = plateau_end.\n",
    "             Then, take all candidate spikes within that window, add the window boundaries,\n",
    "             compute inter-spike intervals (in s) and if the maximum interval exceeds threshold_for_MU_to_be_continuous,\n",
    "             MFR is set to NaN, else MFR = 1/mean(intervals).\n",
    "      4. If display_plot is True, produce a combined figure with:\n",
    "             - A top (large) subplot showing the filtered force curve (black) with the candidate window (red solid line)\n",
    "               for recruitment threshold overlaid. Also vertical lines at ramp start (blue) and plateau onset (orange)\n",
    "               are drawn. A raster plot of spike times (each MU on a separate row) with recruitment threshold markers is\n",
    "               superimposed on a twin y-axis.\n",
    "             - A bottom (smaller) subplot showing the normalized derivative (purple) with vertical lines at the same markers\n",
    "               and a horizontal line at derivative_epsilon.\n",
    "    \n",
    "    Returns:\n",
    "      MU_properties (dict): Dictionary keyed by grid. For each grid, a list is produced (one per MU) with a dictionary:\n",
    "            {\n",
    "                'MU': motor unit index,\n",
    "                'recruitment_thresholds': [threshold value per plateau or NaN],\n",
    "                'recruitment_times': [spike sample index for threshold or NaN],\n",
    "                'mean_firing_rates': [mean firing rate (Hz) per plateau or NaN]\n",
    "            }\n",
    "    \"\"\"\n",
    "    MU_properties = {}\n",
    "    ramp_duration_samples = int(time_window_to_check_for_ramp * input_fsamp)\n",
    "    \n",
    "    # Lowpass filter the normalized force.\n",
    "    filtered_force = lowpass_filter(normalized_force, input_fsamp, \n",
    "                                    freq_cutoff=lowpass_filter_force_freq, filter_order=4)\n",
    "    \n",
    "    # Compute derivative and normalize it.\n",
    "    force_deriv = np.diff(filtered_force)\n",
    "    force_deriv_normalized = (force_deriv - np.mean(force_deriv)) / np.std(force_deriv)\n",
    "    \n",
    "    # Prepare candidate window force array for debugging plot.\n",
    "    candidate_force_overall = np.full(len(normalized_force), np.nan)\n",
    "    \n",
    "    for grid in spike_times.keys():\n",
    "        MU_properties[grid] = []\n",
    "        for mu_idx, mu_spikes in enumerate(spike_times[grid]):\n",
    "            # Avoid empty MUs (because of the cell structure it is stored in in Matlab)\n",
    "            if len(mu_spikes) < 1:\n",
    "                continue\n",
    "            mu_spikes = np.array(mu_spikes).astype(int)\n",
    "            mu_recruitment_thresholds = []\n",
    "            mu_recruitment_times = []\n",
    "            mu_mean_firing_rates = []\n",
    "            mu_mean_cov = []\n",
    "            for i, plateau in enumerate(plateau_samples_at_each_plateau):\n",
    "                plateau_onset = plateau.start if hasattr(plateau, 'start') else plateau[0]\n",
    "                plateau_end = plateau.stop - 1 if hasattr(plateau, 'stop') else plateau[-1]\n",
    "                \n",
    "                if plateau_onset < ramp_duration_samples:\n",
    "                    mu_recruitment_thresholds.append(np.nan)\n",
    "                    mu_recruitment_times.append(np.nan)\n",
    "                    mu_mean_firing_rates.append(np.nan)\n",
    "                    mu_mean_cov.append(np.nan)\n",
    "                    continue\n",
    "                \n",
    "                ramp_start = plateau_onset - ramp_duration_samples\n",
    "                ramp_deriv = force_deriv_normalized[ramp_start:plateau_onset]\n",
    "                pos_fraction = np.sum(ramp_deriv > derivative_epsilon) / len(ramp_deriv) if len(ramp_deriv) else 0\n",
    "                \n",
    "                if pos_fraction < positive_fraction_threshold:\n",
    "                    mu_recruitment_thresholds.append(np.nan)\n",
    "                    mu_recruitment_times.append(np.nan)\n",
    "                    mu_mean_firing_rates.append(np.nan)\n",
    "                    mu_mean_cov.append(np.nan)\n",
    "                else:\n",
    "                    # Valid ramp. For recruitment threshold, candidate window includes time before plateau onset.\n",
    "                    candidate_start_thresh = max(0, plateau_onset - ramp_duration_samples)\n",
    "                    candidate_end = plateau_end\n",
    "                    candidate_idx_thresh = np.arange(candidate_start_thresh, candidate_end+1)\n",
    "                    candidate_idx_thresh = candidate_idx_thresh[candidate_idx_thresh >= candidate_start_thresh]\n",
    "                    candidate_force_overall[candidate_idx_thresh] = filtered_force[candidate_idx_thresh]\n",
    "                    \n",
    "                    # Get candidate spikes for threshold: use all spikes from candidate_start_thresh to candidate_end.\n",
    "                    candidates_thresh = mu_spikes[(mu_spikes >= candidate_start_thresh) & (mu_spikes <= candidate_end)]\n",
    "                    if candidates_thresh.size < 5:\n",
    "                        mu_recruitment_thresholds.append(np.nan)\n",
    "                        mu_recruitment_times.append(np.nan)\n",
    "                    else:\n",
    "                        found = False\n",
    "                        for j in range(len(candidates_thresh) - 4):\n",
    "                            window = candidates_thresh[j:j+5]\n",
    "                            if (window[-1] - window[0]) <= input_fsamp:  # within 1 s\n",
    "                                recruitment_time_idx = window[0]\n",
    "                                found = True\n",
    "                                break\n",
    "                        if found:\n",
    "                            mu_recruitment_thresholds.append(filtered_force[recruitment_time_idx])\n",
    "                            mu_recruitment_times.append(recruitment_time_idx)\n",
    "                        else:\n",
    "                            mu_recruitment_thresholds.append(np.nan)\n",
    "                            mu_recruitment_times.append(np.nan)\n",
    "                    \n",
    "                    # Mean firing rate (MFR) calculation uses plateau samples only.\n",
    "                    candidate_start_mfr = plateau_onset\n",
    "                    candidate_end_mfr = plateau_end\n",
    "                    candidate_idx_mfr = np.arange(candidate_start_mfr, candidate_end_mfr+1)\n",
    "                    candidate_spikes_mfr = mu_spikes[(mu_spikes >= candidate_start_mfr) & (mu_spikes <= candidate_end_mfr)]\n",
    "                    if candidate_spikes_mfr.size < 2:\n",
    "                        # need at least 2 spikes to form an interval\n",
    "                        mfr = np.nan\n",
    "                        mcov = np.nan\n",
    "                    else:\n",
    "                        # check continuity: include start/end as well\n",
    "                        combined_times = np.sort(np.concatenate(([candidate_start_mfr], candidate_spikes_mfr, [candidate_end_mfr])))\n",
    "                        if np.max(np.diff(combined_times) / input_fsamp) > threshold_for_MU_to_be_continuous:\n",
    "                            mfr = np.nan\n",
    "                            mcov = np.nan\n",
    "                        else:\n",
    "                            intervals = np.diff(candidate_spikes_mfr) / input_fsamp  # in seconds\n",
    "                            mfr = 1 / np.mean(intervals)\n",
    "                            mcov = np.std(intervals) / np.mean(intervals)\n",
    "                    mu_mean_firing_rates.append(mfr)\n",
    "                    mu_mean_cov.append(mcov)\n",
    "            MU_properties[grid].append({\n",
    "                'MU': mu_idx,\n",
    "                'recruitment_thresholds': mu_recruitment_thresholds,\n",
    "                'recruitment_times': mu_recruitment_times,\n",
    "                'mean_firing_rates': mu_mean_firing_rates,\n",
    "                'mean_cov': mu_mean_cov\n",
    "                })\n",
    "    \n",
    "    # Combined debug figure.\n",
    "    if display_plot:\n",
    "        import matplotlib.gridspec as gridspec\n",
    "        fig = plt.figure(figsize=(20, 15))\n",
    "        gs = fig.add_gridspec(nrows=2, ncols=1, height_ratios=[3, 1])\n",
    "        ax_force = fig.add_subplot(gs[0])\n",
    "        ax_deriv = fig.add_subplot(gs[1])\n",
    "        \n",
    "        time_axis = np.arange(len(normalized_force)) / input_fsamp\n",
    "        ax_force.plot(time_axis, filtered_force, color='black', label='Filtered Force')\n",
    "        ax_force.plot(time_axis, candidate_force_overall, color='red', linestyle='-', linewidth=2, label='Candidate Window')\n",
    "        for plateau in plateau_samples_at_each_plateau:\n",
    "            plateau_onset = plateau.start if hasattr(plateau, 'start') else plateau[0]\n",
    "            t_onset = plateau_onset / input_fsamp\n",
    "            if plateau_onset >= ramp_duration_samples:\n",
    "                ramp_start = plateau_onset - ramp_duration_samples\n",
    "                ax_force.axvline(x=ramp_start/input_fsamp, color='blue', linestyle='--', alpha=0.7)\n",
    "            ax_force.axvline(x=t_onset, color='orange', linestyle='--', alpha=0.7)\n",
    "        ax_force.set_xlabel(\"Time (s)\")\n",
    "        ax_force.set_ylabel(\"Filtered Force\")\n",
    "        ax_force.set_title(f\"{filename} - Filtered Force, Candidate Window & Raster\")\n",
    "        ax_force.legend(loc='upper left')\n",
    "        \n",
    "        # Raster overlay (twin y-axis).\n",
    "        ax_raster = ax_force.twinx()\n",
    "        mu_global_index = 0\n",
    "        # raster_spike_times = []\n",
    "        # raster_mu_indices = []\n",
    "        # Also, for each MU, plot a marker (e.g., magenta circle) at each recruitment threshold.\n",
    "        for grid in spike_times.keys():\n",
    "            for mu_idx, mu_spikes in enumerate(spike_times[grid]):\n",
    "                # Avoid empty MUs (because of the cell structure it is stored in in Matlab)\n",
    "                if len(mu_spikes) < 1:\n",
    "                    continue\n",
    "                color_of_MU = globals()[f\"{data[current_subject][current_condition]['muscle_per_grid'][grid]}_color\"]\n",
    "                mu_spikes = np.array(mu_spikes).astype(int)\n",
    "                y_val = mu_global_index + 1\n",
    "                # raster_spike_times.extend(mu_spikes)\n",
    "                # raster_mu_indices.extend([y_val] * len(mu_spikes))\n",
    "                # Check recruitment thresholds.\n",
    "                for entry in MU_properties[grid]:\n",
    "                    if entry['MU'] == mu_idx:\n",
    "                        for rec_time in entry['recruitment_times']:\n",
    "                            if not np.isnan(rec_time):\n",
    "                                ax_raster.plot(rec_time / input_fsamp, y_val, marker='o', markersize=10, color=color_of_MU, markeredgecolor='black')\n",
    "                ax_raster.scatter(mu_spikes/input_fsamp,\n",
    "                                  [y_val] * len(mu_spikes), marker='|', color=color_of_MU)\n",
    "                mu_global_index += 1\n",
    "        # ax_raster.scatter(np.array(raster_spike_times)/input_fsamp, raster_mu_indices, marker='|', color=color_of_MU)\n",
    "        ax_raster.set_ylabel(\"MU Index (Raster)\")\n",
    "        ax_raster.set_ylim(0, mu_global_index + 1)\n",
    "        ax_raster.tick_params(axis='y', labelcolor='gray')\n",
    "        \n",
    "        # Bottom subplot: Normalized derivative.\n",
    "        time_axis_deriv = np.arange(1, len(filtered_force)) / input_fsamp\n",
    "        ax_deriv.plot(time_axis_deriv, force_deriv_normalized, color='purple', label='Normalized Derivative')\n",
    "        for plateau in plateau_samples_at_each_plateau:\n",
    "            plateau_onset = plateau.start if hasattr(plateau, 'start') else plateau[0]\n",
    "            t_onset = plateau_onset / input_fsamp\n",
    "            if plateau_onset >= ramp_duration_samples:\n",
    "                ramp_start = plateau_onset - ramp_duration_samples\n",
    "                ax_deriv.axvline(x=ramp_start/input_fsamp, color='blue', linestyle='--', alpha=0.7)\n",
    "            ax_deriv.axvline(x=t_onset, color='orange', linestyle='--', alpha=0.7)\n",
    "        ax_deriv.axhline(y=derivative_epsilon, color='green', linestyle='-', label='Epsilon threshold')\n",
    "        ax_deriv.set_xlabel(\"Time (s)\")\n",
    "        ax_deriv.set_ylabel(\"Normalized Derivative\")\n",
    "        ax_deriv.legend(loc='best')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if os.path.exists(current_folder):\n",
    "            fig.savefig(os.path.join(current_folder, f\"{filename}_recruitment_thresholds_debug.png\"))\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "    \n",
    "    return MU_properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_firing_rates(data, current_subject, current_condition, input_fsamp=2048, Wind_s=0.4, display_plots=False):\n",
    "    \"\"\"\n",
    "    Process motor unit data for the current subject and condition.\n",
    "    It will generate smoothed firing rates\n",
    "\n",
    "    Parameters:\n",
    "    data (dict): Dictionary where processed data will be stored.\n",
    "    current_subject (str): The subject's identifier.\n",
    "    current_condition (str): The condition for the current subject.\n",
    "    input_fsamp (int): Sampling rate.\n",
    "    Wind_s (float): Hanning window duration (default: 0.4 seconds).\n",
    "    display_plots (bool): Whether to display the smoothed and preprocessed firing rates plots.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated 'data' dictionary containing processed MU information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve spike times\n",
    "    spike_times = data[current_subject][current_condition]['spike_times']\n",
    "\n",
    "    # Retrieve the force array for the condition\n",
    "    force = data[current_subject][current_condition]['force']\n",
    "    force_len = len(force)\n",
    "\n",
    "    # Create Hanning filter for smoothed firing rates\n",
    "    Wind_length = round(input_fsamp * Wind_s)\n",
    "    HanningW = windows.hann(Wind_length)  # Hanning window filter\n",
    "    HanningW = HanningW / np.sum(HanningW)  # Normalize window to have a unitary area\n",
    "\n",
    "    # Initialize lists for each processed variable\n",
    "    smoothed_firing_rates = {grid: [] for grid in data[current_subject][current_condition]['spike_times'].keys()}\n",
    " \n",
    "    # Extract continuous plateau windows from plateau_samples\n",
    "    plateau_samples = data[current_subject][current_condition]['plateau_samples']\n",
    "\n",
    "    # failsafe in case plateau_samples is empty = make all samples be plateau samples\n",
    "    if plateau_samples == []:\n",
    "        plateau_samples = np.arange(len(data[current_subject][current_condition]['force']))\n",
    "\n",
    "    # Ensure spike times are cast to integer for binary spike train creation\n",
    "    for grid in data[current_subject][current_condition]['spike_times'].keys():\n",
    "        for mu_idx, mu_spikes in enumerate(spike_times[grid]):\n",
    "            # If MU has spikes, process them\n",
    "            if len(mu_spikes) > 0:\n",
    "                mu_spikes = mu_spikes.astype(int)  # Ensure spike times are integers\n",
    "\n",
    "                binary_spike_train = np.zeros(force_len, dtype=int)\n",
    "                binary_spike_train[mu_spikes] = 1  # Mark spike times in the binary train\n",
    "\n",
    "                # Smoothed firing rate\n",
    "                smoothed_rate = filtfilt(HanningW, 1, binary_spike_train * input_fsamp)\n",
    "\n",
    "                # Ensure that smoothed_rate matches the length of the force array\n",
    "                if len(smoothed_rate) > force_len:\n",
    "                    smoothed_rate = smoothed_rate[:force_len]  # Trim if longer\n",
    "                elif len(smoothed_rate) < force_len:\n",
    "                    smoothed_rate = np.pad(smoothed_rate, (0, force_len - len(smoothed_rate)))  # Pad with zeros if shorter\n",
    "\n",
    "                smoothed_firing_rates[grid].append(smoothed_rate)\n",
    "\n",
    "    # Store smoothed firing rates and valid MUs\n",
    "    data[current_subject][current_condition]['smoothed_firing_rates'] = smoothed_firing_rates\n",
    "\n",
    "    # Initialize for plotting\n",
    "    if display_plots:\n",
    "        plt.figure(figsize=(15, 10))  # Make the plots wider\n",
    "        time_axis = np.arange(force_len) / input_fsamp\n",
    "        for grid in data[current_subject][current_condition]['spike_times'].keys():\n",
    "            for mu_idx, smoothed_rate in enumerate(smoothed_firing_rates[grid]):\n",
    "                color = grid_colors[grid]\n",
    "                plt.plot(time_axis, smoothed_rate, color=color, alpha=0.5)\n",
    "        plt.title(f'Smoothed Firing Rates - Subject {current_subject} - Condition {current_condition} (all MUs)')\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Firing Rate (Hz)')\n",
    "        plt.grid(True)\n",
    "        if os.path.exists(current_folder):\n",
    "            plt.savefig(f'{current_folder}//{filei[:-4]}_firing_rates.png')\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_plateaus_manually_with_user_interacton(force, time_axis):\n",
    "    \"\"\"\n",
    "    Displays an interactive figure using the Qt backend.\n",
    "    \n",
    "    The user clicks on the figure to record ROI boundaries.\n",
    "    - Press 'z' to undo the last click.\n",
    "    - Press Enter when finished.\n",
    "    \n",
    "    When done, the recorded boundaries are sorted such that:\n",
    "      * The lowest boundary defines the start of the first ROI.\n",
    "      * The second lowest defines the end of the first ROI.\n",
    "      * The third lowest defines the start of the second ROI, etc.\n",
    "      \n",
    "    If an odd number of boundaries is recorded, the final ROI will end at the maximum x value.\n",
    "    \n",
    "    Non-ROI regions are shaded using axvspan.\n",
    "    \n",
    "    Returns:\n",
    "        roi_starts (np.array of int): The start positions of the ROIs.\n",
    "        roi_ends   (np.array of int): The end positions of the ROIs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create figure and plot the force signal.\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # display raster plot of spike times\n",
    "    raster_spike_times = []\n",
    "    raster_mu_indices = []\n",
    "    raster_colors = []\n",
    "    spike_times = data[current_subject][current_condition]['spike_times']\n",
    "    mu_total_iter = -1\n",
    "    for grid in spike_times.keys():\n",
    "        for mu_idx, mu_spikes in enumerate(spike_times[grid]):\n",
    "            if len(mu_spikes) == 0:\n",
    "                continue  # No spikes for this MU, skip\n",
    "            mu_total_iter += 1  # Increment total motor unit index\n",
    "            # Convert spike times to sample indices (rounding)\n",
    "            mu_spikes = np.array(mu_spikes).astype(int)\n",
    "            # Store spike times and indices for raster plotting\n",
    "            raster_spike_times.extend(mu_spikes)\n",
    "            raster_mu_indices.extend([mu_total_iter] * len(mu_spikes))  # Ensure each MU is on a different row\n",
    "            # Color by grid\n",
    "            # raster_colors.extend([f\"C{grid}\"] * len(mu_spikes)) \n",
    "            raster_colors.extend([grid_colors[grid][0:3]] * len(mu_spikes)) # without the alpha value\n",
    "    ax.scatter(np.array(raster_spike_times), raster_mu_indices, \n",
    "               zorder = -1, c=raster_colors, marker='|', alpha = 0.25)\n",
    "    \n",
    "    ax.plot(time_axis, force*mu_total_iter, color='black', linewidth = 2, label = 'Smoothed force (a.u.)')\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"{current_subject} - {current_condition}\\nClick to add ROI boundaries.\\nPress 'z' to undo; press Enter when done.\")\n",
    "    ax.set_xlabel(\"Time (samples)\")\n",
    "    \n",
    "    x_min, x_max = time_axis[0], time_axis[-1]\n",
    "    boundaries = []       # Interactive buffer to store clicked x positions.\n",
    "    clicked_lines = []    # List to store permanent blue vertical lines for each boundary.\n",
    "    shading = []          # List to store axvspan objects for non-ROI areas.\n",
    "    \n",
    "    def update_shading():\n",
    "        \"\"\"Update the shaded regions based on current boundaries.\"\"\"\n",
    "        nonlocal shading\n",
    "        # Remove previous shading.\n",
    "        for sh in shading:\n",
    "            sh.remove()\n",
    "        shading = []\n",
    "        \n",
    "        sorted_boundaries = sorted(boundaries)\n",
    "        if len(sorted_boundaries) == 0:\n",
    "            fig.canvas.draw_idle()\n",
    "            return\n",
    "        # If an odd number of boundaries, assume final ROI extends to x_max.\n",
    "        if len(sorted_boundaries) % 2 == 1:\n",
    "            sorted_boundaries.append(x_max)\n",
    "        # Shade region before the first ROI.\n",
    "        if sorted_boundaries[0] > x_min:\n",
    "            shading.append(ax.axvspan(x_min, sorted_boundaries[0], color='gray', alpha=0.3))\n",
    "        # Shade gaps between ROIs.\n",
    "        for i in range(0, len(sorted_boundaries)-2, 2):\n",
    "            roi_end = sorted_boundaries[i+1]\n",
    "            next_roi_start = sorted_boundaries[i+2]\n",
    "            if next_roi_start > roi_end:\n",
    "                shading.append(ax.axvspan(roi_end, next_roi_start, color='gray', alpha=0.3))\n",
    "        # Shade region after the last ROI.\n",
    "        if sorted_boundaries[-1] < x_max:\n",
    "            shading.append(ax.axvspan(sorted_boundaries[-1], x_max, color='gray', alpha=0.3))\n",
    "        fig.canvas.draw_idle()\n",
    "    \n",
    "    def update_boundary_lines():\n",
    "        \"\"\"Update permanent blue lines for each recorded boundary.\"\"\"\n",
    "        nonlocal clicked_lines\n",
    "        # Remove existing boundary lines.\n",
    "        for line in clicked_lines:\n",
    "            line.remove()\n",
    "        clicked_lines = []\n",
    "        # Draw a blue vertical line at each boundary.\n",
    "        for b in boundaries:\n",
    "            clicked_lines.append(ax.axvline(x=b, color='blue', linestyle='-', linewidth=1))\n",
    "        fig.canvas.draw_idle()\n",
    "    \n",
    "    finished = False  # Flag to signal that ROI selection is complete.\n",
    "    \n",
    "    def on_click(event):\n",
    "        \"\"\"Record the x position when the user clicks and update the display.\"\"\"\n",
    "        if event.inaxes != ax or event.xdata is None:\n",
    "            return\n",
    "        boundaries.append(event.xdata)\n",
    "        update_boundary_lines()\n",
    "        update_shading()\n",
    "    \n",
    "    def on_key(event):\n",
    "        \"\"\"Handle key presses:\n",
    "           - 'enter' finishes the selection.\n",
    "           - 'z' undoes the last click.\n",
    "        \"\"\"\n",
    "        nonlocal finished\n",
    "        if event.key == 'enter':\n",
    "            finished = True\n",
    "            fig.canvas.stop_event_loop()\n",
    "        elif event.key.lower() == 'z':\n",
    "            if boundaries:\n",
    "                boundaries.pop()  # Remove the last clicked boundary.\n",
    "                update_boundary_lines()\n",
    "                update_shading()\n",
    "    \n",
    "    # Connect event callbacks.\n",
    "    cid_click = fig.canvas.mpl_connect('button_press_event', on_click)\n",
    "    cid_key   = fig.canvas.mpl_connect('key_press_event', on_key)\n",
    "    \n",
    "    print(\"ROI selection in progress.\\nClick to add boundaries; press 'z' to undo; press Enter when done.\")\n",
    "    # Start an event loop that waits until the user presses Enter.\n",
    "    fig.canvas.start_event_loop(0)\n",
    "    while not finished:\n",
    "        plt.pause(0.1)\n",
    "    \n",
    "    # Disconnect the event handlers.\n",
    "    fig.canvas.mpl_disconnect(cid_click)\n",
    "    fig.canvas.mpl_disconnect(cid_key)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Process the recorded boundaries.\n",
    "    sorted_boundaries = sorted(boundaries)\n",
    "    if len(sorted_boundaries) == 0:\n",
    "        # No clicks: the ROI covers the entire signal.\n",
    "        roi_starts = [x_min]\n",
    "        roi_ends   = [x_max]\n",
    "    else:\n",
    "        if len(sorted_boundaries) % 2 == 1:\n",
    "            sorted_boundaries.append(x_max)\n",
    "        roi_starts = []\n",
    "        roi_ends   = []\n",
    "        for i in range(0, len(sorted_boundaries), 2):\n",
    "            roi_starts.append(sorted_boundaries[i])\n",
    "            roi_ends.append(sorted_boundaries[i+1])\n",
    "    \n",
    "    # Convert boundaries to integer arrays.\n",
    "    roi_starts = np.array(roi_starts, dtype=int)\n",
    "    roi_ends   = np.array(roi_ends, dtype=int)\n",
    "    \n",
    "    print(\"Final ROIs:\")\n",
    "    print(\"Starts:\", roi_starts)\n",
    "    print(\"Ends:  \", roi_ends)\n",
    "    \n",
    "    return roi_starts, roi_ends\n",
    "\n",
    "# Example usage:\n",
    "# roi_starts, roi_ends = select_plateaus_manually_with_user_interacton(force, time_axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_plateau_subset_for_K_plateaus(valid_MUs_per_plateau, nb_plateaus_to_keep):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "    - valid_MUs_per_plateau: a (nb_MUs x nb_plateaus) numpy array of 0/1,\n",
    "        where rows are Motor Units (MUs), columns are plateaus.\n",
    "    - nb_plateaus_to_keep (K): the number of columns (plateaus) to select (K < nb_plateaus).\n",
    "    Returns:\n",
    "    - best_nb_remaining_MUs: the maximum number of MUs (rows) that can be retained\n",
    "                            by using exactly K columns\n",
    "    - best_plateaus_subset:  the subset of K column indices that achieves that maximum\n",
    "                            number of rows\n",
    "    - best_valid_MUs:        the indices of the MUs (rows) that remain (i.e., have all 1s \n",
    "                            in the chosen subset of columns)\n",
    "    \"\"\"\n",
    "    nb_MUs, nb_plateaus = valid_MUs_per_plateau.shape\n",
    "    \n",
    "    best_nb_remaining_MUs = -1\n",
    "    best_plateaus_subset = None\n",
    "    best_valid_MUs = None  # We'll store the row indices that remain\n",
    "\n",
    "    # Enumerate all subsets of columns of size nb_plateaus_to_keep\n",
    "    for cols in itertools.combinations(range(nb_plateaus), nb_plateaus_to_keep):\n",
    "        # Extract the submatrix for these columns only\n",
    "        submat = valid_MUs_per_plateau[:, cols]  # shape (nb_MUs, nb_plateaus_to_keep)\n",
    "        \n",
    "        # Identify which rows have all 1s across these K columns\n",
    "        row_mask = np.all(submat == 1, axis=1)  # Boolean mask of shape (nb_MUs,)\n",
    "        coverage = np.sum(row_mask)             # Number of rows that remain\n",
    "        \n",
    "        # Update the best number of rows if this subset is better\n",
    "        if coverage > best_nb_remaining_MUs:\n",
    "            best_nb_remaining_MUs = coverage\n",
    "            best_plateaus_subset = cols\n",
    "            best_valid_MUs = np.where(row_mask)[0]  # Indices of the rows that remain\n",
    "\n",
    "    return best_nb_remaining_MUs, best_plateaus_subset, best_valid_MUs\n",
    "\n",
    "def Select_plateaus_for_dim_and_coh_analyses(data, current_subject, current_condition, display_plots=False, \n",
    "                                              minimum_mu_coverage=0.5):\n",
    "    \"\"\"\n",
    "    Select a subset of plateaus for dimensionality and coherence analyses such that the\n",
    "    number of motor units (MUs) retained is maximized.\n",
    "    \n",
    "    A MU is considered â€œvalidâ€ in a plateau if a criterion is met (e.g. based on its spike times).\n",
    "    This function computes a matrix (nb_MUs x nb_plateaus) of 0/1 values (0 = invalid, 1 = valid),\n",
    "    and then uses an optimization routine to select a subset of plateaus (of size K) that retains the\n",
    "    maximum number of MUs. The loop terminates when the optimal subset retains at least\n",
    "    (nb_MUs Ã— minimum_mu_coverage) MUs.\n",
    "    \n",
    "    Parameters:\n",
    "      data: nested dictionary structure for the subject and condition.\n",
    "      current_subject: key for current subject.\n",
    "      current_condition: key for current condition.\n",
    "      display_plots (bool): whether to display a heatmap for debugging.\n",
    "      minimum_mu_coverage (float): minimal fraction (between 0 and 1) of MUs that must be valid across the selected\n",
    "                                   plateaus (default=0.5).\n",
    "    \n",
    "    Returns:\n",
    "      data: the input dictionary updated with:\n",
    "            'selected_plateaus' (the indices of the selected plateaus) and\n",
    "            'valid_MUs_given_selected_plateaus' (the indices of the MUs retained).\n",
    "    \"\"\"\n",
    "    \n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Number of plateaus (from your data)\n",
    "    nb_plateaus = len(data[current_subject][current_condition]['plateau_samples_at_each_plateau'])\n",
    "    nb_MUs = len(data[current_subject][current_condition]['MU_spike_times_list'])\n",
    "    \n",
    "    # Build the validity matrix: 1 if MU is valid for a plateau, 0 otherwise.\n",
    "    valid_MUs_per_plateau = np.zeros((nb_MUs, nb_plateaus))  # shape: (nb_MUs, nb_plateaus)\n",
    "    \n",
    "    for plateau_i in range(nb_plateaus):\n",
    "        for mn_i in range(nb_MUs):\n",
    "            # Get the lower and upper bounds (sample indices) of the current plateau.\n",
    "            plateau_current_lower_bound = data[current_subject][current_condition]['plateau_samples_at_each_plateau'][plateau_i][0]\n",
    "            plateau_current_upper_bound = data[current_subject][current_condition]['plateau_samples_at_each_plateau'][plateau_i][-1]\n",
    "            \n",
    "            # Retrieve the spike times for this MU.\n",
    "            spikes_within_plateau = data[current_subject][current_condition]['MU_spike_times_list'][mn_i]\n",
    "            # Select only spikes that occur strictly between the lower and upper bound.\n",
    "            spikes_within_plateau = spikes_within_plateau[(spikes_within_plateau > plateau_current_lower_bound) &\n",
    "                                                          (spikes_within_plateau < plateau_current_upper_bound)]\n",
    "            # Append the boundaries to ensure we capture gaps at the start and end.\n",
    "            spikes_within_plateau = np.concatenate(([plateau_current_lower_bound],\n",
    "                                                      spikes_within_plateau,\n",
    "                                                      [plateau_current_upper_bound]))\n",
    "            # If the maximum inter-spike interval (converted to seconds) is below a threshold, mark as valid.\n",
    "            if (np.max(np.diff(spikes_within_plateau))/input_fsamp) < ISI_thresh_for_discontinuity:\n",
    "                valid_MUs_per_plateau[mn_i, plateau_i] = 1\n",
    "\n",
    "    if not select_plateaus_manually:\n",
    "        # Select the number of plateaus to keep depending on the condition.\n",
    "        # Find keys in nb_plateaus_to_select that match current_condition.\n",
    "        matching_keys = [key for key in nb_plateaus_to_select.keys() if key in current_condition]\n",
    "        if len(matching_keys) == 1:\n",
    "            matched_key = matching_keys[0]\n",
    "            nb_plateaus_to_keep = nb_plateaus_to_select[matched_key]\n",
    "        elif len(matching_keys) > 1:\n",
    "            print(f\"Warning: Multiple matches found {matching_keys}, selecting the first one.\")\n",
    "            matched_key = matching_keys[0]\n",
    "            nb_plateaus_to_keep = nb_plateaus_to_select[matched_key]\n",
    "        else:\n",
    "            print(\"No matching key found, using default value.\")\n",
    "            nb_plateaus_to_keep = 1\n",
    "    else: # if plateaus have been selected manually, keep all of them\n",
    "        nb_plateaus_to_keep = nb_plateaus\n",
    "\n",
    "    # Optimize plateau selection.\n",
    "    best_nb_remaining_MUs = 0\n",
    "    nb_iter_optimize_MUs_to_keep = -1\n",
    "    \n",
    "    while (best_nb_remaining_MUs < nb_MUs * minimum_mu_coverage) and ((nb_iter_optimize_MUs_to_keep + 1) < nb_plateaus_to_keep):\n",
    "        nb_iter_optimize_MUs_to_keep += 1\n",
    "        K = nb_plateaus_to_keep - nb_iter_optimize_MUs_to_keep\n",
    "        best_nb_remaining_MUs, best_plateaus_subset, remaining_valid_MUs = find_optimal_plateau_subset_for_K_plateaus(valid_MUs_per_plateau, nb_plateaus_to_keep=K)\n",
    "        print(f\"Optimal solution for {K} plateaus:\")\n",
    "        print(f\"    Best coverage = {best_nb_remaining_MUs} MUs (out of {nb_MUs})\")\n",
    "        print(f\"    Best subset of plateaus = {best_plateaus_subset}\")\n",
    "        print(f\"    Remaining MUs: {remaining_valid_MUs}\")\n",
    "    \n",
    "    # If no subset meets the desired coverage, force at least one plateau.\n",
    "    if best_plateaus_subset is None:\n",
    "        best_nb_remaining_MUs, best_plateaus_subset, remaining_valid_MUs = find_optimal_plateau_subset_for_K_plateaus(valid_MUs_per_plateau, nb_plateaus_to_keep=1)\n",
    "    \n",
    "    data[current_subject][current_condition]['selected_plateaus'] = best_plateaus_subset\n",
    "    data[current_subject][current_condition]['valid_MUs_given_selected_plateaus'] = remaining_valid_MUs\n",
    "\n",
    "    # (Optional) Plot the heatmap for debugging.\n",
    "    if display_plots:\n",
    "        valid_MUs_per_plateau_to_plot = np.copy(valid_MUs_per_plateau)\n",
    "        valid_MUs_per_plateau_to_plot[:,best_plateaus_subset] += 2\n",
    "        all_rows = set(range(nb_MUs))\n",
    "        invalid_rows = all_rows - set(remaining_valid_MUs)\n",
    "        for r in invalid_rows:\n",
    "            valid_MUs_per_plateau_to_plot[r, :] = 4\n",
    "        from matplotlib.colors import ListedColormap\n",
    "        discrete_cmap = ListedColormap([\"royalblue\",\"brown\",\"skyblue\", \"gold\", \"darkgray\"])\n",
    "        plt.figure(figsize=(6,6))\n",
    "        sns.heatmap(valid_MUs_per_plateau_to_plot, cmap=discrete_cmap, cbar=False, \n",
    "                    linewidths=0.25, linecolor='white', vmin=0, vmax=4)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.xlabel(\"Plateau #\")\n",
    "        plt.ylabel(\"MU #\")\n",
    "        plt.title(f'Selected plateaus (n={nb_plateaus_to_keep}) for dimensionality and coherence analyses, and remaining valid MUs \\nSubject {current_subject} - Condition {current_condition} (Valid MUs)')\n",
    "        import matplotlib.patches as mpatches\n",
    "        mapping = {\n",
    "            0: (\"Invalid (plateau not selected)\", discrete_cmap.colors[0]),\n",
    "            1: (\"Valid (plateau not selected)\", discrete_cmap.colors[1]),\n",
    "            2: (\"Invalid (plateaus selected)\", discrete_cmap.colors[2]),\n",
    "            3: (\"Valid (plateaus selected)\", discrete_cmap.colors[3]),\n",
    "            4: (\"Invalid MUs\", discrete_cmap.colors[4])\n",
    "        }\n",
    "        unique_codes = np.unique(valid_MUs_per_plateau_to_plot.astype(int))\n",
    "        legend_order = [0,1,2,3,4]\n",
    "        legend_patches = [mpatches.Patch(color=mapping[code][1], label=mapping[code][0])\n",
    "                          for code in legend_order if code in unique_codes]\n",
    "        plt.legend(handles=legend_patches, loc=\"upper right\", bbox_to_anchor=(1.6, 1))\n",
    "        if os.path.exists(current_folder):\n",
    "            plt.savefig(f'{current_folder}//{filei[:-4]}_selected_plateaus_and_valid_MUs.png', bbox_inches='tight')\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepare_spike_trains_and_firing_rates_for_analyses(data, current_subject, current_condition, display_plots=False):\n",
    "    # Get list of MU being valid or not based on selected plateaus and whether or not they are continuous on the plateaus\n",
    "    nb_MUs = len(data[current_subject][current_condition]['MU_spike_times_list'])\n",
    "    valid_MUs_given_selected_plateaus = data[current_subject][current_condition]['valid_MUs_given_selected_plateaus']\n",
    "    # Get selected plateaus and their corresponding samples\n",
    "    selected_plateaus = data[current_subject][current_condition]['selected_plateaus']\n",
    "    sample_ranges_corresponding_to_selected_plateaus = [\n",
    "        data[current_subject][current_condition]['plateau_samples_at_each_plateau'][i]\n",
    "        for i in selected_plateaus]\n",
    "    total_selected_samples_nb = 0\n",
    "    for range_plateau_i in sample_ranges_corresponding_to_selected_plateaus:\n",
    "        total_selected_samples_nb += len(range_plateau_i)\n",
    "    # Initializing preprocessed smoothed discharge rates on selected plateaus as a np.array [nb_MUs x time (sum of samples of selected plateaus)], with nan values for invalid MUs => For factor analysis\n",
    "    preprocessed_smoothed_firing_rates_during_selected_plateaus = np.full((nb_MUs, total_selected_samples_nb), np.nan)\n",
    "    smoothed_firing_rates_full_temp = data[current_subject][current_condition]['smoothed_firing_rates']\n",
    "    # Initializing binary discharge matrix on selected plateaus as a np.array [nb_MUs x time (sum of samples of selected plateaus)], with nan values for invalid MUs => For factor analysis with surrogate data, and coherence analysis\n",
    "    binary_discharge_matrix_during_selected_plateaus = np.full((nb_MUs, total_selected_samples_nb), np.nan)\n",
    "    spike_times_during_selected_plateaus = [[] for _ in range(nb_MUs)]\n",
    "    spike_times_full_temp = data[current_subject][current_condition]['spike_times']\n",
    "    # Filling the initialized matrix\n",
    "    end_of_previous_range = -1\n",
    "    end_of_each_selected_plateau = []\n",
    "    selected_plateau_samples_idx_total = []\n",
    "    for range_plateau_i in sample_ranges_corresponding_to_selected_plateaus:\n",
    "        current_sample_range = np.arange(len(range_plateau_i))+1+end_of_previous_range\n",
    "        selected_plateau_samples_idx_total.extend(np.arange(range_plateau_i[0], range_plateau_i[-1]+1))\n",
    "        for mn_i in range(nb_MUs): # valid_MUs_given_selected_plateaus:\n",
    "            # Find corresponding grid and the index within the grid\n",
    "            grid_of_MU = data[current_subject][current_condition]['MU_grid_idx_list'][mn_i]\n",
    "            nb_of_MUs_in_previous_grids = len(np.where(np.array(data[current_subject][current_condition]['MU_grid_idx_list'])<grid_of_MU)[0])\n",
    "            idx_of_MN_within_grid = mn_i - nb_of_MUs_in_previous_grids\n",
    "            # Smoothed discharge rates - preprocess\n",
    "            if mn_i in valid_MUs_given_selected_plateaus:\n",
    "                temp_smoothed_firing_rate = np.copy(smoothed_firing_rates_full_temp[grid_of_MU][idx_of_MN_within_grid])\n",
    "                temp_smoothed_firing_rate = temp_smoothed_firing_rate[range_plateau_i]\n",
    "                temp_smoothed_firing_rate = detrend(temp_smoothed_firing_rate)\n",
    "                temp_smoothed_firing_rate = temp_smoothed_firing_rate - np.mean(temp_smoothed_firing_rate)\n",
    "                temp_smoothed_firing_rate = temp_smoothed_firing_rate / np.std(temp_smoothed_firing_rate)\n",
    "                preprocessed_smoothed_firing_rates_during_selected_plateaus[mn_i,current_sample_range] = temp_smoothed_firing_rate\n",
    "            # Binary spike times\n",
    "            temp_spike_times = np.copy(spike_times_full_temp[grid_of_MU][idx_of_MN_within_grid])\n",
    "            temp_spike_times = temp_spike_times[(temp_spike_times >= range_plateau_i[0]) & (temp_spike_times <= range_plateau_i[-1])]\n",
    "            temp_spike_times -= range_plateau_i[0]\n",
    "            temp_binary_spike_times = np.zeros(len(range_plateau_i))\n",
    "            temp_binary_spike_times[temp_spike_times.astype(int)] = 1\n",
    "            binary_discharge_matrix_during_selected_plateaus[mn_i,current_sample_range] = temp_binary_spike_times\n",
    "            spike_times_during_selected_plateaus[mn_i].extend(temp_spike_times + end_of_previous_range + 1)\n",
    "        # end of \"for each MU\"\n",
    "        end_of_previous_range = current_sample_range[-1]\n",
    "        end_of_each_selected_plateau.append(end_of_previous_range)\n",
    "    # end of \"for each plateau\"\n",
    "\n",
    "    # ASSIGN VALUES\n",
    "    data[current_subject][current_condition]['preprocessed_smoothed_firing_rates_during_selected_plateaus'] = preprocessed_smoothed_firing_rates_during_selected_plateaus\n",
    "    data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'] = binary_discharge_matrix_during_selected_plateaus\n",
    "    data[current_subject][current_condition]['spike_times_during_selected_plateaus'] = spike_times_during_selected_plateaus\n",
    "    data[current_subject][current_condition]['selected_plateaus_samples_idx'] = selected_plateau_samples_idx_total\n",
    "\n",
    "    if display_plots:\n",
    "        # PLOT - smoothed firing rates\n",
    "        plt.figure(figsize=(30,6))\n",
    "        previous_muscle_of_MN = \"\"\n",
    "        for mn_i in range(nb_MUs):\n",
    "            muscle_of_MN = data[current_subject][current_condition]['MU_muscle_list'][mn_i]\n",
    "            color_of_plot = globals()[f\"{muscle_of_MN}_color\"]\n",
    "            if muscle_of_MN != previous_muscle_of_MN:\n",
    "                plt.plot(np.arange(total_selected_samples_nb)/input_fsamp, preprocessed_smoothed_firing_rates_during_selected_plateaus[mn_i,:], color=color_of_plot, alpha = 1, linewidth = 1.5,\n",
    "                        label = f\"{muscle_of_MN}\")\n",
    "            else:\n",
    "                plt.plot(np.arange(total_selected_samples_nb)/input_fsamp, preprocessed_smoothed_firing_rates_during_selected_plateaus[mn_i,:], color=color_of_plot, alpha = 0.1, linewidth = 1.5)\n",
    "            previous_muscle_of_MN = muscle_of_MN\n",
    "        plt.xlabel(\"Second\")\n",
    "        plt.ylabel(\"Firing rate (detrended and normalized)\")\n",
    "        plt.title(f\"Preprocessed smoothed firing rates on selected plateaus, only valid MUs\\nSubject {current_subject} - Condition {current_condition}\")\n",
    "        plt.axvline(x=0, color='black', linestyle='--')\n",
    "        for end_i in end_of_each_selected_plateau:\n",
    "            plt.axvline(x=end_i/input_fsamp, color='black', linestyle='--', linewidth = 3, alpha=0.5)\n",
    "        plt.xlim((-1,total_selected_samples_nb/input_fsamp+1))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        for end_i in end_of_each_selected_plateau:\n",
    "            plt.axvline(x=end_i, color='black', linestyle='--')\n",
    "        if os.path.exists(current_folder):\n",
    "            plt.savefig(f'{current_folder}//{filei[:-4]}_preprocessed_smoothed_firing_rates_selected.png', bbox_inches='tight')\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "        # PLOT - spike trains\n",
    "        plt.figure(figsize=(30,6))\n",
    "        # valid_count = -1\n",
    "        for mn_i in range(nb_MUs):\n",
    "            muscle_of_MN = data[current_subject][current_condition]['MU_muscle_list'][mn_i]\n",
    "            color_of_plot = globals()[f\"{muscle_of_MN}_color\"]\n",
    "            if mn_i not in data[current_subject][current_condition]['valid_MUs_given_selected_plateaus']:\n",
    "                color_of_plot = mix_colors(color_of_plot, \"#979797\") # invalid MUs are greyed out\n",
    "                previous_muscle_of_MN = muscle_of_MN\n",
    "            if muscle_of_MN != previous_muscle_of_MN:\n",
    "                plt.scatter(np.array(spike_times_during_selected_plateaus[mn_i])/input_fsamp, np.ones(len(spike_times_during_selected_plateaus[mn_i]))*(mn_i+1),\n",
    "                            color=color_of_plot, alpha = 0.5, marker = '|', label = f\"{muscle_of_MN}\")\n",
    "            else:\n",
    "                plt.scatter(np.array(spike_times_during_selected_plateaus[mn_i])/input_fsamp, np.ones(len(spike_times_during_selected_plateaus[mn_i]))*(mn_i+1),\n",
    "                            color=color_of_plot, alpha = 0.5, marker = '|')\n",
    "            previous_muscle_of_MN = muscle_of_MN\n",
    "        plt.axvline(x=0, color='black', linestyle='--')\n",
    "        for end_i in end_of_each_selected_plateau:\n",
    "            plt.axvline(x=end_i/input_fsamp, color='black', linestyle='--', linewidth = 3, alpha=0.5)\n",
    "        plt.title(f\"Spike trains on selected plateaus\\nSubject {current_subject} - Condition {current_condition}\")\n",
    "        plt.xlabel(\"Second\")\n",
    "        plt.ylabel(\"MN #\")\n",
    "        if os.path.exists(current_folder):\n",
    "            plt.savefig(f'{current_folder}//{filei[:-4]}_spike_trains_selected.png', bbox_inches='tight')\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Helper functions for CSV\n",
    "# --------------------------------------\n",
    "def append_to_csv(file_path, row_dict, fieldnames):\n",
    "    \"\"\"\n",
    "    Appends a single dictionary 'row_dict' to a CSV file at 'file_path',\n",
    "    creating the file with header if it doesn't exist.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    with open(file_path, mode='a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "def write_big_csv(file_path, data_list, fieldnames):\n",
    "    \"\"\"\n",
    "    Writes a list of dictionary rows (data_list) into a CSV file at file_path,\n",
    "    overwriting if it already exists (no old rows accumulate).\n",
    "    \"\"\"\n",
    "    with open(file_path, mode='w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "### LOAD EXPERIMENTAL DATA (.mat)\n",
    "\n",
    "nb_of_files = -1\n",
    "\n",
    "# Build default field names for the MU properties CSV\n",
    "fieldnames = [\"subject\", \"condition\",\"MU_idx\",\"muscle_of_MU\",\"grid\",\n",
    "              \"Recruitment_thresholds\",\"Recruitment_threshold_mean\",\"firing_rates\",\"firing_rates_mean\",\"cov\",\"cov_mean\"]\n",
    "MU_properties_all_subjects_csv = []\n",
    "all_subjects_csv_path = path_of_files\n",
    "\n",
    "for filei in files_to_analyze_experimental_data:\n",
    "    nb_of_files += 1\n",
    "\n",
    "    file_path = os.path.join(path_of_files, filei)\n",
    "    current_subject, current_condition = filei.replace('.mat', '').split('_', 1)\n",
    "    print(\"Currently loading...\")\n",
    "    print(\"     current_subject = \", current_subject)\n",
    "    print(\"     current_condition = \", current_condition)\n",
    "\n",
    "    # Create a new folder for each participant, and sub-folders for each condition\n",
    "    path_for_participant = os.path.join(path_of_files, current_subject)\n",
    "    path_for_participant_and_condition = os.path.join(path_for_participant, f'{current_subject}_{current_condition}')\n",
    "    if not os.path.exists(path_for_participant):\n",
    "        os.makedirs(path_for_participant)\n",
    "    if not os.path.exists(path_for_participant_and_condition):\n",
    "        os.makedirs(path_for_participant_and_condition)\n",
    "    current_folder = path_for_participant_and_condition\n",
    "\n",
    "    # Initialize dict keys\n",
    "    if current_subject not in data:\n",
    "        data[current_subject] = {}\n",
    "    if current_condition not in data[current_subject]:\n",
    "        data[current_subject][current_condition] = {}\n",
    "    current_muscle = current_condition.split('_')[0]\n",
    "    nb_grids = nb_grids_origin[current_muscle]\n",
    "\n",
    "    # Load .mat file\n",
    "    data[current_subject][current_condition] = load_file(file_path)\n",
    "\n",
    "    # Overwrite local CSV => remove if it already exists\n",
    "    subject_and_condition_csv_path = os.path.join(path_for_participant_and_condition, \"MU_properties.csv\")\n",
    "    if os.path.exists(subject_and_condition_csv_path):\n",
    "        os.remove(subject_and_condition_csv_path)\n",
    "\n",
    "    # Reorganize EMG data\n",
    "    data[current_subject][current_condition]['EMG'] = reorganize_emg(\n",
    "        data[current_subject][current_condition]['EMG'], \n",
    "        data[current_subject][current_condition]['channels_mask']\n",
    "    )\n",
    "\n",
    "    # Estimate MUAP onset times\n",
    "    data[current_subject][current_condition]['spike_times'] = realign_spike_times(\n",
    "        data[current_subject][current_condition]['EMG'], \n",
    "        data[current_subject][current_condition]['spike_times'], \n",
    "        sampling_rate=input_fsamp, \n",
    "        display_muap=False, #display_muap=True, \n",
    "        filename=filei\n",
    "    )\n",
    "\n",
    "    # Normalize and scale force data\n",
    "    # Use a regular expression to extract the number\n",
    "    match_number = re.search(r'\\d+', current_condition)\n",
    "    if match_number:\n",
    "        extracted_number = float(match_number.group())  # Convert the extracted number to float\n",
    "        # print(\"Extracted number:\", extracted_number)\n",
    "        current_condition_force = extracted_number\n",
    "    else:\n",
    "        print(\"No number found in the condition name\")\n",
    "        if ('abduction' in current_condition) or ('flexion' in current_condition) or ('Eyes' in current_condition):\n",
    "            current_condition_force = 20\n",
    "        else:\n",
    "            current_condition_force = 100\n",
    "    if force_is_3D:\n",
    "        (normalized_force, plateau_samples, plateau_samples_at_each_plateau,\n",
    "        force_psd_each_plateau, force_psd_total, force_std_each_plateau, force_coeff_of_variation_each_plateau) = normalize_force_and_identify_plateaus(\n",
    "            data[current_subject][current_condition]['force_3D'],\n",
    "            input_fsamp=input_fsamp, \n",
    "            display_plot=True, \n",
    "            condition=current_condition_force,  # Scaling by current condition\n",
    "            filename=filei\n",
    "        )\n",
    "    else:\n",
    "        (normalized_force, plateau_samples, plateau_samples_at_each_plateau,\n",
    "        force_psd_each_plateau, force_psd_total, force_std_each_plateau, force_coeff_of_variation_each_plateau) = normalize_force_and_identify_plateaus(\n",
    "            data[current_subject][current_condition]['force'],\n",
    "            input_fsamp=input_fsamp, \n",
    "            display_plot=True, \n",
    "            condition=current_condition_force,  # Scaling by current condition\n",
    "            filename=filei\n",
    "        )      \n",
    "    # if current_condition == 'FDI_flexion':\n",
    "    #     sys.error(\"stop\")\n",
    "    data[current_subject][current_condition]['force'] = normalized_force\n",
    "    data[current_subject][current_condition]['plateau_samples'] = plateau_samples\n",
    "    data[current_subject][current_condition]['plateau_samples_at_each_plateau'] = plateau_samples_at_each_plateau\n",
    "    data[current_subject][current_condition]['force_psd'] = {}\n",
    "    data[current_subject][current_condition]['force_psd']['total'] = force_psd_total\n",
    "    data[current_subject][current_condition]['force_psd']['each_plateau'] = force_psd_each_plateau\n",
    "    data[current_subject][current_condition]['force_coeff_of_var_each_plateau'] = force_coeff_of_variation_each_plateau\n",
    "    data[current_subject][current_condition]['force_std_each_plateau'] = force_std_each_plateau \n",
    "\n",
    "    # Call process_firing_rates function to update the existing 'data' structure\n",
    "    data = process_firing_rates(\n",
    "        data, current_subject, current_condition, input_fsamp=input_fsamp, Wind_s=Wind_s, display_plots=True)\n",
    "\n",
    "    del data[current_subject][current_condition]['EMG'] # removing the EMG data to save memory\n",
    "\n",
    "    # Assign muscle names to the grids and motor units\n",
    "    current_muscle = current_condition.split('_')[0]\n",
    "    # current_contraction = current_condition.split('_')[1]\n",
    "    data[current_subject][current_condition]['muscle_per_grid'] = []\n",
    "    if current_muscle == 'Quadriceps':\n",
    "        for gridi in range(nb_grids_origin[current_muscle]):\n",
    "            if gridi in VM_grids:\n",
    "                data[current_subject][current_condition]['muscle_per_grid'].append('VM')\n",
    "            elif gridi in VL_grids:\n",
    "                data[current_subject][current_condition]['muscle_per_grid'].append('VL')\n",
    "    elif current_muscle == 'TA':\n",
    "        for gridi in range(nb_grids_origin[current_muscle]):\n",
    "            data[current_subject][current_condition]['muscle_per_grid'].append('TA')\n",
    "    elif current_muscle == 'TricepsSurae':\n",
    "        for gridi in range(nb_grids_origin[current_muscle]):\n",
    "            if gridi in SOL_grids:\n",
    "                data[current_subject][current_condition]['muscle_per_grid'].append('SOL')\n",
    "            elif gridi in GM_grids:\n",
    "                data[current_subject][current_condition]['muscle_per_grid'].append('GM')\n",
    "    elif current_muscle == 'Fingers':\n",
    "        for gridi in range(nb_grids_origin[current_muscle]):\n",
    "            if gridi in FDI_grids:\n",
    "                data[current_subject][current_condition]['muscle_per_grid'].append('FDI')\n",
    "            elif gridi in APB_grids:\n",
    "                data[current_subject][current_condition]['muscle_per_grid'].append('APB')\n",
    "    elif current_muscle == 'FDI':\n",
    "        for gridi in range(nb_grids_origin[current_muscle]):\n",
    "            if gridi in FDI_grids:\n",
    "                data[current_subject][current_condition]['muscle_per_grid'].append('FDI')\n",
    "    elif current_muscle == 'VL':\n",
    "        for gridi in range(nb_grids_origin[current_muscle]):\n",
    "            if gridi in FDI_grids:\n",
    "                data[current_subject][current_condition]['muscle_per_grid'].append('VL')\n",
    "    \n",
    "    # Compute recruitment thresholds\n",
    "    data[current_subject][current_condition]['MU_properties'] = compute_recruitment_thresholds_and_firing_rates(\n",
    "        data[current_subject][current_condition]['force'], \n",
    "        data[current_subject][current_condition]['spike_times'],\n",
    "        data[current_subject][current_condition]['plateau_samples_at_each_plateau'],\n",
    "        time_window_to_check_for_ramp=7,  # in seconds\n",
    "        lowpass_filter_force_freq = 1,\n",
    "        positive_fraction_threshold = 0.5,\n",
    "        derivative_epsilon = 0.05,\n",
    "        threshold_for_MU_to_be_continuous = ISI_thresh_for_discontinuity,\n",
    "        input_fsamp=input_fsamp,  # Sampling rate\n",
    "        display_plot=True,\n",
    "        filename=filei\n",
    "    )\n",
    "\n",
    "    # Flatten the spike times dictionnary into a single list of NumPy arrays\n",
    "    #  flattened_list = [array for arrays in data[current_subject][current_condition]['spike_times'].values() for array in arrays]\n",
    "    MU_spike_times_list = []\n",
    "    MU_grid_idx_list = []\n",
    "    for key, arrays in data[current_subject][current_condition]['spike_times'].items():\n",
    "        MU_spike_times_list.extend(arrays)  # Add all arrays to the flattened list\n",
    "        MU_grid_idx_list.extend([int(key)] * len(arrays))  # Add the key for each array\n",
    "    MU_all_recruitment_thresholds_list = []\n",
    "    MU_mean_recruitment_thresholds_list = np.zeros(len(MU_spike_times_list))\n",
    "    MU_all_firing_rates_list = []\n",
    "    MU_mean_firing_rates_list = np.zeros(len(MU_spike_times_list))\n",
    "    MU_all_cov_list = []\n",
    "    MU_mean_cov_list = np.zeros(len(MU_spike_times_list))\n",
    "    MU_empty = np.zeros(len(MU_spike_times_list))\n",
    "    MU_muscle_list = []\n",
    "\n",
    "    current_grid = -1\n",
    "    current_mu_within_grid = -1\n",
    "    for mui in range(len(MU_spike_times_list)):\n",
    "        # Get corresponding grid and within-grid MU idx\n",
    "        current_grid_previous = current_grid\n",
    "        current_grid = MU_grid_idx_list[mui]\n",
    "        if current_grid_previous != current_grid:\n",
    "            current_mu_within_grid = -1\n",
    "        current_mu_within_grid += 1\n",
    "        # find corresponding muscle MUs\n",
    "        MU_muscle_list.append(data[current_subject][current_condition]['muscle_per_grid'][current_grid])\n",
    "        # find empty MUs\n",
    "        if len(MU_spike_times_list[mui]) < 1:\n",
    "            MU_empty[mui] = 1\n",
    "        else: # Get properties\n",
    "            MU_all_recruitment_thresholds_list.append(np.array(\n",
    "                data[current_subject][current_condition]['MU_properties'][current_grid][current_mu_within_grid]['recruitment_thresholds']))\n",
    "            MU_mean_recruitment_thresholds_list[mui] = np.nanmean(\n",
    "                data[current_subject][current_condition]['MU_properties'][current_grid][current_mu_within_grid]['recruitment_thresholds'])\n",
    "            MU_all_firing_rates_list.append(np.array(\n",
    "                data[current_subject][current_condition]['MU_properties'][current_grid][current_mu_within_grid]['mean_firing_rates']))\n",
    "            MU_mean_firing_rates_list[mui] = np.nanmean(\n",
    "                data[current_subject][current_condition]['MU_properties'][current_grid][current_mu_within_grid]['mean_firing_rates'])\n",
    "            MU_all_cov_list.append(np.array(\n",
    "                data[current_subject][current_condition]['MU_properties'][current_grid][current_mu_within_grid]['mean_cov']))\n",
    "            MU_mean_cov_list[mui] = np.nanmean(\n",
    "                data[current_subject][current_condition]['MU_properties'][current_grid][current_mu_within_grid]['mean_cov'])\n",
    "        \n",
    "    # Remove empty MUs from all lists\n",
    "    MU_spike_times_list = [MU_spike_times_list[i] for i in range(len(MU_spike_times_list)) if MU_empty[i] == 0]\n",
    "    MU_grid_idx_list = [MU_grid_idx_list[i] for i in range(len(MU_grid_idx_list)) if MU_empty[i] == 0]\n",
    "    MU_muscle_list = [MU_muscle_list[i] for i in range(len(MU_muscle_list)) if MU_empty[i] == 0]\n",
    "    MU_mean_recruitment_thresholds_list = [MU_mean_recruitment_thresholds_list[i] for i in range(len(MU_mean_recruitment_thresholds_list)) if MU_empty[i] == 0]\n",
    "    MU_mean_firing_rates_list = [MU_mean_firing_rates_list[i] for i in range(len(MU_mean_firing_rates_list)) if MU_empty[i] == 0]\n",
    "    MU_mean_cov_list = [MU_mean_cov_list[i] for i in range(len(MU_mean_cov_list)) if MU_empty[i] == 0]\n",
    "\n",
    "    data[current_subject][current_condition]['Empty_MUs_removed_idx'] = MU_empty\n",
    "    data[current_subject][current_condition]['MU_spike_times_list'] = MU_spike_times_list\n",
    "    data[current_subject][current_condition]['MU_grid_idx_list'] = MU_grid_idx_list\n",
    "    data[current_subject][current_condition]['MU_muscle_list'] = MU_muscle_list\n",
    "    data[current_subject][current_condition]['MU_mean_recruitment_thresholds_list'] = MU_mean_recruitment_thresholds_list\n",
    "    data[current_subject][current_condition]['MU_mean_firing_rates_list'] = MU_mean_firing_rates_list\n",
    "    data[current_subject][current_condition]['MU_mean_cov_list'] = MU_mean_cov_list\n",
    "\n",
    "    # Update iteratively a csv with MU properties, which is saved at the end (once all files have been loaded)\n",
    "    for mui in range(len(MU_spike_times_list)):\n",
    "        row = {\n",
    "            \"subject\":current_subject,\n",
    "            \"condition\":current_condition,\n",
    "            \"MU_idx\":mui,\n",
    "            \"muscle_of_MU\":MU_muscle_list[mui],\n",
    "            \"grid\":MU_grid_idx_list[mui],\n",
    "            \"Recruitment_thresholds\":MU_all_recruitment_thresholds_list[mui],\n",
    "            \"Recruitment_threshold_mean\":MU_mean_recruitment_thresholds_list[mui],\n",
    "            \"firing_rates\":MU_all_firing_rates_list[mui],\n",
    "            \"firing_rates_mean\":MU_mean_firing_rates_list[mui],\n",
    "            \"cov\": MU_all_cov_list[mui],\n",
    "            \"cov_mean\": MU_mean_cov_list[mui]\n",
    "        }\n",
    "        # Write to the local CSV (for this subject)\n",
    "        append_to_csv(subject_and_condition_csv_path, row, fieldnames)\n",
    "        # Also add to the global list for the big CSV (all subjects)\n",
    "        MU_properties_all_subjects_csv.append(row)\n",
    "    # end of \"for each MU\" / \"each csv row\"\n",
    "\n",
    "    data = Select_plateaus_for_dim_and_coh_analyses(data, current_subject, current_condition, display_plots=True, minimum_mu_coverage = min_proportion_of_MUs_to_keep)\n",
    "    data = Prepare_spike_trains_and_firing_rates_for_analyses(data, current_subject, current_condition, display_plots=True)\n",
    "\n",
    "    if export_processed_spike_trains_as_hdf5_file_for_new_analysis:\n",
    "        spike_trains_output_as_hdf5_file = os.path.join(path_of_files, f\"{current_subject}_{current_condition}.h5\")\n",
    "        with h5py.File(spike_trains_output_as_hdf5_file, \"w\") as f:\n",
    "            # Save onlythe motoneuron and pool indices + the spike trains\n",
    "            # 1) motoneuron and pool indices\n",
    "            mn_pool_grp = f.create_group(\"motoneurons_and_pools_indices\")\n",
    "            # (a) pool_list_by_MN -- a string array\n",
    "            pool_list = data[current_subject][current_condition]['MU_muscle_list']\n",
    "            # make a numpy array of dtype \"variableâ€length UTFâ€8 string\"\n",
    "            str_dt = string_dtype(encoding=\"utf-8\")\n",
    "            mn_pool_grp.create_dataset(\n",
    "                \"pool_list_by_MN\",\n",
    "                data=np.array(pool_list, dtype=str_dt),\n",
    "                dtype=str_dt)\n",
    "            # (b) idx_of_MN_by_pool -- subgroup of integer arrays\n",
    "            by_pool_grp = mn_pool_grp.create_group(\"idx_of_MN_by_pool\")\n",
    "            idx_of_MN_by_pool = {}\n",
    "            for mn_idx, pool_name in enumerate(pool_list):\n",
    "                if pool_name not in idx_of_MN_by_pool:\n",
    "                    idx_of_MN_by_pool[pool_name] = []\n",
    "                idx_of_MN_by_pool[pool_name].append(mn_idx)\n",
    "            for poolname, idx_array in idx_of_MN_by_pool.items():\n",
    "                idx_array = np.array(idx_array)\n",
    "                # poolname is something like \"VM\", \"VL\", ...\n",
    "                by_pool_grp.create_dataset(poolname, data=idx_array.astype(int))\n",
    "\n",
    "            # 2) spike trains\n",
    "            spikes = f.create_group(\"spike_trains\")\n",
    "            mn_spikes = spikes.create_group(\"MN\")\n",
    "            for i, tr in enumerate(data[current_subject][current_condition]['MU_spike_times_list']):\n",
    "                mn_spikes.create_dataset(f\"MN_{i}\", data=tr)\n",
    "            print(f\"Processed spike trains saved to {spike_trains_output_as_hdf5_file}\")\n",
    "    # end of \"if export_processed_spike_trains_as_hdf5_file_for_new_analysis\"\n",
    "\n",
    "# End of \"for each file\"\n",
    "gc.collect()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Write results of MU properties for all subjects\n",
    "# --------------------------------------------------------------------\n",
    "all_subjects_csv_path = os.path.join(path_of_files, \"MU_properties_ALL.csv\")\n",
    "write_big_csv(all_subjects_csv_path, MU_properties_all_subjects_csv, fieldnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV AND FIGURES ABOUT FORCE VARIABILITY & PSD\n",
    "\n",
    "# -----------------------------\n",
    "# Params & helpers\n",
    "# -----------------------------\n",
    "# Set to the same fmax used when computing PSDs (default in your code was 10.0)\n",
    "FMAX_USED = 10.0\n",
    "EPS = 1e-20\n",
    "\n",
    "# Save folder (optional); if you have a variable path_of_files already, weâ€™ll use it.\n",
    "PATH_OF_FILES = f\"{path_of_files}\" # globals().get(\"path_of_files\", \".\")\n",
    "# os.makedirs(PATH_OF_FILES, exist_ok=True)\n",
    "\n",
    "# Your color map and special-case mapping\n",
    "MUSCLE_COLORS = {\n",
    "    \"Quadriceps\": \"#ff3c00\",\n",
    "    \"TA\": \"#00c921\",\n",
    "    \"TricepsSurae\": \"#0066ff\",\n",
    "    \"FDI\": \"#00aeda\",\n",
    "}\n",
    "DEFAULT_COLOR = \"gray\"\n",
    "\n",
    "CORRESPONDANCE_MUSCLE_SPECIAL_CASES = {\n",
    "    \"Fingers\": \"FDI\"\n",
    "}\n",
    "\n",
    "def map_muscle(m):\n",
    "    return CORRESPONDANCE_MUSCLE_SPECIAL_CASES.get(m, m)\n",
    "\n",
    "def parse_condition(cond):\n",
    "    # expects \"Muscle_Intensity\", e.g., \"Quadriceps_10percent\"\n",
    "    if \"_\" in cond:\n",
    "        m, i = cond.split(\"_\", 1)\n",
    "    else:\n",
    "        m, i = cond, \"\"\n",
    "    return map_muscle(m), i\n",
    "\n",
    "def safe_mean_list(x_list):\n",
    "    if x_list is None:\n",
    "        return np.nan\n",
    "    xs = [x for x in x_list if x is not None]\n",
    "    return float(np.nan) if len(xs) == 0 else float(np.nanmean(xs))\n",
    "\n",
    "def first_psd_and_len(data):\n",
    "    # Find one PSD vector to infer NBINS\n",
    "    for subj, dsub in data.items():\n",
    "        for cond, dcond in dsub.items():\n",
    "            psd = dcond.get(\"force_psd\", {}).get(\"each_plateau\", None)  # mean PSD across plateaus\n",
    "            if psd is not None:\n",
    "                psd = np.asarray(psd, dtype=float)\n",
    "                return psd, psd.size\n",
    "    return None, None\n",
    "\n",
    "# Determine frequency bin centers from PSD length\n",
    "psd_sample, NBINS = first_psd_and_len(data)\n",
    "if NBINS is None:\n",
    "    raise RuntimeError(\"No PSD found in data to infer frequency bins.\")\n",
    "dfreq = FMAX_USED / NBINS\n",
    "freq_centers = (np.arange(NBINS) + 0.5) * dfreq  # matches earlier bin-centers style\n",
    "\n",
    "# Intensity ordering\n",
    "INTENSITY_ORDER = [\"10percent\", \"40percent\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 1) CSV: variability per plateau (std & cov)\n",
    "# -----------------------------\n",
    "stats_rows = []\n",
    "subjects = sorted(list(data.keys()))\n",
    "\n",
    "for subj in subjects:\n",
    "    for cond, dcond in data[subj].items():\n",
    "        muscle, intensity = parse_condition(cond)\n",
    "\n",
    "        std_list = dcond.get(\"force_std_each_plateau\", None)                 # list per plateau\n",
    "        cov_list = dcond.get(\"force_coeff_of_var_each_plateau\", None)        # list per plateau\n",
    "\n",
    "        if std_list is None and cov_list is None:\n",
    "            continue\n",
    "\n",
    "        n_plateaus = 0\n",
    "        if isinstance(std_list, (list, tuple)):\n",
    "            n_plateaus = max(n_plateaus, len(std_list))\n",
    "        if isinstance(cov_list, (list, tuple)):\n",
    "            n_plateaus = max(n_plateaus, len(cov_list))\n",
    "\n",
    "        for p_idx in range(n_plateaus):\n",
    "            std_val = None\n",
    "            cov_val = None\n",
    "            if isinstance(std_list, (list, tuple)) and p_idx < len(std_list):\n",
    "                std_val = std_list[p_idx]\n",
    "            if isinstance(cov_list, (list, tuple)) and p_idx < len(cov_list):\n",
    "                cov_val = cov_list[p_idx]\n",
    "\n",
    "            stats_rows.append({\n",
    "                \"subject\": subj,\n",
    "                \"muscle\": muscle,\n",
    "                \"intensity\": intensity,\n",
    "                \"plateau_index\": p_idx + 1,\n",
    "                \"std\": std_val,\n",
    "                \"cov\": cov_val,\n",
    "            })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_rows)\n",
    "df_stats = df_stats.sort_values([\"subject\", \"muscle\", \"intensity\", \"plateau_index\"]).reset_index(drop=True)\n",
    "\n",
    "# Save CSV\n",
    "stats_csv_path = os.path.join(PATH_OF_FILES, \"force_variability_by_plateau.csv\")\n",
    "df_stats.to_csv(stats_csv_path, index=False)\n",
    "print(f\"Saved: {os.path.abspath(stats_csv_path)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) CSV: mean PSD (across plateaus) by condition\n",
    "#     one row per subject Ã— muscle Ã— intensity, columns = frequencies\n",
    "# -----------------------------\n",
    "psd_rows = []\n",
    "for subj in subjects:\n",
    "    for cond, dcond in data[subj].items():\n",
    "        muscle, intensity = parse_condition(cond)\n",
    "        psd_mean = dcond.get(\"force_psd\", {}).get(\"each_plateau\", None)  # mean across plateaus\n",
    "        if psd_mean is None:\n",
    "            continue\n",
    "        psd_mean = np.asarray(psd_mean, dtype=float)\n",
    "        if psd_mean.size != NBINS:\n",
    "            # if binning differs, you could resample; for now, skip to keep rectangular\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"subject\": subj,\n",
    "            \"muscle\": muscle,\n",
    "            \"intensity\": intensity,\n",
    "        }\n",
    "        for k, f in enumerate(freq_centers):\n",
    "            row[f\"f_{f:.2f}Hz\"] = psd_mean[k]\n",
    "        psd_rows.append(row)\n",
    "\n",
    "df_psd = pd.DataFrame(psd_rows)\n",
    "df_psd = df_psd.sort_values([\"subject\", \"muscle\", \"intensity\"]).reset_index(drop=True)\n",
    "\n",
    "psd_csv_path = os.path.join(PATH_OF_FILES, \"force_psd_mean_by_condition.csv\")\n",
    "df_psd.to_csv(psd_csv_path, index=False)\n",
    "print(f\"Saved: {os.path.abspath(psd_csv_path)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) FIGURE: std & cov (4 subplots)\n",
    "# -----------------------------\n",
    "# Intensities present, ordered\n",
    "intensities = sorted(\n",
    "    {parse_condition(c)[1] for subj in subjects for c in data[subj].keys()},\n",
    "    key=lambda x: (INTENSITY_ORDER.index(x) if x in INTENSITY_ORDER else 999, x)\n",
    ")\n",
    "muscles = sorted({parse_condition(c)[0] for subj in subjects for c in data[subj].keys()})\n",
    "\n",
    "# Collect per-subject means (over plateaus) for each metric\n",
    "per_subj_std = defaultdict(lambda: defaultdict(dict))  # [intensity][muscle][subject] = mean std\n",
    "per_subj_cov = defaultdict(lambda: defaultdict(dict))  # [intensity][muscle][subject] = mean cov\n",
    "\n",
    "for subj in subjects:\n",
    "    for cond, dcond in data[subj].items():\n",
    "        muscle, intensity = parse_condition(cond)\n",
    "        std_mean = safe_mean_list(dcond.get(\"force_std_each_plateau\", None))\n",
    "        cov_mean = safe_mean_list(dcond.get(\"force_coeff_of_var_each_plateau\", None))\n",
    "        if not np.isnan(std_mean):\n",
    "            per_subj_std[intensity][muscle][subj] = std_mean\n",
    "        if not np.isnan(cov_mean):\n",
    "            per_subj_cov[intensity][muscle][subj] = cov_mean\n",
    "\n",
    "def gather_vals(metric_dict):\n",
    "    vals = []\n",
    "    for intensity in intensities[:2]:  # expect two\n",
    "        for muscle in muscles:\n",
    "            vals.extend(list(metric_dict[intensity].get(muscle, {}).values()))\n",
    "    vals = np.array(vals, dtype=float)\n",
    "    vals = vals[np.isfinite(vals)]\n",
    "    if vals.size == 0:\n",
    "        return (0.0, 1.0)\n",
    "    lo, hi = float(np.nanmin(vals)), float(np.nanmax(vals))\n",
    "    if lo == hi:\n",
    "        lo, hi = (lo - 0.1, hi + 0.1) if hi == 0 else (0.9*lo, 1.1*hi)\n",
    "    return lo, hi\n",
    "\n",
    "# std_ylim = gather_vals(per_subj_std)\n",
    "# cov_ylim = gather_vals(per_subj_cov)\n",
    "std_ylim = [0, gather_vals(per_subj_std)[1]*1.1]\n",
    "cov_ylim = [0, gather_vals(per_subj_cov)[1]*1.1]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10), sharex=\"col\")\n",
    "for col, intensity in enumerate(intensities[:2]):\n",
    "    # Top: STD\n",
    "    ax = axes[0, col]\n",
    "    xs = np.arange(len(muscles))\n",
    "    for i_m, m in enumerate(muscles):\n",
    "        color = MUSCLE_COLORS.get(m, DEFAULT_COLOR)\n",
    "        vals = list(per_subj_std[intensity].get(m, {}).values())\n",
    "        # jittered subject points\n",
    "        for j, v in enumerate(vals):\n",
    "            ax.scatter(i_m + (j - len(vals)/2) * 0.02, v, s=30, color=color, alpha=0.7)\n",
    "        # diamond = mean per muscle\n",
    "        if len(vals):\n",
    "            ax.scatter(i_m, np.mean(vals), s=140, marker=\"D\", color=color, edgecolor=\"k\", zorder=3)\n",
    "    ax.set_title(f\"STD (mean over plateaus) â€” {intensity}\")\n",
    "    ax.set_ylabel(\"STD (%MVC)\")\n",
    "    ax.set_ylim(std_ylim)\n",
    "    ax.set_xticks(xs)\n",
    "    ax.set_xticklabels(muscles, rotation=0)\n",
    "\n",
    "    # Bottom: COV\n",
    "    ax = axes[1, col]\n",
    "    for i_m, m in enumerate(muscles):\n",
    "        color = MUSCLE_COLORS.get(m, DEFAULT_COLOR)\n",
    "        vals = list(per_subj_cov[intensity].get(m, {}).values())\n",
    "        for j, v in enumerate(vals):\n",
    "            ax.scatter(i_m + (j - len(vals)/2) * 0.02, v, s=30, color=color, alpha=0.7)\n",
    "        if len(vals):\n",
    "            ax.scatter(i_m, np.mean(vals), s=140, marker=\"D\", color=color, edgecolor=\"k\", zorder=3)\n",
    "    ax.set_title(f\"COV (mean over plateaus) â€” {intensity}\")\n",
    "    ax.set_ylabel(\"COV (ratio)\")\n",
    "    ax.set_ylim(cov_ylim)\n",
    "    ax.set_xticks(xs)\n",
    "    ax.set_xticklabels(muscles, rotation=0)\n",
    "\n",
    "fig.suptitle(\"Force variability by muscle and intensity (points = subjects; diamonds = muscle means)\", y=1.02)\n",
    "fig.tight_layout()\n",
    "plt.savefig(f\"{path_of_files}force_variability_by_muscle_and_intensity.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) FIGURE: PSD in dB â€” grid (rows=muscles, cols=intensities)\n",
    "# -----------------------------\n",
    "def collect_psd_db_limits(intensity):\n",
    "    vals = []\n",
    "    for subj in subjects:\n",
    "        for cond, dcond in data[subj].items():\n",
    "            m, inten = parse_condition(cond)\n",
    "            if inten != intensity:\n",
    "                continue\n",
    "            psd_mean = dcond.get(\"force_psd\", {}).get(\"each_plateau\", None)\n",
    "            if psd_mean is None: \n",
    "                continue\n",
    "            arr = 10 * np.log10(np.asarray(psd_mean, dtype=float) + EPS)\n",
    "            vals.append(arr)\n",
    "    if not len(vals):\n",
    "        return (-100, 0)\n",
    "    allv = np.concatenate(vals)\n",
    "    allv = allv[np.isfinite(allv)]\n",
    "    if allv.size == 0:\n",
    "        return (-100, 0)\n",
    "    lo, hi = float(np.nanmin(allv)), float(np.nanmax(allv))\n",
    "    if lo == hi:\n",
    "        lo, hi = lo - 1.0, hi + 1.0\n",
    "    return (lo, hi)\n",
    "\n",
    "n_rows = len(muscles)\n",
    "n_cols = min(2, len(intensities))\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 3*n_rows), sharex=True)\n",
    "\n",
    "if n_rows == 1 and n_cols == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif n_rows == 1:\n",
    "    axes = axes.reshape(1, n_cols)\n",
    "elif n_cols == 1:\n",
    "    axes = axes.reshape(n_rows, 1)\n",
    "\n",
    "# per-column y-lims in dB\n",
    "col_ylims_db = {col: collect_psd_db_limits(intensities[col]) for col in range(n_cols)}\n",
    "\n",
    "for i_m, muscle in enumerate(muscles):\n",
    "    for col, intensity in enumerate(intensities[:n_cols]):\n",
    "        ax = axes[i_m, col]\n",
    "        color = MUSCLE_COLORS.get(muscle, DEFAULT_COLOR)\n",
    "        for subj in subjects:\n",
    "            for cond, dcond in data[subj].items():\n",
    "                m, inten = parse_condition(cond)\n",
    "                if m != muscle or inten != intensity:\n",
    "                    continue\n",
    "                psd_mean = dcond.get(\"force_psd\", {}).get(\"each_plateau\", None)\n",
    "                if psd_mean is None:\n",
    "                    continue\n",
    "                psd_mean = np.asarray(psd_mean, dtype=float)\n",
    "                if psd_mean.size != NBINS:\n",
    "                    continue\n",
    "                ax.plot(freq_centers, 10*np.log10(psd_mean + EPS), color=color, alpha=0.5)\n",
    "        ax.set_title(f\"{muscle} â€” {intensity}\")\n",
    "        ax.set_ylim(col_ylims_db[col])\n",
    "        if i_m == n_rows - 1:\n",
    "            ax.set_xlabel(\"Frequency (Hz)\")\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(\"PSD (dB re (%%MVC)^2/Hz)\")\n",
    "\n",
    "fig.suptitle(\"Mean PSD across plateaus by muscle & intensity (per-subject curves, dB scale)\", y=1.02)\n",
    "fig.tight_layout()\n",
    "plt.savefig(f\"{path_of_files}force_PSD_by_muscle_and_intensity_decibels.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) FIGURE: PSD linear â€” same layout, linear y; per-intensity limits\n",
    "# -----------------------------\n",
    "def collect_psd_lin_limits(intensity):\n",
    "    vals = []\n",
    "    for subj in subjects:\n",
    "        for cond, dcond in data[subj].items():\n",
    "            m, inten = parse_condition(cond)\n",
    "            if inten != intensity:\n",
    "                continue\n",
    "            psd_mean = dcond.get(\"force_psd\", {}).get(\"each_plateau\", None)\n",
    "            if psd_mean is None:\n",
    "                continue\n",
    "            arr = np.asarray(psd_mean, dtype=float)\n",
    "            vals.append(arr)\n",
    "    if not len(vals):\n",
    "        return (0.0, 1.0)\n",
    "    allv = np.concatenate(vals)\n",
    "    allv = allv[np.isfinite(allv)]\n",
    "    if allv.size == 0:\n",
    "        return (0.0, 1.0)\n",
    "    lo, hi = float(np.nanmin(allv)), float(np.nanmax(allv))\n",
    "    if lo == hi:\n",
    "        lo, hi = (lo - 0.1, hi + 0.1) if hi == 0 else (0.9*lo, 1.1*hi)\n",
    "    return (lo, hi)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 3*n_rows), sharex=True)\n",
    "if n_rows == 1 and n_cols == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif n_rows == 1:\n",
    "    axes = axes.reshape(1, n_cols)\n",
    "elif n_cols == 1:\n",
    "    axes = axes.reshape(n_rows, 1)\n",
    "\n",
    "col_ylims_lin = {col: collect_psd_lin_limits(intensities[col]) for col in range(n_cols)}\n",
    "\n",
    "for i_m, muscle in enumerate(muscles):\n",
    "    for col, intensity in enumerate(intensities[:n_cols]):\n",
    "        ax = axes[i_m, col]\n",
    "        color = MUSCLE_COLORS.get(muscle, DEFAULT_COLOR)\n",
    "        for subj in subjects:\n",
    "            for cond, dcond in data[subj].items():\n",
    "                m, inten = parse_condition(cond)\n",
    "                if m != muscle or inten != intensity:\n",
    "                    continue\n",
    "                psd_mean = dcond.get(\"force_psd\", {}).get(\"each_plateau\", None)\n",
    "                if psd_mean is None:\n",
    "                    continue\n",
    "                psd_mean = np.asarray(psd_mean, dtype=float)\n",
    "                if psd_mean.size != NBINS:\n",
    "                    continue\n",
    "                ax.plot(freq_centers, psd_mean, color=color, alpha=0.5)\n",
    "        ax.set_title(f\"{muscle} â€” {intensity}\")\n",
    "        ax.set_ylim(col_ylims_lin[col])\n",
    "        if i_m == n_rows - 1:\n",
    "            ax.set_xlabel(\"Frequency (Hz)\")\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(\"PSD (units^2/Hz)\")\n",
    "\n",
    "fig.suptitle(\"Mean PSD across plateaus by muscle & intensity (per-subject curves, linear scale)\", y=1.02)\n",
    "fig.tight_layout()\n",
    "plt.savefig(f\"{path_of_files}force_PSD_by_muscle_and_intensity_linear.png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to get list of matched CSV files\n",
    "def get_match_csv_files(folder_path, substring):\n",
    "    matching_files = [\n",
    "        os.path.join(folder_path, file)  # Full path\n",
    "        for file in os.listdir(folder_path) \n",
    "        if file.endswith(\".csv\") and substring in file\n",
    "    ]\n",
    "    return matching_files\n",
    "\n",
    "substring = \"match\"\n",
    "match_csv_files = get_match_csv_files(path_of_files, substring)\n",
    "\n",
    "match_files_data = {}\n",
    "\n",
    "# Loop through the matched files\n",
    "for file in match_csv_files:\n",
    "    # Extract just the filename (without the full path)\n",
    "    filename_only = os.path.basename(file)  # Extract filename from path\n",
    "    filename_without_ext = os.path.splitext(filename_only)[0]  # Remove .csv extension\n",
    "    parts = filename_without_ext.split(\"_\")  # Split by \"_\"\n",
    "\n",
    "    if len(parts) >= 3:  # Ensure there are at least three parts (subject, muscle, \"match\")\n",
    "        subject = parts[0]  # First substring before the first \"_\"\n",
    "        muscle = parts[1]   # Second substring between first and second \"_\"\n",
    "    else:\n",
    "        subject, muscle = None, None  # Handle unexpected filename format\n",
    "    \n",
    "    # Load CSV file\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Store column names in a dictionary (filename -> column names)\n",
    "    column_names = df.columns.tolist()\n",
    "\n",
    "    conditions_matched_temp = []\n",
    "    matched_MU_idx = {}\n",
    "    for columni in range(len(column_names)):\n",
    "        if columni > 0:  # Skip first iteration (idx of MUs)\n",
    "            conditions_matched_temp.append(column_names[columni])\n",
    "            matched_MU_idx[column_names[columni]] = df.iloc[:, columni].values\n",
    "\n",
    "    # Store the extracted information\n",
    "    match_files_data[subject] = {}\n",
    "    match_files_data[subject][muscle] = {}\n",
    "    for condition in conditions_matched_temp:\n",
    "        match_files_data[subject][muscle][condition] = {\n",
    "            \"filename\": filename_only,  # Use only filename, not full path\n",
    "            \"MUs_idx_list_arbitrary\": df.iloc[:, 0].values,\n",
    "            \"matched_MUs_idx\": matched_MU_idx[condition] - 1,  # -1 because of 0-indexing\n",
    "            \"full_table\": df\n",
    "        }\n",
    "\n",
    "# # # # # LOOP THROUGH DATA, AND ADD MATCHED INFO, IF ANY # # # # #\n",
    "for current_subject in data.keys():\n",
    "    for current_condition in data[current_subject].keys():\n",
    "        data[current_subject][current_condition]['matched_MUs_idx'] = []  # Initialize as empty list\n",
    "        splitted_condition = current_condition.split('_')\n",
    "        current_muscle = splitted_condition[0]\n",
    "        if len(splitted_condition) > 1:\n",
    "            current_task = splitted_condition[1]\n",
    "            # print(current_subject, current_condition, current_muscle, current_task)\n",
    "            if current_subject in match_files_data:\n",
    "                if current_muscle in match_files_data[current_subject]:\n",
    "                    if current_task in match_files_data[current_subject][current_muscle]:\n",
    "                        data[current_subject][current_condition]['matched_MUs_idx'] = match_files_data[current_subject][current_muscle][current_task]['matched_MUs_idx']\n",
    "        data[current_subject][current_condition]['matched_MUs_idx_global'] = []  # Initialize as empty list\n",
    "        data[current_subject][current_condition]['matched_MUs_idx_local_per_grid'] = {}  # Initialize as empty list\n",
    "        if len(data[current_subject][current_condition]['matched_MUs_idx'])>0:\n",
    "            for mui_matched in data[current_subject][current_condition]['matched_MUs_idx']:\n",
    "                corresponding_grid = (mui_matched // 100) - 1 # -1 because of 0-indexing\n",
    "                corresponding_local_idx = mui_matched % 100 # -1 because of 0-indexing already performed earlier\n",
    "                if corresponding_grid not in data[current_subject][current_condition]['matched_MUs_idx_local_per_grid'].keys():\n",
    "                    data[current_subject][current_condition]['matched_MUs_idx_local_per_grid'][corresponding_grid] = []\n",
    "                data[current_subject][current_condition]['matched_MUs_idx_local_per_grid'][corresponding_grid].append(corresponding_local_idx)\n",
    "                nb_of_MUs_in_all_previous_grids = 0\n",
    "                for gridi in range(corresponding_grid):\n",
    "                    nb_of_MUs_in_all_previous_grids += len(data[current_subject][current_condition]['spike_times'][gridi])\n",
    "                data[current_subject][current_condition]['matched_MUs_idx_global'].append(nb_of_MUs_in_all_previous_grids + corresponding_local_idx)\n",
    "        # convert back to numpy arrays\n",
    "        data[current_subject][current_condition]['matched_MUs_idx_global'] = np.flipud(np.array(data[current_subject][current_condition]['matched_MUs_idx_global']))\n",
    "        for gridi in data[current_subject][current_condition]['matched_MUs_idx_local_per_grid'].keys():\n",
    "            data[current_subject][current_condition]['matched_MUs_idx_local_per_grid'][gridi] = np.array(data[current_subject][current_condition]['matched_MUs_idx_local_per_grid'][gridi])\n",
    "\n",
    "# Update matched_MUs_idx_global based on the kept MUs\n",
    "for current_subject in data.keys():\n",
    "    for current_condition in data[current_subject].keys():\n",
    "        if 'matched_MUs_idx_global' in data[current_subject][current_condition]:\n",
    "            # Retrieve the mask for valid (kept) MUs\n",
    "            empty_MUs_mask = data[current_subject][current_condition].get('Empty_MUs_removed_idx', None)\n",
    "\n",
    "            if empty_MUs_mask is not None:\n",
    "                # Get the indices of MUs that were kept\n",
    "                valid_indices = np.where(empty_MUs_mask == 0)[0]  # Extract indices of kept MUs\n",
    "                \n",
    "                # Update matched_MUs_idx_global by mapping old indices to new ones\n",
    "                updated_matched_indices = []\n",
    "                for old_idx in data[current_subject][current_condition]['matched_MUs_idx_global']:\n",
    "                    if old_idx in valid_indices:\n",
    "                        new_idx = np.where(valid_indices == old_idx)[0][0]  # Find new index\n",
    "                        updated_matched_indices.append(new_idx)\n",
    "\n",
    "                # Convert back to numpy array and store the updated indices\n",
    "                data[current_subject][current_condition]['matched_MUs_idx_global'] = np.array(updated_matched_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate surrogate signal function (used to estimate the dimensionality obtained FA)\n",
    "# Create surrogate random signal\n",
    "# Only for the first pool\n",
    "\n",
    "def Create_shuffled_surrogate_spike_trains(spike_times_list, signal_length, perform_preprocessing = True, nb_shuffle_iter=1, sanity_check_plot = False):\n",
    "    taper_duration = len(HanningW) / input_fsamp  # Duration of the taper (artefact removals) in seconds\n",
    "    smoothed_shuffled_firing_rates = []\n",
    "    print(f'Creating {nb_shuffle_iter} surrogate shuffled signal(s)')\n",
    "    for shuffli in range(nb_shuffle_iter):\n",
    "        print('     Shuffling iteration ' + str(shuffli) + '...')\n",
    "        # initialize the output array\n",
    "        smoothed_shuffled_firing_rates.append(np.zeros((len(spike_times_list),signal_length)) )\n",
    "        for mni in range(len(spike_times_list)):\n",
    "            shuffled_ISI_temp = np.random.permutation(np.diff(spike_times_list[mni])) # list of random ISIs (one list per motoneuron)\n",
    "            shuffled_spike_times_temp = np.cumsum(shuffled_ISI_temp)\n",
    "            shuffled_binary_spike_train_temp = np.zeros(signal_length)\n",
    "            shuffled_binary_spike_train_temp[shuffled_spike_times_temp.astype(int)] = 1\n",
    "            # Smooth using filtfilt\n",
    "            shuffled_binary_firing_rate_temp = filtfilt(HanningW, 1, shuffled_binary_spike_train_temp * input_fsamp)\n",
    "            # Compute the median of the signal\n",
    "            signal_median = np.median(shuffled_binary_firing_rate_temp)\n",
    "            # Define taper length in samples\n",
    "            taper_samples = int(taper_duration * input_fsamp)\n",
    "            # Create a Hanning window taper\n",
    "            taper_window = windows.hann(taper_samples * 2)  # Symmetric taper\n",
    "            # Apply taper to the beginning\n",
    "            shuffled_binary_firing_rate_temp[:taper_samples] *= taper_window[:taper_samples]\n",
    "            shuffled_binary_firing_rate_temp[:taper_samples] += (1 - taper_window[:taper_samples]) * signal_median  # Blend with median\n",
    "            # Apply taper to the end\n",
    "            shuffled_binary_firing_rate_temp[-taper_samples:] *= taper_window[-taper_samples:]\n",
    "            shuffled_binary_firing_rate_temp[-taper_samples:] += (1 - taper_window[-taper_samples:]) * signal_median  # Blend with median\n",
    "            if perform_preprocessing:\n",
    "                shuffled_binary_firing_rate_temp = detrend(shuffled_binary_firing_rate_temp)\n",
    "                shuffled_binary_firing_rate_temp = shuffled_binary_firing_rate_temp - np.mean(shuffled_binary_firing_rate_temp)\n",
    "                if np.std(shuffled_binary_firing_rate_temp) > 0.001: # avoid division by zero\n",
    "                    shuffled_binary_firing_rate_temp = shuffled_binary_firing_rate_temp / np.std(shuffled_binary_firing_rate_temp)\n",
    "            # Store the final smoothed and tapered signal\n",
    "            smoothed_shuffled_firing_rates[-1][mni, :] = shuffled_binary_firing_rate_temp\n",
    "            \n",
    "    sanity_check_plot = False\n",
    "    if sanity_check_plot:\n",
    "        # PLOT - smoothed firing rates from first spike times shuffling iteration\n",
    "        plt.figure(figsize=(12,5))\n",
    "        previous_muscle_of_MN = \"\"\n",
    "        for mn_i in range(len(spike_times_list)):\n",
    "            muscle_of_MN = data[current_subject][current_condition]['MU_muscle_list'][mn_i]\n",
    "            color_of_plot = globals()[f\"{muscle_of_MN}_color\"]\n",
    "            if muscle_of_MN != previous_muscle_of_MN:\n",
    "                plt.plot(np.arange(signal_length)/input_fsamp, smoothed_shuffled_firing_rates[0][mn_i,:], color=color_of_plot, alpha = 0.5, linewidth = 1.5,\n",
    "                        label = f\"{muscle_of_MN}\")\n",
    "            else:\n",
    "                plt.plot(np.arange(signal_length)/input_fsamp, smoothed_shuffled_firing_rates[0][mn_i,:], color=color_of_plot, alpha = 0.1, linewidth = 1.5)\n",
    "            previous_muscle_of_MN = muscle_of_MN\n",
    "        plt.xlabel(\"Second\")\n",
    "        plt.ylabel(\"Firing rates\")\n",
    "        plt.title(f\"Smoothed firing rates of surrogate, shuffled spike trains (1st shuffling iteration)\\nSubject {current_subject} - Condition {current_condition}\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "    return smoothed_shuffled_firing_rates\n",
    "\n",
    "\n",
    "def Get_Rsquared_from_factor_analysis(firing_rates_array, max_nb_of_factors=6):\n",
    "    # Find columns that contain NaN or infinite values\n",
    "    bad_columns = np.where(np.logical_or(np.isnan(firing_rates_array), np.isinf(firing_rates_array)).any(axis=0))[0]\n",
    "\n",
    "    if bad_columns.size > 0:\n",
    "        print(f\"Warning: Removing columns with NaN or infinite values at indices: {bad_columns.tolist()}\")\n",
    "        firing_rates_array = np.delete(firing_rates_array, bad_columns, axis=1)\n",
    "\n",
    "    # Check for columns with near-zero variance\n",
    "    stds = np.std(firing_rates_array, axis=0)\n",
    "    zero_std_columns = np.where(stds < 1e-10)[0]\n",
    "    if zero_std_columns.size > 0:\n",
    "        print(f\"Warning: Removing columns with near-zero variance at indices: {zero_std_columns.tolist()}\")\n",
    "        firing_rates_array = np.delete(firing_rates_array, zero_std_columns, axis=1)\n",
    "\n",
    "    # Initialize storage for results\n",
    "    loadings = []\n",
    "    scores = []\n",
    "    reconstructed_Rsquared = []\n",
    "\n",
    "    for factori in range(max_nb_of_factors - 1):\n",
    "        fa = FactorAnalyzer(n_factors=factori + 1, rotation='promax')\n",
    "        fa.fit(firing_rates_array)\n",
    "        loadings.append(fa.loadings_)  # weights (coefficients) for each MU\n",
    "        scores.append(fa.transform(firing_rates_array))  # The latent factors\n",
    "\n",
    "        # Get RÂ² value of FA's data reconstruction\n",
    "        reconstructed_data_temp = np.dot(loadings[factori], scores[factori].T)\n",
    "        Rsquared_per_MN_temp = []\n",
    "        \n",
    "        for mni in range(firing_rates_array.shape[1]):\n",
    "            r = np.corrcoef(firing_rates_array[:, mni], reconstructed_data_temp[mni])[0, 1]\n",
    "            Rsquared_per_MN_temp.append(r ** 2)  # Compute RÂ² as rÂ²\n",
    "        \n",
    "        reconstructed_Rsquared.append(np.mean(Rsquared_per_MN_temp))\n",
    "\n",
    "    # Add 0 at the beginning of the array\n",
    "    reconstructed_Rsquared = np.insert(reconstructed_Rsquared, 0, 0.0)\n",
    "\n",
    "    return reconstructed_Rsquared, loadings, scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_factor_loadings(loadings, matched_idx, ax):\n",
    "    \"\"\"\n",
    "    Plots factor loadings as:\n",
    "    - A bar plot if n_factors == 1\n",
    "    - A 2D biplot if n_factors == 2\n",
    "    - A 3D biplot if n_factors == 3\n",
    "    - No plot if n_factors < 1 or > 3\n",
    "\n",
    "    Parameters:\n",
    "    - loadings: (MxN) array of factor loadings.\n",
    "    \"\"\"\n",
    "\n",
    "    n_factors = loadings.shape[1]  # Number of selected factors\n",
    "\n",
    "    # Check if the number of factors is valid\n",
    "    if n_factors < 1 or n_factors > 3:\n",
    "        print(f\"Factor Analysis selected {n_factors} factors. A biplot cannot be displayed.\")\n",
    "        return\n",
    "\n",
    "    # plt.figure(figsize=(8, 8))\n",
    "\n",
    "    if n_factors == 1:\n",
    "        # **1 Factor â†’ Bar Plot**\n",
    "        ax.bar(range(loadings.shape[0]), loadings[:, 0], color=\"red\", alpha=0.7)\n",
    "        for i in range(loadings.shape[0]):\n",
    "            if i in matched_idx:\n",
    "                ax.bar(i, loadings[i, 0], color=\"black\", alpha=0, edgecolor=\"black\")\n",
    "        ax.set_xlabel(\"Motor units (global indices)\")\n",
    "        ax.set_ylabel(\"Factor Loadings (weights)\")\n",
    "        ax.set_xticks(range(loadings.shape[0]), [f\"MU #{i}\" for i in range(loadings.shape[0])], rotation=45)\n",
    "        # ax.grid()\n",
    "\n",
    "    elif n_factors == 2:\n",
    "        # **2 Factors â†’ 2D Biplot**\n",
    "        factor_1, factor_2 = loadings[:, 0], loadings[:, 1]\n",
    "\n",
    "        # Compute Colors Based on Factor Loadings (RGB from absolute values)\n",
    "        abs_loadings = np.abs(loadings)\n",
    "        normalized_loadings = abs_loadings / np.max(abs_loadings, axis=0)\n",
    "        colors = np.clip(normalized_loadings, 0, 1)\n",
    "        zero_column = np.zeros((colors.shape[0], 1))  # Shape (rows, 1)\n",
    "        colors = np.concatenate((colors, zero_column), axis=1)\n",
    "\n",
    "        ax.axhline(0, color='black', linestyle='--', linewidth=1.5)\n",
    "        ax.axvline(0, color='black', linestyle='--', linewidth=1.5)\n",
    "\n",
    "        for i in range(loadings.shape[0]):\n",
    "            color = colors[i]  # Assign color based on factor loadings\n",
    "            if i in matched_idx:\n",
    "                ax.scatter(factor_1[i], factor_2[i], color='orange', s=350, alpha = 0.5)\n",
    "                ax.scatter(factor_1[i], factor_2[i], color='black', s=160)\n",
    "            ax.scatter(factor_1[i], factor_2[i], color=color, s=80)\n",
    "            ax.plot([0, factor_1[i]], [0, factor_2[i]], color=color, alpha=0.7, linewidth=2)  # No arrowhead\n",
    "            # plt.text(factor_1[i], factor_2[i], f\"Var{i+1}\", fontsize=12, ha='right')\n",
    "\n",
    "        ax.grid()\n",
    "        ax.axis(\"equal\")\n",
    "        ax.set_xlabel(\"Latent factor 1\", color = \"red\")\n",
    "        ax.set_ylabel(\"Latent factor 2\", color = \"green\")\n",
    "\n",
    "    elif n_factors == 3:\n",
    "        # **3 Factors â†’ 3D Biplot**\n",
    "        factor_1, factor_2, factor_3 = loadings[:, 0], loadings[:, 1], loadings[:, 2]\n",
    "\n",
    "        # Compute Colors Based on Factor Loadings (RGB from absolute values)\n",
    "        abs_loadings = np.abs(loadings)\n",
    "        normalized_loadings = abs_loadings / np.max(abs_loadings, axis=0)\n",
    "        colors = np.clip(normalized_loadings, 0, 1)\n",
    "\n",
    "        # # fig = plt.figure(figsize=(10, 8))\n",
    "        # ax.remove()  # Remove the existing 2D subplot\n",
    "        # ax = fig.add_subplot(axes, projection='3d')  # Convert the existing subplot to 3D\n",
    "\n",
    "        ax.set_xlim([-1, 1])\n",
    "        ax.set_ylim([-1, 1])\n",
    "        ax.set_zlim([-1, 1])\n",
    "\n",
    "        for i in range(loadings.shape[0]):\n",
    "            color = colors[i]  # Assign color based on factor loadings\n",
    "            ax.plot([0, factor_1[i]], [0, factor_2[i]], [0, factor_3[i]], color=color, alpha=0.7, linewidth=1.5)\n",
    "            if i in matched_idx:\n",
    "                ax.scatter(factor_1[i], factor_2[i], color='orange', s=350, alpha = 0.5)\n",
    "                ax.scatter(factor_1[i], factor_2[i], color='black', s=160)\n",
    "            ax.scatter(factor_1[i], factor_2[i], factor_3[i], color=color, s=80, edgecolors='black')\n",
    "\n",
    "            # Projected Shadows\n",
    "            shadow_alpha = 0.5\n",
    "            # shadow of factors 1 and 2\n",
    "            ax.plot([0, factor_1[i]], [0, factor_2[i]], [-1], color=([0,0,.5] + np.array([1, 1, 1])) / 2, alpha=shadow_alpha, linewidth=1.5)\n",
    "            ax.scatter(factor_1[i], factor_2[i], -1, color=([0,0,.5] + np.array([1, 1, 1])) / 2, s=80, alpha=shadow_alpha, edgecolors='none')\n",
    "            # shadow of factors 2 and 3\n",
    "            ax.plot([0, factor_1[i]], [1], [0, factor_3[i]], color=([.5,0,0] + np.array([1, 1, 1])) / 2, alpha=shadow_alpha, linewidth=1.5)\n",
    "            ax.scatter(factor_1[i], 1, factor_3[i], color=([.5,0,0] + np.array([1, 1, 1])) / 2, s=80, alpha=shadow_alpha, edgecolors='none')\n",
    "            # shadow of factors 1 and 3\n",
    "            ax.plot([-1], [0, factor_2[i]], [0, factor_3[i]], color=([0,.5,0] + np.array([1, 1, 1])) / 2, alpha=shadow_alpha, linewidth=1.5)\n",
    "            ax.scatter(-1, factor_2[i], factor_3[i], color=([0,.5,0] + np.array([1, 1, 1])) / 2, s=80, alpha=shadow_alpha, edgecolors='none')\n",
    "\n",
    "            # ax.text(factor_1[i], factor_2[i], factor_3[i], f\"Var{i+1}\", fontsize=10)\n",
    "\n",
    "        ax.set_xlabel(\"Latent factor 1\", color = \"red\")\n",
    "        ax.set_ylabel(\"Latent factor 2\", color = \"green\")\n",
    "        ax.set_zlabel(\"Latent factor 3\", color = \"blue\")\n",
    "        # ax = fig.add_subplot(111, projection='3d')\n",
    "        # set_axes_equal(ax)\n",
    "\n",
    "        # Reduce background grid size\n",
    "        ax.xaxis.set_major_locator(ticker.MaxNLocator(4))\n",
    "        ax.yaxis.set_major_locator(ticker.MaxNLocator(4))\n",
    "        ax.zaxis.set_major_locator(ticker.MaxNLocator(4))\n",
    "        ax.set_box_aspect([1,1,1])\n",
    "\n",
    "    # if display_figures_inline:\n",
    "    #     plt.show()\n",
    "    # else:\n",
    "    #     plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint to clean up some space in the memory\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PERFORM DIMENSIONALITY ANALYSIS\n",
    "\n",
    "# --------------------------------------\n",
    "# User-defined parameter for the analysis\n",
    "# --------------------------------------\n",
    "# max_nb_of_factors = 6  # e.g., 6, but can be anything you need\n",
    "# surrogate_iter_nb = 1\n",
    "\n",
    "# ------------------------------\n",
    "# PERFORM DIMENSIONALITY ANALYSIS\n",
    "# ------------------------------\n",
    "if perform_analysis_dimensionality:\n",
    "    print(\"Starting dimensionality analysis...\")\n",
    "\n",
    "    # --------------------------------------\n",
    "    # Build dynamic fieldnames, depending on max_nb_of_factors\n",
    "    # --------------------------------------\n",
    "    fieldnames = [\"subject\", \"condition\", \"muscles\"]\n",
    "    for factor_i in range(1, max_nb_of_factors+1):\n",
    "        fieldnames.append(f\"R2_f{factor_i}\")\n",
    "    fieldnames += [\n",
    "        \"selected_nb_of_dimensions\",\n",
    "        \"nb_mus\",\n",
    "        \"avg_slope_surrogate_R2\"\n",
    "    ]\n",
    "\n",
    "    # This will collect all rows (from all subjects & conditions) so that\n",
    "    # we can write one \"big CSV\" at the end.\n",
    "    all_dimensionality_results = []\n",
    "\n",
    "    for current_subject in data.keys():\n",
    "        for current_condition in data[current_subject].keys():\n",
    "            data[current_subject][current_condition]['FA_results'] = {}\n",
    "\n",
    "            # Create folder to save output (per subject, per condition)\n",
    "            folder_subject_condition = os.path.join(\n",
    "                path_of_files,\n",
    "                current_subject,\n",
    "                f'{current_subject}_{current_condition}'\n",
    "            )\n",
    "            if not os.path.exists(folder_subject_condition):\n",
    "                os.makedirs(folder_subject_condition)\n",
    "\n",
    "            print(f\"     Dimensionality analysis for {current_subject} - {current_condition}\")\n",
    "\n",
    "            # Overwrite local CSV => remove if it already exists\n",
    "            csv_path = os.path.join(folder_subject_condition, \"Dimensionality_analysis_results.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                os.remove(csv_path)\n",
    "\n",
    "            valid_MUs = data[current_subject][current_condition]['valid_MUs_given_selected_plateaus']\n",
    "            muscles_unique = list(set(data[current_subject][current_condition]['MU_muscle_list']))\n",
    "            muscle_pairs = list(itertools.combinations(muscles_unique, 2))\n",
    "\n",
    "            # Build possible muscle groups (single muscles + pairs)\n",
    "            muscle_groups_for_FA = {}\n",
    "            if fa_on_first_pool_only_for_sim and (('sim' in current_subject) or ('sim' in current_condition)): # factor analysis on first pool only\n",
    "                muscle_groups_for_FA[0] = muscles_unique[0]\n",
    "            else:\n",
    "                iter_muscle_group_FA = 0\n",
    "                for muscle_i in muscles_unique:\n",
    "                    muscle_groups_for_FA[iter_muscle_group_FA] = muscle_i\n",
    "                    iter_muscle_group_FA += 1\n",
    "                for muscle_pair_i in muscle_pairs:\n",
    "                    muscle_groups_for_FA[iter_muscle_group_FA] = [muscle_pair_i[0], muscle_pair_i[1]]\n",
    "                    iter_muscle_group_FA += 1\n",
    "                \n",
    "            # Prepare the figure\n",
    "            plt.figure(figsize=(8,5))\n",
    "\n",
    "            muscle_iter = -1\n",
    "            for muscle_current_FA in muscle_groups_for_FA.values():\n",
    "                if type(muscle_current_FA) == list:\n",
    "                    muscle_current_FA = tuple(muscle_current_FA) # convert to tuple so that it can be used as key for a dict\n",
    "\n",
    "                data[current_subject][current_condition]['FA_results'][muscle_current_FA] = {}\n",
    "                muscle_iter += 1\n",
    "\n",
    "                MU_muscle_list = data[current_subject][current_condition]['MU_muscle_list']\n",
    "                # Valid MUs only\n",
    "                MU_idx_for_FA_only_valid_MUs = [MU_muscle_list[i] for i in valid_MUs]\n",
    "\n",
    "                # Identify the relevant MU indices\n",
    "                if isinstance(muscle_current_FA, str):\n",
    "                    # Single muscle\n",
    "                    muscle_corresponding_MUs = [\n",
    "                        i for i, val in enumerate(MU_muscle_list)\n",
    "                        if val == muscle_current_FA\n",
    "                    ]\n",
    "                    muscle_corresponding_MUs_only_valid_MUs = [\n",
    "                        i for i, val in enumerate(MU_idx_for_FA_only_valid_MUs)\n",
    "                        if val == muscle_current_FA\n",
    "                    ]\n",
    "                else:\n",
    "                    # List of muscles (pair)\n",
    "                    muscle_corresponding_MUs = np.where(\n",
    "                        np.isin(MU_muscle_list, muscle_current_FA)\n",
    "                    )[0]\n",
    "                    muscle_corresponding_MUs_only_valid_MUs = np.where(\n",
    "                        np.isin(MU_idx_for_FA_only_valid_MUs, muscle_current_FA)\n",
    "                    )[0]\n",
    "\n",
    "                MU_idx_for_FA = np.intersect1d(muscle_corresponding_MUs, valid_MUs)\n",
    "                if len(MU_idx_for_FA) < max_nb_of_factors+1:\n",
    "                    temp_max_nb_of_factors = len(MU_idx_for_FA) - 1\n",
    "                else:\n",
    "                    temp_max_nb_of_factors = max_nb_of_factors\n",
    "\n",
    "                # --------------------------------------------------\n",
    "                # Surrogate data creation\n",
    "                # --------------------------------------------------\n",
    "                filtered_spike_times_for_surrogate_signal = [\n",
    "                    data[current_subject][current_condition]['spike_times_during_selected_plateaus'][i]\n",
    "                    for i in muscle_corresponding_MUs_only_valid_MUs\n",
    "                ]\n",
    "                surrogate_dataset = Create_shuffled_surrogate_spike_trains(\n",
    "                    filtered_spike_times_for_surrogate_signal,\n",
    "                    len(data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'][0]),\n",
    "                    perform_preprocessing=True,\n",
    "                    nb_shuffle_iter=surrogate_iter_nb,\n",
    "                    sanity_check_plot=False\n",
    "                )\n",
    "\n",
    "                # --------------------------------------------------\n",
    "                # True data RÂ²\n",
    "                # --------------------------------------------------\n",
    "                true_data_R2 = np.full(max_nb_of_factors,np.nan)\n",
    "                temp_rsqured, fa_coef_for_each_MUs, latent_factors = Get_Rsquared_from_factor_analysis(\n",
    "                    data[current_subject][current_condition]['preprocessed_smoothed_firing_rates_during_selected_plateaus'][MU_idx_for_FA,:].T,\n",
    "                    max_nb_of_factors=temp_max_nb_of_factors\n",
    "                )\n",
    "                # fa_coef_for_each_MUs =\n",
    "                # list of N elements, with N being the number of factors to try\n",
    "                # each element is a MxF matrix, with M being the number of motor units and F being the number of factors\n",
    "                true_data_R2[:len(temp_rsqured)] = temp_rsqured\n",
    "                slope_true_R2 = np.gradient(true_data_R2)\n",
    "\n",
    "                # --------------------------------------------------\n",
    "                # Surrogate data RÂ²\n",
    "                # --------------------------------------------------\n",
    "                surrogate_data_R2_list = []\n",
    "                for shuffle_i in range(surrogate_iter_nb):\n",
    "                    surrogate_data_R2_list.append(np.full(max_nb_of_factors,np.nan))\n",
    "                    temp_rsqured, _, _ = Get_Rsquared_from_factor_analysis(\n",
    "                            surrogate_dataset[shuffle_i].T,\n",
    "                            max_nb_of_factors=temp_max_nb_of_factors)\n",
    "                    surrogate_data_R2_list[-1][:len(temp_rsqured)] = temp_rsqured\n",
    "                surrogate_data_R2 = np.nanmean(np.array(surrogate_data_R2_list).T, axis=1)\n",
    "                slope_surrogate_R2 = np.nanmin(np.gradient(surrogate_data_R2))\n",
    "\n",
    "                # --------------------------------------------------\n",
    "                # Dimensionality threshold\n",
    "                # --------------------------------------------------\n",
    "                # We find first factor index 'N' where slope_true_R2 < surrogate_data_R2 => dimension = N - 1\n",
    "                # Note: ensure that the index is found; otherwise, handle edge cases.\n",
    "                temp_bool_dim = np.where(slope_true_R2 < slope_surrogate_R2)[0]\n",
    "                add_to_legend_temp = f\"\"\n",
    "                if len(temp_bool_dim) >= 1:\n",
    "                    selected_nb_of_dimensions = temp_bool_dim[0] - 1\n",
    "                else:\n",
    "                    # Case 1 = not enough MUs\n",
    "                    if np.all(np.isnan(slope_true_R2)):\n",
    "                        selected_nb_of_dimensions = 0\n",
    "                    elif np.sum(np.sum(np.isnan(slope_true_R2)))>=1:\n",
    "                        nan_slope_idx = np.where(np.isnan(slope_true_R2))[0]\n",
    "                        # find the first nan slope value. Dimensionality = index of this value - 1\n",
    "                        # Unless there is only one slope value. Then, it is the first index\n",
    "                        if nan_slope_idx[0]==1:\n",
    "                            selected_nb_of_dimensions = 1\n",
    "                        else:\n",
    "                            selected_nb_of_dimensions = nan_slope_idx[0]-1\n",
    "                    # Case 2 = True RÂ² slope never goes below surrogate RÂ² slope\n",
    "                    else:\n",
    "                        selected_nb_of_dimensions = temp_max_nb_of_factors-1\n",
    "                        add_to_legend_temp = f\"\\nWarning: True slope is never < surrogate slope\"\n",
    "\n",
    "                # Assign values of FA results for later use\n",
    "                if selected_nb_of_dimensions == 0: # ensuring the script still works in this edge case (typicallhy when not enough MUs to perform FA)\n",
    "                    fa_coef_for_each_MUs = {}\n",
    "                    fa_coef_for_each_MUs[-1] = np.ones((len(MU_idx_for_FA),1))\n",
    "                data[current_subject][current_condition]['FA_results'][muscle_current_FA]['fa_weights_for_each_MUs'] = fa_coef_for_each_MUs[selected_nb_of_dimensions-1]\n",
    "                data[current_subject][current_condition]['FA_results'][muscle_current_FA]['dimensionality'] = selected_nb_of_dimensions\n",
    "                data[current_subject][current_condition]['FA_results'][muscle_current_FA]['MU_idx_for_FA'] = MU_idx_for_FA\n",
    "\n",
    "                # Get similarity matrix between MUs according to their fa_weights\n",
    "                # Compute all pairwise dot products\n",
    "                latent_factor_space_vectors_dot_products = np.dot(fa_coef_for_each_MUs[selected_nb_of_dimensions-1],\n",
    "                    fa_coef_for_each_MUs[selected_nb_of_dimensions-1].T)  # Compute full pairwise dot-product matrix\n",
    "                # latent_factor_space_vectors_dot_products = np.where(np.tril(latent_factor_space_vectors_dot_products) != 0, np.tril(latent_factor_space_vectors_dot_products), np.nan)\n",
    "                data[current_subject][current_condition]['FA_results'][muscle_current_FA]['latent_factor_space_vectors_dot_products'] = latent_factor_space_vectors_dot_products\n",
    "\n",
    "                # --------------------------------------------------\n",
    "                # Plot\n",
    "                # --------------------------------------------------\n",
    "                #       # Plot of RÂ²\n",
    "                if isinstance(muscle_current_FA, str):\n",
    "                    figure_color = globals()[f\"{muscle_current_FA}_color\"]\n",
    "                    muscle_label = muscle_current_FA\n",
    "                else:\n",
    "                    figure_color = mix_colors(\n",
    "                        globals()[f\"{muscle_current_FA[0]}_color\"],\n",
    "                        globals()[f\"{muscle_current_FA[1]}_color\"]\n",
    "                    )\n",
    "                    muscle_label = str(muscle_current_FA)  # or \" & \".join(muscle_current_FA)\n",
    "                if (\",\" in muscle_label) and display_Rsquared_for_muscle_pair==False:\n",
    "                    print(\"Not displaying RÂ² plot for synergist muscles combination\")\n",
    "                else:\n",
    "                    # Just an empty line to make space in the legend\n",
    "                    plt.plot(0,0,color=\"white\",label=\" \\n \")\n",
    "                    plt.plot(\n",
    "                        true_data_R2,\n",
    "                        label=f\"{muscle_label} - True ({len(MU_idx_for_FA)} MUs)\",\n",
    "                        color=figure_color,\n",
    "                        linewidth=3,\n",
    "                        alpha=0.7\n",
    "                    )\n",
    "                    plt.plot(\n",
    "                        surrogate_data_R2,\n",
    "                        label=f\"{muscle_label} - Surrogate\",\n",
    "                        linestyle='--',\n",
    "                        color=figure_color,\n",
    "                        alpha=1\n",
    "                    )\n",
    "                    plt.scatter(\n",
    "                        selected_nb_of_dimensions,\n",
    "                        true_data_R2[selected_nb_of_dimensions],\n",
    "                        s=150, edgecolors=figure_color, facecolors=\"white\",\n",
    "                        linewidth=3, zorder=10,\n",
    "                        label=f\"{muscle_label} - {selected_nb_of_dimensions}\\ndimensions selected{add_to_legend_temp}\",\n",
    "                    )\n",
    "\n",
    "\n",
    "                # ------------------------------------------------------\n",
    "                # Prepare the row for the local CSV and big CSV\n",
    "                # ------------------------------------------------------\n",
    "                row = {\n",
    "                    \"subject\":       current_subject,\n",
    "                    \"condition\":     current_condition,\n",
    "                    \"muscles\":      muscle_label,\n",
    "                    \"selected_nb_of_dimensions\": selected_nb_of_dimensions,\n",
    "                    \"nb_mus\":        len(MU_idx_for_FA),\n",
    "                    \"avg_slope_surrogate_R2\": slope_surrogate_R2\n",
    "                }\n",
    "                # Add the RÂ² values for each factor\n",
    "                for factor_i in range(1, max_nb_of_factors+1):\n",
    "                    row[f\"R2_f{factor_i}\"] = true_data_R2[factor_i - 1]\n",
    "\n",
    "                # Write to the local CSV (for this subject + condition)\n",
    "                append_to_csv(csv_path, row, fieldnames)\n",
    "\n",
    "                # Also add to our global list for the big CSV\n",
    "                all_dimensionality_results.append(row)\n",
    "\n",
    "            # End muscle_groups_for_FA loop\n",
    "\n",
    "            # Save figure\n",
    "            plt.legend(title=\"Visually, dimensionality seems\\nunderestimated when the number\\nof MUs is low\",\n",
    "                       loc=\"upper right\", bbox_to_anchor=(1.5, 1))\n",
    "            plt.xlabel(\"Nb of factors\")\n",
    "            plt.ylabel(\"RÂ²\")\n",
    "            plt.ylim([0,1])\n",
    "            plt.title(\n",
    "                f\"Dimensionality analysis (over \"\n",
    "                f\"{data[current_subject][current_condition]['preprocessed_smoothed_firing_rates_during_selected_plateaus'][MU_idx_for_FA, :].shape[1]/input_fsamp:.1f} s)\\n\"\n",
    "                f\"Subject {current_subject} - Condition {current_condition}\"\n",
    "            )\n",
    "            if os.path.exists(folder_subject_condition):\n",
    "                plt.savefig(\n",
    "                    os.path.join(folder_subject_condition, \"Dimensionality_analysis_results.png\"),\n",
    "                    bbox_inches='tight'\n",
    "                )\n",
    "            else:\n",
    "                print(\"Output folder does not exist; figure not saved\")\n",
    "\n",
    "            # End of \"for each muscle or muscle pair\"\n",
    "            if display_figures_inline:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "\n",
    "            # # # # # # Plot of the \"latent factor space\" and also save results for only matched MUs\n",
    "            # Create figure and subplots\n",
    "            fig, axes = plt.subplots(nrows=len(muscle_groups_for_FA), ncols=2, figsize=(16, 8*len(muscle_groups_for_FA))) # Ensure axes is always iterable (even if there's only one subplot)\n",
    "\n",
    "            if len(muscle_groups_for_FA) == 1:\n",
    "                axes = [axes]\n",
    "            for muscle_iter, (muscle_FA_i, ax_pair) in enumerate(zip(data[current_subject][current_condition]['FA_results'].keys(), axes)):\n",
    "                ax1, ax2 = ax_pair  # First column = FA plot, Second column = heatmap\n",
    "                # -------------------\n",
    "                # First Column: FA Biplot\n",
    "                # -------------------\n",
    "                latent_factor_loadings = data[current_subject][current_condition]['FA_results'][muscle_FA_i]['fa_weights_for_each_MUs']\n",
    "\n",
    "                # Find the indices in MU_idx_for_FA that correspond to matched_MUs\n",
    "                MU_idx_for_FA = np.sort(data[current_subject][current_condition]['FA_results'][muscle_FA_i]['MU_idx_for_FA'])\n",
    "                if 'matched_MUs_idx_global' in data[current_subject][current_condition].keys():\n",
    "                    matched_MUs_temp = np.sort(data[current_subject][current_condition]['matched_MUs_idx_global'])\n",
    "                else:\n",
    "                    matched_MUs_temp = []\n",
    "                matched_MUs_used_for_FA_temp = np.intersect1d(MU_idx_for_FA, matched_MUs_temp)\n",
    "                matched_MUs_used_for_FA_relative_to_matched_idx = np.searchsorted(matched_MUs_temp, matched_MUs_used_for_FA_temp)\n",
    "                matched_MUs_used_for_FA_relative_to_valid_FA_idx = np.searchsorted(MU_idx_for_FA, matched_MUs_used_for_FA_temp)\n",
    "\n",
    "                n_factors = latent_factor_loadings.shape[1]  # Number of latent factors\n",
    "                # If there are exactly 3 factors, convert ax1 into a 3D subplot\n",
    "                if n_factors == 3:\n",
    "                    ax1.remove()  # Remove the existing 2D subplot\n",
    "                    ax1 = fig.add_subplot(len(muscle_groups_for_FA), 2, 2*muscle_iter+1, projection='3d')\n",
    "                plot_factor_loadings(latent_factor_loadings, matched_MUs_temp, ax=ax1)  # Pass the updated ax1\n",
    "                ax1.set_title(f\"Projection into latent factor space for {muscle_FA_i}\\nMatched MUs highlighted\")\n",
    "                # -------------------\n",
    "                # Second Column: Heatmap of Dot Product Matrix\n",
    "                # -------------------\n",
    "                latent_factor_space_vectors_dot_products = data[current_subject][current_condition]['FA_results'][muscle_FA_i]['latent_factor_space_vectors_dot_products']\n",
    "                heatmap = sns.heatmap(latent_factor_space_vectors_dot_products,\n",
    "                    cmap=\"RdYlBu_r\", mask=np.isnan(latent_factor_space_vectors_dot_products), \n",
    "                    annot=False, linecolor=\"none\", cbar=True,\n",
    "                    ax=ax2, vmin=-1, vmax=1)\n",
    "                ax2.set_title(f\"Pairwise dot products of MUs' positions in latent factor space ({muscle_FA_i})\")\n",
    "                ax2.set_aspect(\"equal\")  # <-- Make the heatmap square\n",
    "                # Add a label to the color bar\n",
    "                cbar = heatmap.collections[0].colorbar\n",
    "                cbar.ax.set_ylabel(\"Dot Product Value\", fontsize=12)\n",
    "\n",
    "            latent_factor_loadings_only_matched = np.full((len(matched_MUs_temp),n_factors), np.nan)\n",
    "            latent_factor_loadings_only_matched[matched_MUs_used_for_FA_relative_to_matched_idx,:] = latent_factor_loadings[matched_MUs_used_for_FA_relative_to_valid_FA_idx,:]\n",
    "\n",
    "            latent_factor_space_vectors_dot_products_only_matched = np.full((len(matched_MUs_temp),len(matched_MUs_temp)), np.nan)\n",
    "            latent_factor_space_vectors_dot_products_only_matched[\n",
    "                np.ix_(matched_MUs_used_for_FA_relative_to_matched_idx, matched_MUs_used_for_FA_relative_to_matched_idx)] = latent_factor_space_vectors_dot_products[\n",
    "                    np.ix_(matched_MUs_used_for_FA_relative_to_valid_FA_idx, matched_MUs_used_for_FA_relative_to_valid_FA_idx)]\n",
    "\n",
    "            data[current_subject][current_condition]['FA_results'][muscle_FA_i]['fa_weights_only_matched_MUs'] = latent_factor_loadings_only_matched\n",
    "            data[current_subject][current_condition]['FA_results'][muscle_FA_i]['latent_factor_space_vectors_dot_products_only_matched_MUs'] = latent_factor_space_vectors_dot_products_only_matched\n",
    "\n",
    "            plt.suptitle(f\"Latent factor space (Subject {current_subject} - Condition {current_condition})\") \n",
    "            plt.tight_layout() \n",
    "            if os.path.exists(folder_subject_condition):\n",
    "                plt.savefig(\n",
    "                        os.path.join(folder_subject_condition, \"Dimensionality_latent_factor_space.png\"),\n",
    "                        bbox_inches='tight')\n",
    "            else:\n",
    "                print(\"Output folder does not exist; figure not saved\")\n",
    "            if display_figures_inline:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "        # End of \"for each condition\"\n",
    "    # End of \"for each subject\"\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Now write ONE big CSV with all rows, saved at 'path_of_files' level\n",
    "    # --------------------------------------------------------------------\n",
    "    big_csv_path = os.path.join(path_of_files, \"Dimensionality_analysis_ALL.csv\")\n",
    "    write_big_csv(big_csv_path, all_dimensionality_results, fieldnames)\n",
    "\n",
    "    print(f\"\\nA concatenated CSV with all dimensionality results was saved at:\\n{big_csv_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"Dimensionality analysis (factor analysis) not performed because 'perform_analysis_dimensionality=False'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check of the factors in latent space according to the selected MUs\n",
    "for current_subject in match_files_data.keys():\n",
    "    for current_muscle in match_files_data[current_subject].keys():\n",
    "        conditions_compared = [current_muscle + \"_\" + key for key in match_files_data[current_subject][current_muscle].keys()]\n",
    "        condition_combinations_list = list(itertools.combinations(conditions_compared, 2))\n",
    "\n",
    "        plt.figure(figsize=(8, 6))  # Create a new figure for each subject & muscle\n",
    "        legend_added = set()  # Track conditions already added to the legend\n",
    "\n",
    "        # Store scatter plot data for each condition\n",
    "        scatter_data = {condition: {\"x\": [], \"y\": [], \"color\": f\"C{idx}\"} for idx, condition in enumerate(conditions_compared)}\n",
    "\n",
    "        for mni in range(len(data[current_subject][conditions_compared[0]]['matched_MUs_idx'])):\n",
    "            conditions_latent_space_MU_pos = {}\n",
    "\n",
    "            for idx, conditioni in enumerate(conditions_compared):\n",
    "                muscle_FA_i = 'FDI' # temporary fix\n",
    "                # Access the correct FA results\n",
    "                fa_data = data[current_subject][conditioni]['FA_results'][muscle_FA_i]['fa_weights_only_matched_MUs']\n",
    "                \n",
    "                # ensure that there are 2 dimensions at least\n",
    "                if fa_data.shape[1]<2:\n",
    "                    continue\n",
    "\n",
    "                # Ensure we don't index out of bounds\n",
    "                if mni >= fa_data.shape[0]: \n",
    "                    continue\n",
    "\n",
    "                # Store latent space positions\n",
    "                conditions_latent_space_MU_pos[conditioni] = np.array([\n",
    "                    fa_data[mni, 0], \n",
    "                    fa_data[mni, 1]\n",
    "                ])\n",
    "\n",
    "                # Collect scatter points per condition (avoiding repeated plt.scatter calls)\n",
    "                if not np.isnan(conditions_latent_space_MU_pos[conditioni]).any():\n",
    "                    scatter_data[conditioni][\"x\"].append(fa_data[mni, 0])\n",
    "                    scatter_data[conditioni][\"y\"].append(fa_data[mni, 1])\n",
    "\n",
    "            # Connect corresponding MUs across conditions\n",
    "            for cond1, cond2 in condition_combinations_list:\n",
    "                if cond1 in conditions_latent_space_MU_pos and cond2 in conditions_latent_space_MU_pos:\n",
    "                    if not np.isnan(conditions_latent_space_MU_pos[cond1]).any() and not np.isnan(conditions_latent_space_MU_pos[cond2]).any():\n",
    "                        plt.plot(\n",
    "                            [conditions_latent_space_MU_pos[cond1][0], conditions_latent_space_MU_pos[cond2][0]],\n",
    "                            [conditions_latent_space_MU_pos[cond1][1], conditions_latent_space_MU_pos[cond2][1]],\n",
    "                            color='purple', alpha=0.5, zorder = 0\n",
    "                        )\n",
    "\n",
    "        # Now, plot each condition **once** with a unique label\n",
    "        for condition, values in scatter_data.items():\n",
    "            if values[\"x\"]:  # Only plot if there are valid points\n",
    "                plt.scatter(values[\"x\"], values[\"y\"], color=values[\"color\"], label=condition)\n",
    "\n",
    "        # Add legend only if there are labeled points\n",
    "        if any(len(values[\"x\"]) > 0 for values in scatter_data.values()):\n",
    "            plt.legend(title=\"Conditions\", loc=\"upper right\")\n",
    "\n",
    "        plt.xlabel(\"Latent Factor 1\")\n",
    "        plt.ylabel(\"Latent Factor 2\")\n",
    "        plt.title(f\"{current_subject}, {current_muscle}\\nComparison of Matched MUs in Latent Space\")\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare recruitment thresholds across matched conditions\n",
    "condition_dodge_visual = 0.1\n",
    "\n",
    "for current_subject in match_files_data.keys():\n",
    "    for current_muscle in match_files_data[current_subject].keys():\n",
    "        conditions_compared = [current_muscle + \"_\" + key for key in match_files_data[current_subject][current_muscle].keys()]\n",
    "        condition_combinations_list = list(itertools.combinations(conditions_compared, 2))\n",
    "\n",
    "        plt.figure(figsize=(15, 5)) \n",
    "\n",
    "        for cond1, cond2 in condition_combinations_list:\n",
    "            # Retrieve matched indices for condition 1\n",
    "            matched_idx_current_cond1 = data[current_subject][cond1]['matched_MUs_idx_local_per_grid']\n",
    "            matched_idx_current_cond2 = data[current_subject][cond2]['matched_MUs_idx_local_per_grid']\n",
    "\n",
    "            recruitment_thresh_cond1 = {}\n",
    "            mean_recruitment_thresh_cond1 = []\n",
    "            # Compute mean recruitment thresholds for ordering in condition 1\n",
    "            for gridi in matched_idx_current_cond1.keys():\n",
    "                recruitment_thresh_cond1[gridi] = []\n",
    "                for mni in matched_idx_current_cond1[gridi]:  # Loop through matched MUs\n",
    "                    current_RTs = data[current_subject][cond1]['recruitment_thresholds'][gridi][mni]['recruitment_thresholds']\n",
    "                    mean_RTs = np.nanmean(current_RTs)  # Compute mean recruitment threshold\n",
    "                    mean_recruitment_thresh_cond1.append((gridi, mni, mean_RTs))  # Store for sorting\n",
    "            # Sort MUs based on their mean recruitment threshold in condition 1\n",
    "            mean_recruitment_thresh_cond1.sort(key=lambda x: x[2])  # Sort by mean RTs\n",
    "            # Build a sorted order list based on condition 1\n",
    "            sorted_indices_cond1 = {gridi: [] for gridi in matched_idx_current_cond1.keys()}\n",
    "            for sorted_index, (gridi, mni, _) in enumerate(mean_recruitment_thresh_cond1):\n",
    "                sorted_indices_cond1[gridi].append(mni)  # Save sorted order per grid\n",
    "            \n",
    "            # Create a mapping for `cond2` to ensure we fetch correct indices\n",
    "            sorted_indices_cond2 = {gridi: [] for gridi in matched_idx_current_cond2.keys()}\n",
    "            for gridi in sorted_indices_cond1.keys():\n",
    "                sorted_indices_cond2[gridi] = [\n",
    "                    matched_idx_current_cond2[gridi][\n",
    "                        np.where(matched_idx_current_cond1[gridi] == mni)[0][0]\n",
    "                    ] if mni in matched_idx_current_cond1[gridi] else None  # Map only existing indices\n",
    "                    for mni in sorted_indices_cond1[gridi] ]\n",
    "\n",
    "            # Plot recruitment thresholds for condition 1\n",
    "            matched_idx_iter_accumulator = -1\n",
    "            mean_RTs_cond1 = []\n",
    "            for gridi in sorted_indices_cond1.keys():\n",
    "                for mni in sorted_indices_cond1[gridi]:  # Use sorted indices\n",
    "                    matched_idx_iter_accumulator += 1\n",
    "                    current_RTs = data[current_subject][cond1]['recruitment_thresholds'][gridi][mni]['recruitment_thresholds']\n",
    "                    mean_RTs_cond1.append(np.nanmean(current_RTs))\n",
    "                    plt.scatter(x=np.ones_like(current_RTs) * matched_idx_iter_accumulator - condition_dodge_visual, y=current_RTs, s=50, color=\"C0\", alpha=0.5)\n",
    "                    plt.scatter(x=matched_idx_iter_accumulator - condition_dodge_visual, y=mean_RTs_cond1[-1], s=100, color=\"C0\", alpha=1, edgecolors='black')\n",
    "            plt.plot(np.arange(len(sorted_indices_cond1[gridi]))-condition_dodge_visual, mean_RTs_cond1, color=\"C0\", alpha=0.5, linewidth=2, label = cond1)\n",
    "\n",
    "            recruitment_thresh_cond2 = {}\n",
    "            mean_recruitment_thresh_cond2 = []\n",
    "            # Compute mean recruitment thresholds for ordering in condition 2\n",
    "            for gridi in matched_idx_current_cond2.keys():\n",
    "                recruitment_thresh_cond2[gridi] = []\n",
    "                for mni in matched_idx_current_cond2[gridi]:  # Loop through matched MUs\n",
    "                    current_RTs = data[current_subject][cond2]['recruitment_thresholds'][gridi][mni]['recruitment_thresholds']\n",
    "                    mean_RTs = np.nanmean(current_RTs)  # Compute mean recruitment threshold\n",
    "                    mean_recruitment_thresh_cond2.append((gridi, mni, mean_RTs))  # Store for sorting\n",
    "            # Sort MUs based on their mean recruitment threshold in condition 2\n",
    "            mean_recruitment_thresh_cond2.sort(key=lambda x: x[2])  # Sort by mean RTs\n",
    "\n",
    "            ###### Deordering in condition 2 relative to condition 1 ######################################\n",
    "            # Build the appropriate mapping between the matched indices of different conditions ###########\n",
    "            # Extract condition types (assuming the format \"muscle_condition\")\n",
    "            cond1_type = cond1.split('_')[1]  # e.g. \"abduction\"\n",
    "            cond2_type = cond2.split('_')[1]  # e.g. \"flexion\"\n",
    "            # Retrieve the full table for the current subject/muscle. \n",
    "            # (Assuming it's the same for both conditions, or you can pick one of them.)\n",
    "            full_table = match_files_data[current_subject][current_muscle][cond1_type]['full_table']\n",
    "            # Ensure full_table is a pandas DataFrame.\n",
    "            # Now build a mapping from condition1 indices to condition2 indices:\n",
    "            if cond1_type == 'abduction' and cond2_type == 'flexion':\n",
    "                mapping_dict = dict(zip(full_table[cond1_type], full_table[cond2_type]))\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected condition types.\")\n",
    "            #### Actual deoredering calculations ###############################################\n",
    "            rank_cond1 = np.arange(len(mean_recruitment_thresh_cond1))\n",
    "            rank_cond2 = np.zeros(len(mean_recruitment_thresh_cond1))  # same length since we compare matched MUs\n",
    "            iter_mn_accumulator = -1\n",
    "            for _, mni, _ in mean_recruitment_thresh_cond1:\n",
    "                iter_mn_accumulator += 1\n",
    "                # Convert the index from condition 1 to the corresponding index in condition 2\n",
    "                mapped_mni = mapping_dict.get(mni+101, None) # +101 is a quick fix because there is only one grid (+100), so the mapping is consistent. The +1 is for the 1-indexing of Matlab\n",
    "                if mapped_mni is None:\n",
    "                    index_match = np.nan\n",
    "                else:\n",
    "                    # Look for the mapped index in the sorted condition 2 list.\n",
    "                    index_match = next((i for i, tup in enumerate(mean_recruitment_thresh_cond2) if tup[1] == mapped_mni-101), np.nan) # -100 is a quick fix because there is only one grid, so the mapping is consistent. The -1 is for the 1-indexing of Matlab\n",
    "                rank_cond2[iter_mn_accumulator] = index_match\n",
    "            ranking_difference = rank_cond2 - rank_cond1\n",
    "\n",
    "            # Plot recruitment thresholds for condition 2 using the corrected order\n",
    "            matched_idx_iter_accumulator = -1\n",
    "            mean_RTs_cond2 = []\n",
    "            for gridi in sorted_indices_cond2.keys():\n",
    "                for mni in sorted_indices_cond2[gridi]:  # Use the remapped order for `cond2`\n",
    "                    matched_idx_iter_accumulator += 1\n",
    "                    if mni is not None:  # Only plot valid mappings\n",
    "                        current_RTs = data[current_subject][cond2]['recruitment_thresholds'][gridi][mni]['recruitment_thresholds']\n",
    "                        mean_RTs_cond2.append(np.nanmean(current_RTs))\n",
    "                        plt.scatter(x=np.ones_like(current_RTs) * matched_idx_iter_accumulator + condition_dodge_visual, y=current_RTs, s=50, color=\"C1\", alpha=0.5)\n",
    "                        plt.scatter(x=matched_idx_iter_accumulator + condition_dodge_visual, y=np.nanmean(current_RTs), s=100, color=\"C1\", alpha=1, edgecolors='black')\n",
    "                        # Plot change in recruitment ordering\n",
    "                        plt.arrow(x = matched_idx_iter_accumulator, y = mean_RTs_cond1[matched_idx_iter_accumulator],\n",
    "                                  dx = 0, dy = ranking_difference[matched_idx_iter_accumulator],\n",
    "                                  width = 0.03, length_includes_head = True, overhang = 0,\n",
    "                                  head_width=0.15, head_length=0.5, color = 'purple', alpha = 1)\n",
    "                        if ranking_difference[matched_idx_iter_accumulator] < 0:\n",
    "                            annot_text = f'{ranking_difference[matched_idx_iter_accumulator]:.0f}'\n",
    "                        else:\n",
    "                            annot_text = f'+{ranking_difference[matched_idx_iter_accumulator]:.0f}'\n",
    "                        plt.text(matched_idx_iter_accumulator, mean_RTs_cond1[matched_idx_iter_accumulator] + 1.5,\n",
    "                            annot_text,\n",
    "                            fontsize=11, color='purple', ha='center', weight='bold',\n",
    "                            bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "            plt.plot(np.arange(len(sorted_indices_cond1[gridi]))+condition_dodge_visual, mean_RTs_cond2, color=\"C1\", alpha=0.5, linewidth=2, label = cond2)\n",
    "            \n",
    "        ########################## Compute spearman correlation coefficient (rho): ##########################\n",
    "        rho_between_cond, _ = stats.spearmanr(mean_RTs_cond1, mean_RTs_cond2)\n",
    "\n",
    "        # Compute intra-class Spearman correlation for cond1\n",
    "        # Get the number of trials from the first motor unit (assuming all have same length)\n",
    "        n_trials = 0\n",
    "        for gridi in sorted_indices_cond1.keys():\n",
    "            if sorted_indices_cond1[gridi]:\n",
    "                for mni in range(len(data[current_subject][cond1]['recruitment_thresholds'][gridi])):\n",
    "                    mni_n_trials_temp = len(data[current_subject][cond1]['recruitment_thresholds'][gridi][mni]['recruitment_thresholds'])\n",
    "                    if mni_n_trials_temp > n_trials:\n",
    "                        n_trials = mni_n_trials_temp\n",
    "        # Build a list of arrays, one per trial, where each array collects the trial's recruitment thresholds\n",
    "        trial_thresholds_cond1 = [ [] for _ in range(n_trials) ]\n",
    "        for gridi in sorted_indices_cond1.keys():\n",
    "            for mni in sorted_indices_cond1[gridi]:\n",
    "                current_RTs = data[current_subject][cond1]['recruitment_thresholds'][gridi][mni]['recruitment_thresholds']\n",
    "                # If current_RTs is an array (e.g., one value per trial) then add each trial value:\n",
    "                for t in range(n_trials):\n",
    "                        if t >= len(current_RTs):\n",
    "                            trial_thresholds_cond1[t].append(np.nan)\n",
    "                        else:\n",
    "                            trial_thresholds_cond1[t].append(current_RTs[t])\n",
    "        # Convert lists to numpy arrays for convenience:\n",
    "        trial_thresholds_cond1 = [np.array(trial_data) for trial_data in trial_thresholds_cond1]\n",
    "        # Compute all pairwise Spearman correlations across trials:\n",
    "        rho_values_cond1 = []\n",
    "        for t1 in range(n_trials - 1):\n",
    "            for t2 in range(t1 + 1, n_trials):\n",
    "                # Create a mask selecting indices where both t1 and t2 arrays are non-NaN\n",
    "                mask = ~np.isnan(trial_thresholds_cond1[t1]) & ~np.isnan(trial_thresholds_cond1[t2])\n",
    "                # Ensure there are at least two points to compute a correlation\n",
    "                if np.sum(mask) < 2:\n",
    "                    continue\n",
    "                rho, _ = stats.spearmanr(trial_thresholds_cond1[t1][mask], trial_thresholds_cond1[t2][mask])\n",
    "                rho_values_cond1.append(rho)\n",
    "        # For a summary measure, take the mean of these correlations\n",
    "        intra_corr_cond1 = np.nanmean(rho_values_cond1)\n",
    "        print(f\"Intra-class (within cond1) Spearman correlation: {intra_corr_cond1:.2f}\")\n",
    "\n",
    "        # Repeat a similar procedure for cond2:\n",
    "        n_trials2 = 0\n",
    "        for gridi in sorted_indices_cond2.keys():\n",
    "            if sorted_indices_cond2[gridi]:\n",
    "                for mni in range(len(data[current_subject][cond2]['recruitment_thresholds'][gridi])):\n",
    "                    mni_n_trials_temp = len(data[current_subject][cond2]['recruitment_thresholds'][gridi][mni]['recruitment_thresholds'])\n",
    "                    if mni_n_trials_temp > n_trials2:\n",
    "                        n_trials2 = mni_n_trials_temp\n",
    "        trial_thresholds_cond2 = [ [] for _ in range(n_trials2) ]\n",
    "        for gridi in sorted_indices_cond2.keys():\n",
    "            for mni in sorted_indices_cond2[gridi]:\n",
    "                if mni is not None:  # if the mapping is valid\n",
    "                    current_RTs = data[current_subject][cond2]['recruitment_thresholds'][gridi][mni]['recruitment_thresholds']\n",
    "                    for t in range(n_trials2):\n",
    "                        if t >= len(current_RTs):\n",
    "                            trial_thresholds_cond2[t].append(np.nan)\n",
    "                        else:\n",
    "                            trial_thresholds_cond2[t].append(current_RTs[t])\n",
    "        trial_thresholds_cond2 = [np.array(trial_data) for trial_data in trial_thresholds_cond2]\n",
    "        rho_values_cond2 = []\n",
    "        for t1 in range(n_trials2 - 1):\n",
    "            for t2 in range(t1 + 1, n_trials2):\n",
    "                # Create a mask selecting indices where both t1 and t2 arrays are non-NaN\n",
    "                mask = ~np.isnan(trial_thresholds_cond2[t1]) & ~np.isnan(trial_thresholds_cond2[t2])\n",
    "                # Ensure there are at least two points to compute a correlation\n",
    "                if np.sum(mask) < 2:\n",
    "                    continue\n",
    "                rho, _ = stats.spearmanr(trial_thresholds_cond2[t1][mask], trial_thresholds_cond2[t2][mask])\n",
    "                rho_values_cond2.append(rho)\n",
    "        intra_corr_cond2 = np.nanmean(rho_values_cond2)\n",
    "        print(f\"Intra-class (within cond2) Spearman correlation: {intra_corr_cond2:.2f}\")\n",
    "        \n",
    "        plt.xlabel(\"Matched motor units (sorted by condition 1 mean recruitment thresholds)\")\n",
    "        plt.ylabel(\"Recruitment Thresholds (% MVC)\")\n",
    "        plt.title(f\"Comparison of Recruitment Thresholds - {current_subject}, {current_muscle}, {conditions_compared[0]} VS {conditions_compared[1]}\\nSpearman's (rank-based) correlation coefficient = {rho_between_cond:.2f}\\nIntra-class correlation: within {conditions_compared[0]} = {intra_corr_cond1:.2f} +/- {np.nanstd(rho_values_cond1):.2f}; within {conditions_compared[1]} = {intra_corr_cond2:.2f} +/- {np.nanstd(rho_values_cond2):.2f}\")\n",
    "        plt.legend()\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COH_between_csts(cst1, cst2, fsamp=2048, coh_window_length=1, coh_windows_overlap = 0.5, max_freq=100):\n",
    "    n_per_seg = coh_window_length * fsamp # convert window length to samples instead of s\n",
    "    n_overlap = np.round(n_per_seg * coh_windows_overlap).astype(int)\n",
    "    n_fft = np.round(upsampling_frequency_resolution * n_per_seg).astype(int)\n",
    "    # Compute intra-group coherence for group 1\n",
    "    f, COH_intragroup_X = csd(detrend(cst1), detrend(cst1), window=windows.hann(n_per_seg), noverlap=n_overlap, nfft=n_fft, fs=fsamp)\n",
    "    # Compute intra-group coherence for group 2\n",
    "    f, COH_intragroup_Y = csd(detrend(cst2), detrend(cst2), window=windows.hann(n_per_seg), noverlap=n_overlap, nfft=n_fft, fs=fsamp)\n",
    "    # Compute inter-group coherence\n",
    "    f, COH_intergroup = csd(detrend(cst1), detrend(cst2), window=windows.hann(n_per_seg), noverlap=n_overlap, nfft=n_fft, fs=fsamp)\n",
    "\n",
    "    # Limiting the frequencies up to the max frequency we are interested in (for efficiency)\n",
    "    max_index = np.where(f[f<=max_freq])[0][-1]\n",
    "    COH_intragroup_X = COH_intragroup_X[:max_index]\n",
    "    COH_intragroup_Y = COH_intragroup_Y[:max_index]\n",
    "    COH_intergroup = COH_intergroup[:max_index]\n",
    "    frequency_bins = f[:max_index]\n",
    "\n",
    "    coherence_over_frequencies = (np.abs(COH_intergroup) ** 2) / (COH_intragroup_X * COH_intragroup_Y) # Welch's method of coherence calculation\n",
    "\n",
    "    return frequency_bins, coherence_over_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_isi_spike_train(spike_train):\n",
    "    \"\"\"\n",
    "    Generate a surrogate spike train by shuffling inter-spike intervals.\n",
    "    Parameters\n",
    "    ----------\n",
    "    spike_train : 1D array_like of {0,1}\n",
    "        Binary vector where 1 indicates a spike and 0 no spike.\n",
    "    Returns\n",
    "    -------\n",
    "    surrogate : 1D numpy.ndarray of {0,1}\n",
    "        A new binary spike train of the same length, with the exact same\n",
    "        number of spikes and the same set of zero-run lengths, but in random order.\n",
    "    \"\"\"\n",
    "    spike_train = np.asarray(spike_train, dtype=int)\n",
    "    T = spike_train.size\n",
    "    spike_idxs = np.flatnonzero(spike_train)\n",
    "\n",
    "    # If no spikes (or only one), just return a copy\n",
    "    if spike_idxs.size == 0:\n",
    "        return spike_train.copy()\n",
    "\n",
    "    # 1) Extract zero-run lengths (including leading & trailing)\n",
    "    zero_runs = []\n",
    "    # leading zeros before the first spike\n",
    "    zero_runs.append(spike_idxs[0])\n",
    "    # zeros between spikes\n",
    "    for i in range(spike_idxs.size - 1):\n",
    "        run_len = spike_idxs[i+1] - spike_idxs[i] - 1\n",
    "        zero_runs.append(run_len)\n",
    "    # trailing zeros after the last spike\n",
    "    zero_runs.append(T - spike_idxs[-1] - 1)\n",
    "\n",
    "    # 2) Shuffle the zero-run lengths\n",
    "    shuffled = np.random.permutation(zero_runs)\n",
    "\n",
    "    # 3) Re-build the train: place each spike after its preceding zero-run\n",
    "    surrogate = np.zeros(T, dtype=int)\n",
    "    # position of the first spike\n",
    "    pos = shuffled[0]\n",
    "    for i in range(spike_idxs.size):\n",
    "        if pos < T:\n",
    "            surrogate[pos] = 1\n",
    "        # move past this spike + next zero-run\n",
    "        pos = pos + 1 + shuffled[i+1]\n",
    "\n",
    "    return surrogate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORM COHERENCE ANALYSIS\n",
    "\n",
    "# --------------------------------------\n",
    "# User-defined parameter for the analysis\n",
    "# --------------------------------------\n",
    "# coh_window_length = 1  # in s\n",
    "# coh_windows_overlap = 0.75 # % relative to coh_window_length\n",
    "# # frequency resolution will be = 1/coh_window_length. If coh_window_length = 10s for instance, resolution will be 0.1hz\n",
    "# upsampling_frequency_resolution = 5 # for the 'nfft' parameter of scipy.csd(). Interpolates the coherence values over the frequencies, to smooth the figure a bit.\n",
    "# # For example, if the resolution defined by coh_window_length is 1hz, and if upsampling_frequency_resolution = 2, the resolution will be upsampled to 0.5hz\n",
    "# max_freq = 60 # maximum frequency that is kept\n",
    "# COH_calc_max_iteration_nb_per_group_size = 1 # More iteration for smaller group sizes, because the value obtained is very dependent upon the exact neurons selected, especially when only a few MNs are used to create the CST\n",
    "# running mock coherence calculation to get the frequency bins, given the parameters\n",
    "freq_bins, _ = COH_between_csts(np.zeros(input_fsamp*coh_window_length), np.zeros(input_fsamp*coh_window_length),\n",
    "                                fsamp=input_fsamp, coh_window_length=coh_window_length, coh_windows_overlap = coh_windows_overlap,\n",
    "                                max_freq = max_freq)\n",
    "\n",
    "# --------------------------------------\n",
    "# Build dynamic fieldnames, depending on nb of frequency bins\n",
    "# --------------------------------------\n",
    "fieldnames = [\"subject\", \"condition\", \"muscles\",\"frequency\",\"coherence\",\"nb_of_MNs_in_group_for_COH_calculation\"]\n",
    "# This will collect all rows (from all subjects & conditions) so that\n",
    "# we can write one \"big CSV\" at the end.\n",
    "all_COH_results = []\n",
    "\n",
    "if perform_analysis_coherence:\n",
    "    print(\"Starting coherence analysis\")\n",
    "    for current_subject in data.keys():\n",
    "        for current_condition in data[current_subject].keys():\n",
    "            # Create folder to save output (per subject, per condition)\n",
    "            folder_subject_condition = os.path.join(path_of_files,current_subject,\n",
    "                f'{current_subject}_{current_condition}' )\n",
    "            if not os.path.exists(folder_subject_condition):\n",
    "                os.makedirs(folder_subject_condition)\n",
    "            print(f\"     Coherence analysis for {current_subject} - {current_condition}\")\n",
    "            # Overwrite local CSV => remove if it already exists\n",
    "            csv_path = os.path.join(folder_subject_condition, \"Coherence_analysis_results.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                os.remove(csv_path)\n",
    "                \n",
    "            # valid_MUs = data[current_subject][current_condition]['valid_MUs_given_selected_plateaus']\n",
    "            # Set all MUs as valid (no need to have a ISI cutoff for COH)\n",
    "            valid_MUs = np.arange(len(data[current_subject][current_condition]['MU_muscle_list']))\n",
    "            muscles_unique = list(set(data[current_subject][current_condition]['MU_muscle_list']))\n",
    "            muscle_pairs = list(itertools.combinations(muscles_unique, 2))\n",
    "\n",
    "            # Re-organize the binary spike trains from list to matrix (because this is the expected format) = already the case for experimental data, but needs to be done for simulated data\n",
    "            if type(data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'])==list:\n",
    "                data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'] = np.array(data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'])\n",
    "\n",
    "            # Build possible muscle groups (single muscles + pairs)\n",
    "            muscle_groups_for_COH = {}\n",
    "            iter_muscle_group_COH = 0\n",
    "            for muscle_i in muscles_unique:\n",
    "                muscle_groups_for_COH[iter_muscle_group_COH] = muscle_i\n",
    "                iter_muscle_group_COH += 1\n",
    "            for muscle_pair_i in muscle_pairs:\n",
    "                muscle_groups_for_COH[iter_muscle_group_COH] = [muscle_pair_i[0], muscle_pair_i[1]]\n",
    "                iter_muscle_group_COH += 1\n",
    "\n",
    "            # Prepare the figure\n",
    "            nb_subplots = len(muscle_groups_for_COH)\n",
    "            plt.figure(figsize=(nb_subplots*7,6))\n",
    "\n",
    "            muscle_iter = -1\n",
    "            for muscle_current_COH in muscle_groups_for_COH.values():\n",
    "                muscle_iter += 1\n",
    "                MU_muscle_list = data[current_subject][current_condition]['MU_muscle_list']\n",
    "                # Identify the relevant MU indices\n",
    "                muscle_corresponding_MUs = [] # store as list, as we may consider 1 (within-muscle) or 2 (between-muscles) pools of MUs\n",
    "                if isinstance(muscle_current_COH, str):\n",
    "                    # Single muscle\n",
    "                    muscle_corresponding_MUs.append([i for i, val in enumerate(MU_muscle_list) if val == muscle_current_COH])\n",
    "                else:\n",
    "                    # List of muscles (pair)\n",
    "                    for muscle_i in muscle_current_COH:\n",
    "                        muscle_corresponding_MUs.append([i for i, val in enumerate(MU_muscle_list) if val == muscle_i])\n",
    "                MU_idx_for_COH = []\n",
    "                binary_spike_trains_for_COH_analysis_temp = []\n",
    "                nb_motoneurons_temp = []\n",
    "                for muscle_corresponding_MUs_iter in muscle_corresponding_MUs:\n",
    "                    MU_idx_for_COH.append(np.intersect1d(muscle_corresponding_MUs_iter, valid_MUs))\n",
    "                    binary_spike_trains_for_COH_analysis_temp.append(np.squeeze(data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'][[MU_idx_for_COH[-1]]]))\n",
    "                    # If only one MN, make sure binary_spike_trains_for_COH_analysis_temp is still 2D\n",
    "                    if len(binary_spike_trains_for_COH_analysis_temp[-1].shape)==1:\n",
    "                        binary_spike_trains_for_COH_analysis_temp[-1] = binary_spike_trains_for_COH_analysis_temp[-1][np.newaxis, :]\n",
    "                    nb_motoneurons_temp.append(binary_spike_trains_for_COH_analysis_temp[-1].shape[0])\n",
    "                \n",
    "                # From groups of 1 up to half of the motor neurons\n",
    "                shuffled_idx_list_COH = []\n",
    "                for muscle_i in range(len(nb_motoneurons_temp)):\n",
    "                    shuffled_idx_list_COH.append(list.copy(list(range(nb_motoneurons_temp[muscle_i])))) # initialize list of indices to be shuffled iteratively\n",
    "                if len(nb_motoneurons_temp) == 1: # Case with only one muscle (within-muscle coherence)\n",
    "                    COH_calc_group_size_nb = int(np.floor(nb_motoneurons_temp[0]/2))\n",
    "                else: # Case with two muscles (between-muscles coherence)\n",
    "                    COH_calc_group_size_nb = np.min(nb_motoneurons_temp)\n",
    "                COH_pooled_per_group_size = []\n",
    "                # For the surrogate-based CI calculations\n",
    "                surrogate_COH_for_CI = [] # If this the is the largest group size, compute the confidence interval with the surrogate spike trains method\n",
    "                printed_shuffle_already = False\n",
    "                n_shuffles = 100\n",
    "                alphas = [0.05, 0.01]\n",
    "\n",
    "                for group_sizi in range(COH_calc_group_size_nb): # for each size of group\n",
    "                    coherence_current_group_size = []\n",
    "                    COH_calc_iteration_nb_per_group_size_temp = int(np.max([np.round(COH_calc_max_iteration_nb_per_group_size/(group_sizi+1)),1]))\n",
    "                    print(f'        {muscle_current_COH} - Iterating for groups of {group_sizi+1} motoneurons (out of a max group size of {COH_calc_group_size_nb}) - {COH_calc_iteration_nb_per_group_size_temp} iterations')\n",
    "\n",
    "                    for group_iteri in range(COH_calc_iteration_nb_per_group_size_temp):\n",
    "                        random.shuffle(shuffled_idx_list_COH[0])\n",
    "                        idx_cst1 = np.copy(shuffled_idx_list_COH[0][0:group_sizi+1])\n",
    "                        cst1 = np.nansum(binary_spike_trains_for_COH_analysis_temp[0][idx_cst1,:],axis=0)\n",
    "                        if len(nb_motoneurons_temp) == 1: # Case with only one muscle (within-muscle coherence)\n",
    "                            idx_cst2 = np.copy(shuffled_idx_list_COH[0][len(shuffled_idx_list_COH[0])-(group_sizi+1):len(shuffled_idx_list_COH[0])])\n",
    "                            cst2 = np.nansum(binary_spike_trains_for_COH_analysis_temp[0][idx_cst2,:],axis=0)\n",
    "                        else: # Case with two muscles (between-muscles coherence)\n",
    "                            idx_cst2 = np.copy(shuffled_idx_list_COH[1][0:group_sizi+1])\n",
    "                            cst2 = np.nansum(binary_spike_trains_for_COH_analysis_temp[1][idx_cst2,:],axis=0)\n",
    "                        # If this the is the largest group size, compute the confidence interval with the surrogate spike trains method\n",
    "                        if group_sizi == COH_calc_group_size_nb-1:\n",
    "                            if not printed_shuffle_already:\n",
    "                                printed_shuffle_already = True\n",
    "                                print(\"     Most number of MUs: calculating CI by creating shuffled, surrogate spike trains...\")\n",
    "                            for shuffli in range(n_shuffles):\n",
    "                                # apply to each MU-train\n",
    "                                surrogate_trains_1 = np.array([\n",
    "                                    shuffle_isi_spike_train(train) \n",
    "                                    for train in list(binary_spike_trains_for_COH_analysis_temp[0][idx_cst1,:])])\n",
    "                                surrogate_trains_2 = np.array([\n",
    "                                    shuffle_isi_spike_train(train) \n",
    "                                    for train in list(binary_spike_trains_for_COH_analysis_temp[0][idx_cst2,:])])\n",
    "                                # sum across motor units\n",
    "                                cst1_surr = np.nansum(surrogate_trains_1, axis=0)\n",
    "                                cst2_surr = np.nansum(surrogate_trains_2, axis=0)\n",
    "                                _, coh_cst_surr_output = COH_between_csts(cst1_surr, cst2_surr,\n",
    "                                    fsamp=input_fsamp, coh_window_length=coh_window_length, coh_windows_overlap = coh_windows_overlap,\n",
    "                                    max_freq = max_freq)\n",
    "                                surrogate_COH_for_CI.append(coh_cst_surr_output.real) # keep only real part\n",
    "                        # # # # #\n",
    "                        _, coh_cst_output = COH_between_csts(cst1, cst2,\n",
    "                                fsamp=input_fsamp, coh_window_length=coh_window_length, coh_windows_overlap = coh_windows_overlap,\n",
    "                                max_freq = max_freq)\n",
    "                        coherence_current_group_size.append(coh_cst_output)\n",
    "                    # end of \"for each iteration (within a group size)\"\n",
    "                    COH_pooled_per_group_size.append(np.nanmean(coherence_current_group_size, axis=0) )\n",
    "\n",
    "                    # If this the is the largest group size, compute the confidence interval ##############\n",
    "                    if group_sizi == COH_calc_group_size_nb-1:\n",
    "                        # Standard, simple method (Muecli et al 2022)\n",
    "                        L = len(cst1)\n",
    "                        n_per_seg = coh_window_length * input_fsamp # get duration in samples instead of s\n",
    "                        n_overlap = np.round(n_per_seg * coh_windows_overlap).astype(int) # get duration in samples instead of s\n",
    "                        step = n_per_seg - n_overlap\n",
    "                        N = int(np.floor((L - n_overlap) / step))\n",
    "                        CL_simple = []\n",
    "                        CL_surrogate = []\n",
    "                        for alpha in alphas:\n",
    "                            CL_simple.append(1 - (1 - alpha)**(1/(N - 1)))\n",
    "                            # Surrogate (shuffled ISIs) method (https://pmc.ncbi.nlm.nih.gov/articles/PMC9223438/?utm_source=chatgpt.com Biomed Signal Proces Control 2022)\n",
    "                            CL_surrogate.append(\n",
    "                                np.percentile(surrogate_COH_for_CI,\n",
    "                                (1-alpha)*100, axis=0)\n",
    "                            )\n",
    "\n",
    "                    # Add analysis results to csv - once for each MN group size\n",
    "                    freq_iter = -1\n",
    "                    muscle_label = str(muscle_current_COH) # Make sure the label is a string\n",
    "                    for freq_i in freq_bins:\n",
    "                        freq_iter += 1\n",
    "                        # Create a new dictionary for this frequency bin\n",
    "                        new_row = {\n",
    "                            \"subject\": current_subject,\n",
    "                            \"condition\": current_condition,\n",
    "                            \"muscles\": muscle_label,\n",
    "                            \"frequency\": freq_i,  # set the current frequency\n",
    "                            \"nb_of_MNs_in_group_for_COH_calculation\": group_sizi+1 # +1 because of zero-indexing\n",
    "                        }\n",
    "                        new_row[\"coherence\"] = COH_pooled_per_group_size[group_sizi][freq_iter].real\n",
    "                        # if group_sizi >= 0: # start at 0 because of zero-indexing\n",
    "                        #     new_row[\"coherence\"] = COH_pooled_per_group_size[group_sizi][freq_iter].real\n",
    "                        # else:\n",
    "                        #     new_row[\"coherence\"] = np.nan\n",
    "\n",
    "                        # Write to the local CSV (for this subject + condition)\n",
    "                        append_to_csv(csv_path, new_row, fieldnames)\n",
    "                        # Also add to our global list for the big CSV\n",
    "                        all_COH_results.append(new_row)\n",
    "                    # end of \"for each group size)\"\n",
    "                # Plot\n",
    "                plt.subplot(1,nb_subplots,muscle_iter+1)\n",
    "                # Assign correct color\n",
    "                if isinstance(muscle_current_COH, str):\n",
    "                    if (\"Sim_parameters\" in data[current_subject][current_condition].keys()) and (color_by_pool_or_by_condition == 'condition'):\n",
    "                        figure_color = Find_color_for_sim_by_condition(current_subject, current_condition)\n",
    "                    else:\n",
    "                        figure_color = globals()[f\"{muscle_current_COH}_color\"]\n",
    "                    muscle_label = muscle_current_COH\n",
    "                    plt.title(f\"{muscle_current_COH} - mean coherence\\nMax MN group size = {COH_calc_group_size_nb}\")\n",
    "                else:\n",
    "                    figure_color = mix_colors(globals()[f\"{muscle_current_COH[0]}_color\"],globals()[f\"{muscle_current_COH[1]}_color\"])\n",
    "                    muscle_label = str(muscle_current_COH)\n",
    "                    plt.title(f\"{muscle_current_COH[0]} VS {muscle_current_COH[1]} - mean coherence\\nMax MN group size = {COH_calc_group_size_nb}\")\n",
    "                for group_sizi in range(COH_calc_group_size_nb):\n",
    "                    max_alpha = 0.35\n",
    "                    min_alpha = 0.15\n",
    "                    figure_alpha_temp = min_alpha + (group_sizi/COH_calc_group_size_nb)*(max_alpha-min_alpha)\n",
    "                    # plt.plot(freq_bins, COH_pooled_per_group_size[group_sizi], color=figure_color, linewidth=2, alpha = figure_alpha_temp,\n",
    "                    #          label = f\"groups of {group_sizi+1} MNs\")\n",
    "                    # Highlight a specific MN nb (to help comparisons)\n",
    "                    plt.plot(freq_bins, COH_pooled_per_group_size[group_sizi], color=figure_color, linewidth=1.5, alpha = figure_alpha_temp)\n",
    "                    if group_sizi == 5:\n",
    "                        plt.plot(freq_bins, COH_pooled_per_group_size[group_sizi],\n",
    "                                 color='k', linewidth=1.5, alpha = 1, zorder = 5, linestyle='--',\n",
    "                                 label=\"5 MNs\")\n",
    "                iter_alpha_thresh = -1\n",
    "                for alpha in alphas:\n",
    "                    iter_alpha_thresh += 1\n",
    "                    opacity = (iter_alpha_thresh+1) / len(alphas)\n",
    "                    # plt.axhline(y=CL_simple[iter_alpha_thresh],\n",
    "                    #       linewidth=1.5, linestyle='--', color='red', alpha = opacity, zorder = 10,\n",
    "                    #        label=f'{(1-alpha)*100}% CI (simple method)')\n",
    "                    plt.plot(freq_bins, CL_surrogate[iter_alpha_thresh],\n",
    "                            linewidth=1.5, linestyle='-', color='red', alpha = opacity, zorder=10,\n",
    "                            label=f'{(1-alpha)*100}% CI (surrogate spike trains method)')\n",
    "                plt.xlabel(\"Frequency (Hz)\")\n",
    "                plt.ylabel(\"Coherence\")\n",
    "                plt.title(f\"Up to {COH_calc_group_size_nb} MNs\")\n",
    "                plt.ylim(0,1)\n",
    "                plt.xlim(0,max_freq)\n",
    "                plt.legend(loc=\"upper right\")\n",
    "\n",
    "            # End of \"for each muscle or muscle pair\"\n",
    "\n",
    "            # Display and save figure\n",
    "            plt.suptitle(f'Participant {current_subject}, {current_condition}')\n",
    "            plt.tight_layout()\n",
    "            if os.path.exists(folder_subject_condition):\n",
    "                plt.savefig(os.path.join(folder_subject_condition, \"Coherence_analysis_results.png\"))\n",
    "            else:\n",
    "                print(\"Output folder does not exist; figure not saved\")\n",
    "            if display_figures_inline:\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.close()\n",
    "        # End of \"for each condition\"\n",
    "    # End of \"for each subject\"\n",
    "\n",
    "    # Now write ONE big CSV with all rows, saved at 'path_of_files' level\n",
    "    big_csv_path = os.path.join(path_of_files, \"Coherence_analysis_ALL.csv\")\n",
    "    write_big_csv(big_csv_path, all_COH_results, fieldnames)\n",
    "\n",
    "    print(f\"\\nA concatenated CSV with all coherence results was saved at:\\n{big_csv_path}\")\n",
    "else:\n",
    "    print(\"Coherence analysis not performed, because 'perform_analysis_coherence=False'. Carrying on with the script...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandstop_sos(data, fs, lowcut, highcut, order=4):\n",
    "    \"\"\"\n",
    "    Zeroâ€‘phase Butterworth bandâ€‘stop (notch) filter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 1D array\n",
    "        The signal to filter (e.g. your CST).\n",
    "    fs : float\n",
    "        Sampling rate in Hz.\n",
    "    lowcut : float\n",
    "        Lower edge of the stop band in Hz.\n",
    "    highcut : float\n",
    "        Upper edge of the stop band in Hz.\n",
    "    order : int\n",
    "        The filter order.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered : 1D array\n",
    "        The bandâ€‘stopped signal.\n",
    "    \"\"\"\n",
    "    nyq = fs / 2.0\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    sos = butter(order, [low, high], btype='bandstop', output='sos')\n",
    "    return sosfiltfilt(sos, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --------------------------------------------------- #\n",
    "# # perform_analysis_frequency_components_dimensionality\n",
    "# # --------------------------------------------------- #\n",
    "\n",
    "if perform_analysis_frequency_components_dimensionality:\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 1) Set your parameters up front\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    fs        = input_fsamp        # e.g. 2 048 Hz\n",
    "    lowpass_smoothing_of_wavelet_analysis_output = 1.0 # 60 # in hz\n",
    "    min_freq  = 1                # lowest band (Hz)â€”warn if <1 Hz\n",
    "    max_freq  = 60.0               # highest band (Hz)\n",
    "    n_freqs   = 60                 # how many wavelet scales/bands\n",
    "    wavelet   = 'morl'             # complex Morlet\n",
    "    trim_cyc  = 3                  # how many cycles @ min_freq to cut off\n",
    "    # cwt_clim  = None               # e.g. (vmin, vmax) or None to auto\n",
    "    cut_cst_to     = None # 60.0              # seconds of CST to keep for fast testing; set to None to disable\n",
    "    notch_filter_mean_firing_rate = False\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 2) Build your frequency vector & corresponding scales\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    if min_freq < 1.0:\n",
    "        import warnings\n",
    "        warnings.warn(\"Using min_freq < 1 Hz can be unstable\", UserWarning)\n",
    "\n",
    "    freqs = np.linspace(min_freq, max_freq, n_freqs)\n",
    "    # center frequency of your wavelet\n",
    "    cf = pywt.central_frequency(wavelet)  \n",
    "    # scale-to-frequency: scale = cf * fs / freq\n",
    "    scales = cf * fs / freqs\n",
    "\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # 3) Compute the CWT\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for current_subject in data.keys(): # ['MeJu']: # \n",
    "        for current_condition in data[current_subject].keys():\n",
    "            # Create folder to save output (per subject, per condition)\n",
    "            # Create folder to save output (per subject, per condition)\n",
    "            folder_subject_condition_timefreq_analysis = os.path.join(path_of_files,current_subject,\n",
    "                f'{current_subject}_{current_condition}','Time_Frequency_analysis' )\n",
    "            if not os.path.exists(folder_subject_condition_timefreq_analysis):\n",
    "                os.makedirs(folder_subject_condition_timefreq_analysis)\n",
    "            print(f\"     Time-frequency analysis for {current_subject} - {current_condition}\")\n",
    "            # Set all MUs as valid (no need to have a ISI cutoff for COH)\n",
    "            valid_MUs = np.arange(len(data[current_subject][current_condition]['MU_muscle_list']))\n",
    "            muscles_unique = list(set(data[current_subject][current_condition]['MU_muscle_list']))\n",
    "            # Re-organize the binary spike trains from list to matrix (because this is the expected format) = already the case for experimental data, but needs to be done for simulated data\n",
    "            if type(data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'])==list:\n",
    "                data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'] = np.array(data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'])\n",
    "            for current_muscle in muscles_unique:\n",
    "                # if current_muscle != 'VL':\n",
    "                #     continue # for testing\n",
    "                \n",
    "                color_of_plot = globals()[f\"{current_muscle}_color\"]\n",
    "                # get numeric indices of MUs for this muscle\n",
    "                MU_list = np.where(\n",
    "                    np.array(data[current_subject][current_condition]['MU_muscle_list']) == current_muscle\n",
    "                )[0]\n",
    "                total_MUs = len(MU_list)\n",
    "                if total_MUs == 0:\n",
    "                    continue\n",
    "                # build CST\n",
    "                cst = np.nansum(\n",
    "                    data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'][MU_list],\n",
    "                    axis=0\n",
    "                )\n",
    "                cst = cst - np.nanmean(cst)\n",
    "\n",
    "                if notch_filter_mean_firing_rate:\n",
    "                    ### Compute mean DR of pool considered\n",
    "                    # grab the list once\n",
    "                    mpg = data[current_subject][current_condition]['muscle_per_grid']\n",
    "                    # find all N such that mpg[N] == current_muscle\n",
    "                    Ns = [i for i, m in enumerate(mpg) if m == current_muscle]\n",
    "                    # now for each such N, extract the mean_firing_rates over all K in that subâ€dict\n",
    "                    means_per_N = [\n",
    "                        np.nanmean([\n",
    "                            entry['mean_firing_rates']\n",
    "                            for entry in data[current_subject][current_condition]['MU_properties'][N]\n",
    "                        ])\n",
    "                        for N in Ns\n",
    "                    ]\n",
    "                    DR_cov_per_N = [\n",
    "                        1/np.nanmean([\n",
    "                            entry['mean_cov']\n",
    "                            for entry in data[current_subject][current_condition]['MU_properties'][N]\n",
    "                        ])\n",
    "                        for N in Ns\n",
    "                    ]\n",
    "                    overall_DR_mean = np.nanmean(means_per_N)\n",
    "                    overall_DR_std = np.nanstd(means_per_N)\n",
    "                    overall_DR_cov = np.nanmean(DR_cov_per_N)\n",
    "                    # define your notch band\n",
    "                    low_f  = overall_DR_mean - overall_DR_cov # overall_DR_std\n",
    "                    high_f = overall_DR_mean + overall_DR_cov # overall_DR_std\n",
    "                    # guard against invalid bands\n",
    "                    low_f  = max(low_f, 0.1)           # canâ€™t go below DC\n",
    "                    high_f = min(high_f, input_fsamp/2 - 0.1)  # must stay below Nyquist\n",
    "                    print(f\"Notching CST around {low_f:.2f}â€“{high_f:.2f} Hz (meanâ€‰Â±â€‰std)\")\n",
    "                    # apply the notch filter to your meanâ€‘centered CST\n",
    "                    cst = bandstop_sos(cst, input_fsamp, low_f, high_f, order=4)\n",
    "\n",
    "                # Make the CST way smaller, \n",
    "                plt.figure(figsize=(20,5))\n",
    "                plt.plot(np.arange(len(cst))/input_fsamp, cst, color=color_of_plot, linewidth=0.2, alpha=0.5)\n",
    "                plt.ylabel(f\"CST amplitude (binary spikes)\\nMean-centered\")\n",
    "                plt.xlabel(\"Time (s)\")\n",
    "                plt.title(f\"{current_subject} - {current_condition} - {current_muscle} - CST\")\n",
    "                # === APPLY CUT FOR FAST TESTING ===\n",
    "                if cut_cst_to is not None:\n",
    "                    max_samp = int(cut_cst_to * fs)\n",
    "                    cst      = cst[:max_samp]\n",
    "                    plt.axvline(x=cut_cst_to, color='red', linestyle='--', label=f\"Cut CST to {cut_cst_to} s\")\n",
    "\n",
    "                coeffs, frequencies = pywt.cwt(cst, scales, wavelet, sampling_period=1/fs)\n",
    "                # coeffs shape = (n_freqs, n_timepoints)\n",
    "\n",
    "                power = np.abs(coeffs)**2\n",
    "                amps = np.abs(coeffs)\n",
    "\n",
    "                # SMOOTH\n",
    "                def lowpass_sos(amps, fs, cutoff, order=4):\n",
    "                    \"\"\"\n",
    "                    Low-pass filter amplitude envelopes via SOS for smooth trends.\n",
    "                    \n",
    "                    Parameters:\n",
    "                        amps (ndarray): shape (n_freqs, n_timepoints)\n",
    "                        fs (float): sampling rate in Hz\n",
    "                        cutoff (float): cutoff frequency in Hz\n",
    "                        order (int): filter order\n",
    "                    \n",
    "                    Returns:\n",
    "                        ndarray: low-pass filtered amps, same shape as input\n",
    "                    \"\"\"\n",
    "                    nyq = fs / 2.0\n",
    "                    norm_cut = cutoff / nyq\n",
    "                    sos = butter(order, norm_cut, btype='low', output='sos')\n",
    "                    return np.array([sosfiltfilt(sos, band) for band in amps])\n",
    "            \n",
    "                def highpass_sos(data, fs, cutoff=0.5, order=4):\n",
    "                    \"\"\"\n",
    "                    Zero-phase high-pass filter to remove <cutoff Hz drift.\n",
    "                    \"\"\"\n",
    "                    sos = butter(order, cutoff, btype='highpass', fs=fs, output='sos')\n",
    "                    return sosfiltfilt(sos, data)\n",
    "                \n",
    "                amps = lowpass_sos(amps, fs, cutoff=lowpass_smoothing_of_wavelet_analysis_output) #max_freq, order=4)  # Smooth the amplitude envelopes\n",
    "                # 1) High-pass each bandâ€™s envelope at 0.5 Hz\n",
    "                amps = np.array([ highpass_sos(band, fs, cutoff=0.5) for band in amps ])\n",
    "\n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                # 4) Trim the edge artifacts\n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                trim_samp = int((trim_cyc/min_freq) * fs)\n",
    "                power = power[:, trim_samp:-trim_samp]\n",
    "                amps = amps[:, trim_samp:-trim_samp]\n",
    "                t = np.arange(len(cst)) / fs\n",
    "                t_trim = t[trim_samp:-trim_samp]\n",
    "\n",
    "                # 2) Subtract the mean of each band (now near zero baseline)\n",
    "                amps = amps - amps.mean(axis=1, keepdims=True)\n",
    "                # 3) Shift upward to make everything â‰¥0 again (required by NMF)\n",
    "                amps = np.array([ band - np.nanmin(band) for band in amps ])\n",
    "\n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                # 5) Bring in your torque (no trimming)\n",
    "                # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "                plateau_idx = data[current_subject][current_condition]['selected_plateaus_samples_idx']\n",
    "                torque = data[current_subject][current_condition]['force'][plateau_idx]\n",
    "                # smooth the torque\n",
    "                torque = lowpass_sos(torque[np.newaxis, :], fs, cutoff=lowpass_smoothing_of_wavelet_analysis_output).squeeze()  # Smooth the torque signal\n",
    "                if cut_cst_to is not None:\n",
    "                    # cut torque to match the CST length\n",
    "                    torque = torque[:max_samp]\n",
    "                # align torque by cropping the same amount on both ends if you want it to match\n",
    "                torque_trim = torque[trim_samp:-trim_samp]\n",
    "\n",
    "                # PLOT\n",
    "                N = len(frequencies) # len(freqs)\n",
    "                cmap = plt.get_cmap('plasma', N)\n",
    "                # define boundaries so each freq gets its own color-bin\n",
    "                # bounds = np.linspace(freqs.min(), freqs.max(), N+1)\n",
    "                bounds = np.linspace(frequencies.min(), frequencies.max(), N+1)\n",
    "                norm   = mpl.colors.BoundaryNorm(boundaries=bounds, ncolors=N)\n",
    "                fig, ax = plt.subplots(figsize=(60, 6))\n",
    "                ax2 = ax.twinx()  # for torque on the right\n",
    "                # plot each band\n",
    "                for i, freq in enumerate(frequencies): #enumerate(freqs):\n",
    "                    color = cmap(i)\n",
    "                    ax.plot(t_trim, amps[i], color=color, linewidth=1, alpha=0.3)\n",
    "                # torque on secondary axis\n",
    "                ax2.plot(t_trim, torque_trim, color=color_of_plot, linewidth=2, label='Torque', alpha = 0.7)\n",
    "                # labels & title\n",
    "                ax.set_xlabel(\"Time (s)\")\n",
    "                ax.set_ylabel(\"CWT amplitude (Morlet)\")\n",
    "                ax2.set_ylabel(\"Torque\")\n",
    "                fig.suptitle(f\"{current_subject} â€“ {current_condition} â€“ {current_muscle} â€“ CWT amplitude\")\n",
    "                # create an axis on the right for the colorbar\n",
    "                divider = make_axes_locatable(ax)\n",
    "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "                sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "                sm.set_array([])\n",
    "                cbar = fig.colorbar(\n",
    "                    sm,\n",
    "                    cax=cax,\n",
    "                    boundaries=bounds,\n",
    "                    ticks=frequencies # freqs\n",
    "                )\n",
    "                cbar.set_label(\"Frequency (Hz)\")\n",
    "                # legend for torque\n",
    "                ax2.legend(loc='upper right')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{folder_subject_condition_timefreq_analysis}/{current_muscle}_CWT_amplitude.png')\n",
    "                plt.show()\n",
    "\n",
    "                # TIME-FREQUENCY SPECTRUM PLOT\n",
    "                # 1) Create figure and GridSpec\n",
    "                fig = plt.figure(figsize=(60, 6))\n",
    "                gs = GridSpec(2, 1, height_ratios=[3, 1], hspace=0.0)\n",
    "\n",
    "                ax_wt = fig.add_subplot(gs[0, 0])\n",
    "                ax_tq = fig.add_subplot(gs[1, 0], sharex=ax_wt)\n",
    "\n",
    "                # 2) Plot your CWT image\n",
    "                cwt_clim = (0, np.nanmax(amps))\n",
    "                im = ax_wt.imshow(\n",
    "                    amps,\n",
    "                    aspect='auto',\n",
    "                    origin='lower',\n",
    "                    extent=[t_trim[0], t_trim[-1], freqs[0], freqs[-1]],\n",
    "                    clim=cwt_clim,\n",
    "                    cmap=cmr.rainforest,\n",
    "                    interpolation=None\n",
    "                )\n",
    "                ax_wt.set_ylabel('Frequency (Hz)')\n",
    "                ax_wt.set_title(f'{current_subject} â€“ {current_condition} â€“ {current_muscle}\\nCWT power (Morlet)')\n",
    "\n",
    "                # 3) Plot torque\n",
    "                ax_tq.plot(t_trim, torque_trim, color=color_of_plot, linewidth=5, alpha=0.7)\n",
    "                ax_tq.set_ylabel('Torque')\n",
    "                ax_tq.set_xlabel('Time (s)')\n",
    "\n",
    "                # 4) Manually add a colorbar axes at the right edge of ax_wt, without resizing either subplot\n",
    "                pos = ax_wt.get_position()  # Bbox of ax_wt in figure coordinates\n",
    "                cax = fig.add_axes([\n",
    "                    pos.x1 + 0.005,   # slightly to the right of ax_wt\n",
    "                    pos.y0,          # same bottom as ax_wt\n",
    "                    0.02,            # width of the colorbar\n",
    "                    pos.height       # same height as ax_wt\n",
    "                ])\n",
    "                fig.colorbar(im, cax=cax, label='Amplitude')\n",
    "                plt.savefig(f'{folder_subject_condition_timefreq_analysis}/{current_muscle}_Time_Frequency_spectrum.png')\n",
    "                plt.show()\n",
    "\n",
    "                # === Assumptions ===\n",
    "                # amp_trim: np.ndarray of shape (n_freqs, n_timepoints)\n",
    "                # freqs: np.ndarray of length n_freqs\n",
    "                # t_trim: np.ndarray of length n_timepoints\n",
    "                # current_subject, current_condition, current_muscle defined\n",
    "\n",
    "                # --- 1) Prepare data for NMF ---\n",
    "                X = amps.T  # shape (n_timepoints, n_freqs)\n",
    "                total_var = np.linalg.norm(X, 'fro')**2\n",
    "\n",
    "                # --- 2) Compute explained variance for 1..10 components ---\n",
    "                max_n = 3\n",
    "                explained = np.zeros(max_n)\n",
    "                for k in range(1, max_n + 1):\n",
    "                    model = NMF(\n",
    "                        n_components=k,\n",
    "                        init='nndsvda',\n",
    "                        random_state=0,\n",
    "                        max_iter=1000,    # â† bump from 200 to 1 000 (or more)\n",
    "                        tol=1e-4          # â† tighten or loosen the stopping tolerance\n",
    "                    )\n",
    "                    W = model.fit_transform(X)\n",
    "                    H = model.components_\n",
    "                    X_hat = W.dot(H)\n",
    "                    explained[k-1] = 1 - (np.linalg.norm(X - X_hat, 'fro')**2 / total_var)\n",
    "\n",
    "                # 3) Plot the **correct** varianceâ€explained curve\n",
    "                plt.figure(figsize=(6,4))\n",
    "                plt.plot(np.arange(0, max_n+1), np.insert(explained,0,0)*100, 'o-')\n",
    "                plt.xlabel('Number of NMF Components')\n",
    "                plt.ylabel('Variance Explained (%)')\n",
    "                plt.title('NMF: Variance Explained vs. # Components')\n",
    "                plt.grid(True)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{folder_subject_condition_timefreq_analysis}/{current_muscle}_NMF_VAF.png')\n",
    "                plt.show()\n",
    "\n",
    "                # --- 4) 3D state-space of first 3 NMF components ---\n",
    "                k3 = min(3, max_n)\n",
    "                model3 = NMF(\n",
    "                        n_components=k3,\n",
    "                        init='nndsvda',\n",
    "                        random_state=0,\n",
    "                        max_iter=1000,    # â† bump from 200 to 1 000 (or more)\n",
    "                        tol=1e-4          # â† tighten or loosen the stopping tolerance\n",
    "                    )\n",
    "                W3 = model3.fit_transform(X)  # shape (n_timepoints, k3)\n",
    "\n",
    "                fig = plt.figure(figsize=(10, 10))\n",
    "                ax = fig.add_subplot(111, projection='3d')\n",
    "                # ax.plot(W3[:, 0], W3[:, 1], W3[:, 2], lw=1)\n",
    "                ax.scatter3D(W3[:, 0], W3[:, 1], W3[:, 2], c=color_of_plot, s=10, alpha=0.002)\n",
    "                ax.set_xlabel('Comp1')\n",
    "                ax.set_ylabel('Comp2')\n",
    "                ax.set_zlabel('Comp3')\n",
    "                ax.set_title(f'{current_subject} â€“ {current_condition} â€“ {current_muscle}\\nNMF First 3 Components Trajectory')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{folder_subject_condition_timefreq_analysis}/{current_muscle}_NMF_3D.png')\n",
    "                plt.show()\n",
    "\n",
    "                # --- 5) Heatmap of NMF loadings for first 10 components ---\n",
    "                modelN = NMF(\n",
    "                        n_components=max_n,\n",
    "                        init='nndsvda',\n",
    "                        random_state=0,\n",
    "                        max_iter=1000,    # â† bump from 200 to 1 000 (or more)\n",
    "                        tol=1e-4          # â† tighten or loosen the stopping tolerance\n",
    "                    )\n",
    "                Wn = modelN.fit_transform(X)\n",
    "                Hn = modelN.components_  # shape (max_n, n_freqs)\n",
    "\n",
    "                # individual variance per component\n",
    "                indiv_var = explained\n",
    "                # labels = [f'Comp{i+1} ({indiv_var[i]*100:.1f}%)' for i in range(max_n)]\n",
    "                labels = [f'Comp{i+1}' for i in range(max_n)]\n",
    "                df_load_nmf = pd.DataFrame(Hn.T, index=freqs, columns=labels)\n",
    "\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(df_load_nmf, cmap='viridis', yticklabels=5)\n",
    "                plt.xlabel('NMF Components')\n",
    "                plt.ylabel('Frequency (Hz)')\n",
    "                plt.title(f'{current_subject} â€“ {current_condition} â€“ {current_muscle}\\nNMF Loadings (3 factors) by Frequency Band')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{folder_subject_condition_timefreq_analysis}/{current_muscle}_NMF_3comp_loadings_heatmap.png')\n",
    "                plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_psd_vs_group_size(\n",
    "    data,\n",
    "    subject,\n",
    "    condition,\n",
    "    muscles_unique,\n",
    "    input_fsamp,\n",
    "    reps_base=100\n",
    "):\n",
    "    \"\"\"\n",
    "    For each muscle in `muscles_unique`, draw random groups of size k from its MU list\n",
    "    (reps_base//k draws each), compute the summed-CST PSD (0â€“200Hz, normalized),\n",
    "    average across draws, and plot all kâ€curves together.\n",
    "    \"\"\"\n",
    "    for current_muscle in muscles_unique:\n",
    "        color_of_plot = globals()[f\"{current_muscle}_color\"]\n",
    "        # get numeric indices of MUs for this muscle\n",
    "        MU_list = np.where(\n",
    "            np.array(data[subject][condition]['MU_muscle_list']) == current_muscle\n",
    "        )[0]\n",
    "        total_MUs = len(MU_list)\n",
    "        if total_MUs == 0:\n",
    "            continue\n",
    "\n",
    "        ### Compute mean DR of pool considered\n",
    "        # grab the list once\n",
    "        mpg = data[current_subject][current_condition]['muscle_per_grid']\n",
    "        # find all N such that mpg[N] == current_muscle\n",
    "        Ns = [i for i, m in enumerate(mpg) if m == current_muscle]\n",
    "        # now for each such N, extract the mean_firing_rates over all K in that subâ€dict\n",
    "        means_per_N = [\n",
    "            np.nanmean([\n",
    "                entry['mean_firing_rates']\n",
    "                for entry in data[current_subject][current_condition]['MU_properties'][N]\n",
    "            ])\n",
    "            for N in Ns\n",
    "        ]\n",
    "        overall_mean = np.nanmean(means_per_N)\n",
    "        # print(\"Positions N matching current_muscle:\", Ns)\n",
    "        # print(\"Mean firing rates per N:\", means_per_N)\n",
    "        # print(\"Overall mean:\", overall_mean)\n",
    "\n",
    "        # prepare to collect meanâ€PSDs\n",
    "        all_avg_PSDs = []\n",
    "\n",
    "        freqs = None\n",
    "\n",
    "        # for each groupâ€size k = 1..total_MUs\n",
    "        for k in range(1, total_MUs+1):\n",
    "            n_reps = max(1, reps_base // k)\n",
    "            psd_accum = []\n",
    "\n",
    "            for _ in range(n_reps):\n",
    "                # sample k distinct MUs\n",
    "                chosen = np.random.choice(MU_list, size=k, replace=False)\n",
    "                # build CST\n",
    "                cst = np.nansum(\n",
    "                    data[subject][condition]['binary_discharge_matrix_during_selected_plateaus'][chosen],\n",
    "                    axis=0\n",
    "                )\n",
    "                cst = cst - np.nanmean(cst)\n",
    "\n",
    "                # compute PSD\n",
    "                nperseg = min(len(cst), input_fsamp)\n",
    "                f, Pxx = welch(cst, fs=input_fsamp,\n",
    "                               window='hann', nperseg=nperseg,\n",
    "                               noverlap=np.round(nperseg/2).astype(int),\n",
    "                               detrend='constant')\n",
    "                # Pxx = Pxx / np.trapz(Pxx, f)\n",
    "\n",
    "                # keep only up to 200 Hz\n",
    "                mask200 = f <= 200\n",
    "                if freqs is None:\n",
    "                    freqs = f[mask200]\n",
    "                psd_accum.append(Pxx[mask200])\n",
    "                psd_accum[-1] = psd_accum[-1] / np.sum(psd_accum[-1])\n",
    "\n",
    "            # average across this groupâ€size\n",
    "            avg_psd = np.mean(psd_accum, axis=0)\n",
    "            all_avg_PSDs.append((k, avg_psd))\n",
    "\n",
    "        # Get the slopes at each frequencies for every increasing nb of MUs\n",
    "        psd_matrix = np.vstack([avg for (k, avg) in all_avg_PSDs])\n",
    "        delta_PSDs_for_increasing_nb_of_MUs =  np.diff(psd_matrix, axis=0)\n",
    "        print(delta_PSDs_for_increasing_nb_of_MUs.shape)\n",
    "\n",
    "        # now plot them\n",
    "        # 1) compute slope-of-increase statistics\n",
    "        mean_slope = delta_PSDs_for_increasing_nb_of_MUs.mean(axis=0)\n",
    "        std_slope  = delta_PSDs_for_increasing_nb_of_MUs.std(axis=0)\n",
    "        min_slope  = delta_PSDs_for_increasing_nb_of_MUs.min(axis=0)\n",
    "        max_slope  = delta_PSDs_for_increasing_nb_of_MUs.max(axis=0)\n",
    "\n",
    "        # 2) define zoom mask (0â€“5â€‰Hz)\n",
    "        zoom_mask = freqs <= 5\n",
    "        freqs_zoom    = freqs[zoom_mask]\n",
    "        psd_zoom_mat  = psd_matrix[:, zoom_mask]\n",
    "        psd_zoom_max = np.max(np.max(psd_zoom_mat))\n",
    "        slope_zoom    = mean_slope[zoom_mask]\n",
    "        std_slope_z   = std_slope[zoom_mask]\n",
    "        min_slope_z   = min_slope[zoom_mask]\n",
    "        max_slope_z   = max_slope[zoom_mask]\n",
    "\n",
    "        # 3) build figure with GridSpec\n",
    "        fig = plt.figure(figsize=(16,10))\n",
    "        gs  = gridspec.GridSpec(2, 2,\n",
    "                                width_ratios =[4,1],\n",
    "                                height_ratios=[1,1],\n",
    "                                wspace=0.3, hspace=0.3)\n",
    "\n",
    "        ax_psd       = fig.add_subplot(gs[0,0])\n",
    "        ax_slope     = fig.add_subplot(gs[1,0]) #, sharex=ax_psd)\n",
    "        ax_psd_zoom  = fig.add_subplot(gs[0,1]) #, sharey=ax_psd)\n",
    "        ax_slope_zoom= fig.add_subplot(gs[1,1]) #, sharey=ax_slope)\n",
    "\n",
    "        # 4) top-left: all PSD curves\n",
    "        for k, avg in all_avg_PSDs:\n",
    "            alpha = max(0.2, (k/psd_matrix.shape[0])*0.5)\n",
    "            ax_psd.plot(freqs, avg,\n",
    "                        color=color_of_plot,\n",
    "                        alpha=alpha,\n",
    "                        linewidth=1)\n",
    "        # highlight a few key groupâ€sizes\n",
    "        for k, ls in [(1,':'), (10,'--'), (30,'-')]:\n",
    "            if len(psd_matrix)<=k-1:\n",
    "                continue\n",
    "            avg = psd_matrix[k-1]\n",
    "            ax_psd.plot(freqs, avg,\n",
    "                        color='k',\n",
    "                        linestyle=ls,\n",
    "                        linewidth=2,\n",
    "                        label=f\"{k} MU{'s' if k>1 else ''}\")\n",
    "        ax_psd.axvline(x=overall_mean, linestyle='--', color='r', label = f'Mean discharge rate ({overall_mean:.2f} hz)')\n",
    "        ax_psd.set_xlim(0,200)\n",
    "        ax_psd.set_ylabel(\"Normalized PSD\")\n",
    "        ax_psd.set_title(\"Normalized PSD as a function of #MUs\")\n",
    "        ax_psd.legend(loc='upper right', title=\"Group size\")\n",
    "        ax_psd.grid(True, ls='--', alpha=0.3)\n",
    "\n",
    "        # 5) top-right: zoomed PSD\n",
    "        for k, avg in all_avg_PSDs:\n",
    "            alpha = max(0.2, (k/psd_matrix.shape[0])*0.5)\n",
    "            ax_psd_zoom.plot(freqs_zoom, avg[zoom_mask],\n",
    "                            color=color_of_plot,\n",
    "                            alpha=alpha,\n",
    "                            linewidth=1)\n",
    "        # same highlights\n",
    "        for k, ls in [(1,':'), (10,'--'), (psd_matrix.shape[0],'-')]:\n",
    "            if len(psd_matrix)<=k-1:\n",
    "                continue\n",
    "            avg = psd_matrix[k-1]\n",
    "            ax_psd_zoom.plot(freqs_zoom, avg[zoom_mask],\n",
    "                            color='k', linestyle=ls, linewidth=2)\n",
    "        ax_psd_zoom.set_xlim(0,5)\n",
    "        # ax_psd_zoom.set_ylim(0,psd_zoom_max)\n",
    "        ax_psd_zoom.set_xlabel(\"Frequency (Hz)\")\n",
    "        ax_psd_zoom.set_title(\"PSD (0â€“5â€‰Hz)\")\n",
    "        ax_psd_zoom.grid(True, ls='--', alpha=0.3)\n",
    "\n",
    "        # 6) bottom-left: mean slope Â± std, min/max\n",
    "        ax_slope.plot(freqs, mean_slope,\n",
    "                    color=color_of_plot, lw=2, label=\"mean Î”PSD\")\n",
    "        # ax_slope.fill_between(freqs,\n",
    "        #                     mean_slope - std_slope,\n",
    "        #                     mean_slope + std_slope,\n",
    "        #                     color=color_of_plot, alpha=0.3,\n",
    "        #                     label=\"Â±1â€‰SD\")\n",
    "        # ax_slope.plot(freqs, min_slope, ':', color=color_of_plot, alpha=0.5, label=\"min/max\")\n",
    "        # ax_slope.plot(freqs, max_slope, ':', color=color_of_plot, alpha=0.5)\n",
    "        ax_slope.set_xlim(0,200)\n",
    "        ax_slope.set_ylabel(\"Î”PSD per additional MU\")\n",
    "        ax_slope.set_xlabel(\"Frequency (Hz)\")\n",
    "        ax_slope.set_title(\"Mean rate of PSD increase (for each new MN in CST)\")\n",
    "        ax_slope.legend(loc='upper right', fontsize='small')\n",
    "        ax_slope.axhline(y=0, color='k', linestyle='--')\n",
    "        ax_slope.grid(True, ls='--', alpha=0.3)\n",
    "\n",
    "        # 7) bottom-right: zoomed slope\n",
    "        ax_slope_zoom.plot(freqs_zoom, slope_zoom,\n",
    "                        color=color_of_plot, lw=2)\n",
    "        # ax_slope_zoom.fill_between(freqs_zoom,\n",
    "        #                         slope_zoom - std_slope_z,\n",
    "        #                         slope_zoom + std_slope_z,\n",
    "        #                         color=color_of_plot, alpha=0.3)\n",
    "        # ax_slope_zoom.plot(freqs_zoom, min_slope_z, ':', color=color_of_plot, alpha=0.5)\n",
    "        # ax_slope_zoom.plot(freqs_zoom, max_slope_z, ':', color=color_of_plot, alpha=0.5)\n",
    "        ax_slope_zoom.set_xlim(0,5)\n",
    "        ax_slope_zoom.set_xlabel(\"Frequency (Hz)\")\n",
    "        ax_slope.axhline(y=0, color='k', linestyle='--')\n",
    "        ax_slope_zoom.set_title(\"Î”PSD zoom (0â€“5â€‰Hz)\")\n",
    "        ax_slope_zoom.grid(True, ls='--', alpha=0.3)\n",
    "\n",
    "        plt.suptitle(f\"{subject}, {condition} â€“ {current_muscle} ({total_MUs} MUs)\")\n",
    "        plt.tight_layout()\n",
    "        if os.path.exists(folder_subject_condition):\n",
    "            plt.savefig(os.path.join(folder_subject_condition, f\"PSD_results_{current_muscle}.png\"))\n",
    "        else:\n",
    "            print(\"Output folder does not exist; figure not saved\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# COMPUTE POWER SPECTRUM DENSITY OF CST - FOR EACH MUSCLE\n",
    "# --------------------------------------\n",
    "\n",
    "# --------------------------------------\n",
    "# Build dynamic fieldnames, depending on nb of frequency bins\n",
    "# --------------------------------------\n",
    "fieldnames = [\"subject\", \"condition\", \"muscles\",\"frequency\",\"PSD\",\"max_MU_nb\"]\n",
    "# This will collect all rows (from all subjects & conditions) so that\n",
    "# we can write one \"big CSV\" at the end.\n",
    "all_PSD_results = []\n",
    "\n",
    "if perform_analysis_CST_PSD:\n",
    "    print(\"Starting Power Spectrum Density of CST analysis\")\n",
    "    for current_subject in data.keys(): # ['MeJu']:\n",
    "        for current_condition in data[current_subject].keys():\n",
    "            # Create folder to save output (per subject, per condition)\n",
    "            folder_subject_condition = os.path.join(path_of_files,current_subject,\n",
    "                f'{current_subject}_{current_condition}' )\n",
    "            if not os.path.exists(folder_subject_condition):\n",
    "                os.makedirs(folder_subject_condition)\n",
    "            print(f\"     PSD analysis for {current_subject} - {current_condition}\")\n",
    "            # Overwrite local CSV => remove if it already exists\n",
    "            csv_path = os.path.join(folder_subject_condition, \"PSD_analysis_results.csv\")\n",
    "            if os.path.exists(csv_path):\n",
    "                os.remove(csv_path)\n",
    "\n",
    "            # valid_MUs = data[current_subject][current_condition]['valid_MUs_given_selected_plateaus']\n",
    "            # Set all MUs as valid (no need to have a ISI cutoff for COH)\n",
    "            valid_MUs = np.arange(len(data[current_subject][current_condition]['MU_muscle_list']))\n",
    "            muscles_unique = list(set(data[current_subject][current_condition]['MU_muscle_list']))\n",
    "\n",
    "            # Re-organize the binary spike trains from list to matrix (because this is the expected format) = already the case for experimental data, but needs to be done for simulated data\n",
    "            if type(data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'])==list:\n",
    "                data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'] = np.array(data[current_subject][current_condition]['binary_discharge_matrix_during_selected_plateaus'])\n",
    "            # â€” example usage â€”\n",
    "            plot_average_psd_vs_group_size(\n",
    "                data,\n",
    "                current_subject,\n",
    "                current_condition,\n",
    "                muscles_unique,\n",
    "                input_fsamp=2048,\n",
    "                reps_base=100\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE CROSS-CORRELATION BETWEEN CST AND TORQUE (new version)\n",
    "\n",
    "if perform_analysis_CST_torque_correlation:\n",
    "\n",
    "    #%% Helper functions\n",
    "    def normalized_cross_correlation(x, y, max_lag):\n",
    "        lags = np.arange(-max_lag, max_lag + 1, dtype=float)\n",
    "        r = np.zeros(len(lags))\n",
    "        for idx, lag in enumerate(lags):\n",
    "            lag_int = int(lag)\n",
    "            if lag_int < 0:\n",
    "                overlap_x = x[-lag_int:]\n",
    "                overlap_y = y[:len(x) + lag_int]\n",
    "            elif lag_int > 0:\n",
    "                overlap_x = x[:len(x) - lag_int]\n",
    "                overlap_y = y[lag_int:]\n",
    "            else:\n",
    "                overlap_x = x\n",
    "                overlap_y = y\n",
    "            if len(overlap_x) > 1:\n",
    "                r[idx] = np.corrcoef(overlap_x, overlap_y)[0, 1]\n",
    "            else:\n",
    "                r[idx] = 0\n",
    "        return lags, r\n",
    "\n",
    "    def create_binary_matrix_for_plateau_from_list(spike_times_list, mu_indices, window_start, window_end):\n",
    "        \"\"\"\n",
    "        Create a binary matrix for the given window using the spike-times list.\n",
    "        The spike_times_list is a list of arrays (one per motor unit), in the same order as MU_muscle_list.\n",
    "        \"\"\"\n",
    "        L = int(window_end - window_start)\n",
    "        binary_mat = np.zeros((len(mu_indices), L))\n",
    "        for i, mu in enumerate(mu_indices):\n",
    "            spikes = spike_times_list[mu]\n",
    "            # If spikes is a list of arrays, concatenate them\n",
    "            if isinstance(spikes, list):\n",
    "                spikes = np.concatenate([np.asarray(s).ravel() for s in spikes])\n",
    "            else:\n",
    "                spikes = np.asarray(spikes).ravel()\n",
    "            in_window = spikes[(spikes >= window_start) & (spikes < window_end)] - window_start\n",
    "            indices = in_window.astype(int)\n",
    "            indices = indices[(indices >= 0) & (indices < L)]\n",
    "            binary_mat[i, indices] = 1\n",
    "        return binary_mat\n",
    "\n",
    "    #%% Main processing\n",
    "    csv_rows = []\n",
    "    correlation_info_dict = {}\n",
    "\n",
    "    for current_subject in data.keys():\n",
    "        print(f\"Processing subject {current_subject}...\")\n",
    "        correlation_info_dict[current_subject] = {}\n",
    "        subject_rows = []\n",
    "        \n",
    "        subject_folder = os.path.join(path_of_files, current_subject)\n",
    "        os.makedirs(subject_folder, exist_ok=True)\n",
    "        \n",
    "        conditions = list(data[current_subject].keys())\n",
    "        fig = plt.figure(figsize=(30, 8 * len(conditions)))\n",
    "        gs = gridspec.GridSpec(len(conditions), 2, width_ratios=[4, 1], wspace=0.15)\n",
    "        \n",
    "        for i, current_condition in enumerate(conditions):\n",
    "            print(f\"  Processing condition {current_condition}...\")\n",
    "            correlation_info_dict[current_subject][current_condition] = {}\n",
    "            \n",
    "            folder_subject_condition = os.path.join(subject_folder, f\"{current_subject}_{current_condition}\")\n",
    "            os.makedirs(folder_subject_condition, exist_ok=True)\n",
    "            \n",
    "            # Identify plateau segments\n",
    "            plateau_samples_temp = data[current_subject][current_condition]['plateau_samples']\n",
    "            plateau_separations = np.where(np.diff(plateau_samples_temp) > 1)[0]\n",
    "            plateau_segments = []\n",
    "            start_idx = 0\n",
    "            for sep in plateau_separations:\n",
    "                plateau_segments.append((plateau_samples_temp[start_idx], plateau_samples_temp[sep]))\n",
    "                start_idx = sep + 1\n",
    "            plateau_segments.append((plateau_samples_temp[start_idx], plateau_samples_temp[-1]))\n",
    "            \n",
    "            # Prepare subplots\n",
    "            ax_signals = fig.add_subplot(gs[i, 0])\n",
    "            ax_signals.set_title(f\"{current_condition}: Torque and CST\")\n",
    "            ax_corr = fig.add_subplot(gs[i, 1])\n",
    "            ax_corr.set_title(f\"{current_condition}: Cross-correlation\")\n",
    "            ax_corr.set_xlabel(\"Time lag (s)\")\n",
    "            ax_corr.set_ylabel(\"Correlation coefficient\")\n",
    "            ax_corr.axvline(0, color='grey', linestyle='--')\n",
    "            ax_corr.axhline(0, color='grey', linestyle='--')\n",
    "            ax_corr.set_xlim(-0.5, 0.5)\n",
    "            ax_corr.set_ylim(-0.5, 1)\n",
    "            \n",
    "            # 1) Apply force low-pass filter to the entire force signal once\n",
    "            full_force = data[current_subject][current_condition]['force']\n",
    "            force_filtered = lowpass_filter(full_force, fsamp=input_fsamp, \n",
    "                                            freq_cutoff=force_lowpass_cutoff, filter_order=4)\n",
    "            \n",
    "            # 2) Plot plateaus in a concatenated manner (no blank spaces)\n",
    "            force_display_list = []\n",
    "            time_display_list = []\n",
    "            running_offset = 0.0\n",
    "            \n",
    "            for (p_start, p_end) in plateau_segments:\n",
    "                ext_start = max(0, p_start - edges_to_remove_samples)\n",
    "                ext_end = p_end + edges_to_remove_samples\n",
    "                force_seg = force_filtered[ext_start:ext_end].copy()\n",
    "                force_seg = detrend(force_seg)\n",
    "                force_seg = force_seg / (np.std(force_seg) if np.std(force_seg) != 0 else 1)\n",
    "                start_in_array = p_start - ext_start\n",
    "                end_in_array = p_end - ext_start\n",
    "                if end_in_array > len(force_seg):\n",
    "                    end_in_array = len(force_seg)\n",
    "                final_force = force_seg[start_in_array:end_in_array]\n",
    "                L = len(final_force)\n",
    "                t_array = np.arange(L) / input_fsamp + running_offset\n",
    "                force_display_list.append(final_force)\n",
    "                time_display_list.append(t_array)\n",
    "                running_offset += L / input_fsamp\n",
    "            \n",
    "            force_concat = np.concatenate(force_display_list)\n",
    "            t_concat = np.concatenate(time_display_list)\n",
    "            ax_signals.plot(t_concat, force_concat, color='k', label='Torque', alpha=0.5, linewidth=1)\n",
    "            \n",
    "            # Add vertical lines for each plateau boundary (using concatenated time)\n",
    "            running_time = 0.0\n",
    "            for (p_start, p_end) in plateau_segments:\n",
    "                plateau_length = p_end - p_start\n",
    "                running_time += plateau_length / input_fsamp\n",
    "                ax_signals.axvline(running_time, color='black', linestyle='--', linewidth=1)\n",
    "            \n",
    "            # 3) Process muscle combinations\n",
    "            mu_muscle_list = data[current_subject][current_condition]['MU_muscle_list']\n",
    "            unique_muscles = np.unique(mu_muscle_list)\n",
    "            # Only add combined \"all\" if more than one unique muscle exists.\n",
    "            if len(unique_muscles) > 1:\n",
    "                muscle_combinations = list(unique_muscles) + ['all']\n",
    "            else:\n",
    "                muscle_combinations = list(unique_muscles)\n",
    "            \n",
    "            for m_idx, muscle in enumerate(muscle_combinations):\n",
    "                if muscle != 'all':\n",
    "                    indices = [idx for idx, m in enumerate(mu_muscle_list) if m == muscle]\n",
    "                    muscle_label = muscle\n",
    "                else:\n",
    "                    indices = list(range(len(mu_muscle_list)))\n",
    "                    # Combine the two unique muscle names for the 'all' case\n",
    "                    muscle_label = f\"{unique_muscles[0]}-{unique_muscles[1]}\" if len(unique_muscles) > 1 else unique_muscles[0]\n",
    "                \n",
    "                correlation_info_dict[current_subject][current_condition][muscle_label] = {}\n",
    "                \n",
    "                # Determine figure color adaptively\n",
    "                if muscle != 'all':\n",
    "                    figure_color = globals()[f\"{muscle}_color\"]\n",
    "                else:\n",
    "                    if len(unique_muscles) >= 2:\n",
    "                        color1 = globals().get(f\"{unique_muscles[0]}_color\", (0,0,1))\n",
    "                        color2 = globals().get(f\"{unique_muscles[1]}_color\", (1,0,0))\n",
    "                        figure_color = mix_colors(color1, color2)\n",
    "                    else:\n",
    "                        figure_color = globals().get(f\"{unique_muscles[0]}_color\", (0,0,1))\n",
    "                \n",
    "                # Use MU_spike_times_list\n",
    "                spike_times_list = data[current_subject][current_condition]['MU_spike_times_list']\n",
    "                mu_indices = ([int(mu) for mu, m in enumerate(mu_muscle_list) if m == muscle]\n",
    "                              if muscle != 'all'\n",
    "                              else list(range(len(mu_muscle_list))))\n",
    "                n_MUs = len(mu_indices)\n",
    "                \n",
    "                print(f\"    Processing muscle {muscle_label} (n_MUs={n_MUs}) for subject {current_subject}, condition {current_condition}...\")\n",
    "                \n",
    "                plateau_results = []\n",
    "                cst_display_list = []\n",
    "                cst_time_list = []\n",
    "                running_offset_cst = 0.0\n",
    "                \n",
    "                for p_idx, (p_start, p_end) in enumerate(plateau_segments):\n",
    "                    ext_start = max(0, p_start - edges_to_remove_samples)\n",
    "                    ext_end = p_end + edges_to_remove_samples\n",
    "                    force_seg = force_filtered[ext_start:ext_end].copy()\n",
    "                    force_seg = detrend(force_seg)\n",
    "                    force_seg = force_seg / (np.std(force_seg) if np.std(force_seg) != 0 else 1)\n",
    "                    \n",
    "                    binary_mat = create_binary_matrix_for_plateau_from_list(spike_times_list, mu_indices, ext_start, ext_end)\n",
    "                    CST_seg = np.nansum(binary_mat, axis=0)\n",
    "                    \n",
    "                    plateau_freq_results = {}\n",
    "                    for lowpass_cutoff_i in low_pass_freqs_to_try:\n",
    "                        print(f\"      Subject={current_subject}, cond={current_condition}, muscle={muscle_label}, cutoff={lowpass_cutoff_i} Hz, plateau={p_idx+1}\")\n",
    "                        start_in_array = p_start - ext_start\n",
    "                        end_in_array = p_end - ext_start\n",
    "                        smoothed_CST = lowpass_filter(CST_seg, fsamp=input_fsamp, \n",
    "                                freq_cutoff=lowpass_cutoff_i, filter_order=4)\n",
    "                        if end_in_array > len(smoothed_CST):\n",
    "                            end_in_array = len(smoothed_CST)\n",
    "                        final_cst = smoothed_CST[start_in_array:end_in_array]\n",
    "                        final_cst = detrend(final_cst)\n",
    "                        final_cst = final_cst / (np.std(final_cst) if np.std(final_cst) != 0 else 1)\n",
    "                        final_force = force_seg[start_in_array:end_in_array]\n",
    "                        \n",
    "                        lags_samples, corr_coeff = normalized_cross_correlation(final_force, final_cst, cross_correl_max_lag)\n",
    "                        lags = lags_samples / float(input_fsamp)\n",
    "                        max_idx = np.argmax(corr_coeff)\n",
    "                        best_corr = corr_coeff[max_idx]\n",
    "                        best_lag = lags[max_idx]\n",
    "                        \n",
    "                        plateau_freq_results[lowpass_cutoff_i] = {\n",
    "                            'max_correlation': best_corr,\n",
    "                            'time_delay': best_lag,\n",
    "                            'lag_vector': lags,\n",
    "                            'correlation_vector': corr_coeff,\n",
    "                            'smoothed_CST': final_cst,\n",
    "                            'force': final_force,\n",
    "                            'n_MUs': n_MUs\n",
    "                        }\n",
    "                        \n",
    "                        subject_rows.append({\n",
    "                            'subject': current_subject,\n",
    "                            'condition': current_condition,\n",
    "                            'muscle': muscle_label,\n",
    "                            'plateau': p_idx+1,\n",
    "                            'n_MUs': n_MUs,\n",
    "                            'low_pass_frequency': lowpass_cutoff_i,\n",
    "                            'max_correlation': best_corr,\n",
    "                            'time_delay': best_lag\n",
    "                        })\n",
    "                    plateau_results.append(plateau_freq_results)\n",
    "                    \n",
    "                    # For plotting the CST at the arbitrary frequency\n",
    "                    final_cst_arbitrary = plateau_freq_results[arbitrary_lowpass_freq]['smoothed_CST']\n",
    "                    L_cst = len(final_cst_arbitrary)\n",
    "                    t_cst = np.arange(L_cst) / input_fsamp + running_offset_cst\n",
    "                    cst_display_list.append(final_cst_arbitrary)\n",
    "                    cst_time_list.append(t_cst)\n",
    "                    running_offset_cst += L_cst / input_fsamp\n",
    "                \n",
    "                # Average across plateaus for each frequency\n",
    "                avg_results = {}\n",
    "                for lowpass_cutoff_i in low_pass_freqs_to_try:\n",
    "                    corr_list = np.array([pr[lowpass_cutoff_i]['correlation_vector'] for pr in plateau_results])\n",
    "                    avg_corr = np.mean(corr_list, axis=0)\n",
    "                    cst_list = [pr[lowpass_cutoff_i]['smoothed_CST'] for pr in plateau_results]\n",
    "                    min_len = min(len(x) for x in cst_list)\n",
    "                    trimmed_cst = [x[:min_len] for x in cst_list]\n",
    "                    cst_array = np.array(trimmed_cst)\n",
    "                    avg_smoothed = np.mean(cst_array, axis=0)\n",
    "                    \n",
    "                    avg_lags = plateau_results[0][lowpass_cutoff_i]['lag_vector']\n",
    "                    max_idx = np.argmax(avg_corr)\n",
    "                    best_corr_avg = avg_corr[max_idx]\n",
    "                    best_lag_avg = avg_lags[max_idx]\n",
    "                    \n",
    "                    avg_results[lowpass_cutoff_i] = {\n",
    "                        'max_correlation': best_corr_avg,\n",
    "                        'time_delay': best_lag_avg,\n",
    "                        'lag_vector': avg_lags,\n",
    "                        'correlation_vector': avg_corr,\n",
    "                        'smoothed_CST': avg_smoothed,\n",
    "                        'n_MUs': n_MUs\n",
    "                    }\n",
    "                    subject_rows.append({\n",
    "                        'subject': current_subject,\n",
    "                        'condition': current_condition,\n",
    "                        'muscle': muscle_label,\n",
    "                        'plateau': 0,\n",
    "                        'n_MUs': n_MUs,\n",
    "                        'low_pass_frequency': lowpass_cutoff_i,\n",
    "                        'max_correlation': best_corr_avg,\n",
    "                        'time_delay': best_lag_avg\n",
    "                    })\n",
    "                \n",
    "                correlation_info_dict[current_subject][current_condition][muscle_label] = {\n",
    "                    'per_plateau': plateau_results,\n",
    "                    'average': avg_results\n",
    "                }\n",
    "                \n",
    "                # Plot if muscle is selected\n",
    "                if CST_torque_corr_display_only_selected_muscles == 'all' or muscle_label in CST_torque_corr_display_only_selected_muscles:\n",
    "                    cst_concat = np.concatenate(cst_display_list)\n",
    "                    t_cst_concat = np.concatenate(cst_time_list)\n",
    "                    \n",
    "                    ax_signals.plot(t_cst_concat, cst_concat, color=figure_color, linestyle='-', linewidth=2,\n",
    "                                    alpha=0.8, label=f'{muscle_label} CST (nMUs={n_MUs}, {arbitrary_lowpass_freq} Hz)')\n",
    "                    \n",
    "                    for p_idx, pr in enumerate(plateau_results):\n",
    "                        ax_corr.plot(pr[arbitrary_lowpass_freq]['lag_vector'],\n",
    "                                     pr[arbitrary_lowpass_freq]['correlation_vector'],\n",
    "                                     color=figure_color, linestyle='-', linewidth=1, alpha=0.3)\n",
    "                    avg_corr = avg_results[arbitrary_lowpass_freq]['correlation_vector']\n",
    "                    avg_lags = avg_results[arbitrary_lowpass_freq]['lag_vector']\n",
    "                    ax_corr.plot(avg_lags, avg_corr, color=figure_color, linestyle='-', linewidth=3, alpha=1,\n",
    "                                 label=f'{muscle_label} avg CC (nMUs={n_MUs}, {arbitrary_lowpass_freq} Hz)')\n",
    "                    max_idx = np.argmax(avg_corr)\n",
    "                    best_corr_avg = avg_corr[max_idx]\n",
    "                    best_lag_avg = avg_lags[max_idx]\n",
    "                    ax_corr.plot(best_lag_avg, best_corr_avg, 'ro')\n",
    "                    ax_corr.axhline(best_corr_avg, color='r', linestyle=':')\n",
    "                    ax_corr.annotate('', xy=(0, best_corr_avg), xytext=(best_lag_avg, best_corr_avg),\n",
    "                                     arrowprops=dict(arrowstyle='<->', color='black', lw=1, alpha=0.85))\n",
    "                    \n",
    "                    total_muscles = len(muscle_combinations)\n",
    "                    y_pos = 1 - (m_idx + 1) / (total_muscles + 1)\n",
    "                    annot_text = f\"{muscle_label}\\nr = {best_corr_avg:.2f}\\ndelay = {best_lag_avg:.3f}s\"\n",
    "                    ax_corr.text(0.98, y_pos, annot_text, transform=ax_corr.transAxes,\n",
    "                                ha='right', va='top', color=figure_color,\n",
    "                                bbox=dict(facecolor='white', edgecolor='none', pad=2))\n",
    "            \n",
    "            ax_signals.set_xlabel(\"Time (s)\")\n",
    "            ax_signals.set_ylabel(\"a.u.\")\n",
    "            ax_signals.legend(fontsize='small')\n",
    "            ax_corr.legend(fontsize='small')\n",
    "        \n",
    "        fig.suptitle(f\"{current_subject} - Torque and CST correlation\", fontsize=16)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        fig_path = os.path.join(subject_folder, f\"{current_subject}_Torque_CST_correlation.png\")\n",
    "        fig.savefig(fig_path, dpi=300)\n",
    "        if display_figures_inline:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        \n",
    "        # Save per-subject CSV\n",
    "        df_subject = pd.DataFrame(subject_rows)\n",
    "        csv_path = os.path.join(subject_folder, f\"{current_subject}_Correl_CST_torque.csv\")\n",
    "        df_subject.to_csv(csv_path, index=False)\n",
    "        print(f\"Saved CSV for subject {current_subject} to {csv_path}\")\n",
    "\n",
    "    # Build global CSV\n",
    "    rows = []\n",
    "    for subject in correlation_info_dict:\n",
    "        for condition in correlation_info_dict[subject]:\n",
    "            for muscle in correlation_info_dict[subject][condition]:\n",
    "                per_plateau = correlation_info_dict[subject][condition][muscle]['per_plateau']\n",
    "                for p_idx, plateau_results in enumerate(per_plateau):\n",
    "                    for freq in plateau_results:\n",
    "                        info = plateau_results[freq]\n",
    "                        rows.append({\n",
    "                            'subject': subject,\n",
    "                            'condition': condition,\n",
    "                            'muscle': muscle,\n",
    "                            'plateau': p_idx+1,\n",
    "                            'n_MUs': info.get('n_MUs', 0),\n",
    "                            'low_pass_frequency': freq,\n",
    "                            'max_correlation': info['max_correlation'],\n",
    "                            'time_delay': info['time_delay']\n",
    "                        })\n",
    "                avg = correlation_info_dict[subject][condition][muscle]['average']\n",
    "                for freq in avg:\n",
    "                    info = avg[freq]\n",
    "                    rows.append({\n",
    "                        'subject': subject,\n",
    "                        'condition': condition,\n",
    "                        'muscle': muscle,\n",
    "                        'plateau': 0,\n",
    "                        'n_MUs': info.get('n_MUs', 0),\n",
    "                        'low_pass_frequency': freq,\n",
    "                        'max_correlation': info['max_correlation'],\n",
    "                        'time_delay': info['time_delay']\n",
    "                    })\n",
    "\n",
    "    df_global = pd.DataFrame(rows)\n",
    "    global_csv_path = os.path.join(path_of_files, \"Global_Correl_CST_torque.csv\")\n",
    "    df_global.to_csv(global_csv_path, index=False)\n",
    "    print(f\"Global CSV saved to {global_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapping_RI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
