{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "import h5py\n",
    "\n",
    "from simulator import SimulationParameters, run_simulation\n",
    "from analyzer import AnalyzesParams, analyze_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb1d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder_to_load_from = 'C:\\\\Users\\\\franc\\\\Documents\\\\Mapping_Recurrent_Inhibition_minimal_datasets_to_run_scripts\\\\Example_small_simulation_batch'\n",
    "within_between_or_all_muscle_pairs = \"all\" # \"all\" # \"all\" \"within\" or \"between\"\n",
    "nb_of_MUs_for_COH = 10 # 5\n",
    "coh_band = (15, 35) # in Hz\n",
    "# \"all\" consider all muscle_pair types\n",
    "# \"between\" consider only muscle_pair types where poolX<->poolY (or vice versa, basically when both pools are different). Drop muscle_pairs where poolX<->poolX\n",
    "# \"within\" consider only muscle_pair types where poolX<->poolX (or vice versa, basically when both pools are the same). Drop muscle_pairs where poolX<->poolY (or vice-versa)\n",
    "load_hdf5_sim_output_for_MNs_electrophysiological_properties = True # Need to be \"True\" for some analysis regarding MN properties (so they get added in the big csv), but takes some time to load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e39d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- utilities -----------------\n",
    "def deep_equal(a, b):\n",
    "    \"\"\"Recursively compare a and b, supporting np.ndarray, dict, list/tuple, and scalars.\"\"\"\n",
    "    if isinstance(a, np.ndarray) and isinstance(b, np.ndarray):\n",
    "        return np.array_equal(a, b)\n",
    "    if isinstance(a, dict) and isinstance(b, dict):\n",
    "        if set(a.keys()) != set(b.keys()):\n",
    "            return False\n",
    "        return all(deep_equal(a[k], b[k]) for k in a)\n",
    "    if isinstance(a, (list, tuple)) and isinstance(b, (list, tuple)):\n",
    "        if len(a) != len(b):\n",
    "            return False\n",
    "        return all(deep_equal(x, y) for x, y in zip(a, b))\n",
    "    return a == b\n",
    "\n",
    "def _load_priors(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def _select_prior_from_files(files, folder_label):\n",
    "    \"\"\"\n",
    "    Given a list of prior-pkl files in one folder, run consistency checks.\n",
    "    Returns the chosen prior object (first file) and prints/warns as needed.\n",
    "    \"\"\"\n",
    "    files = sorted(files)\n",
    "    if len(files) == 0:\n",
    "        return None\n",
    "\n",
    "    if len(files) == 1:\n",
    "        return _load_priors(files[0])\n",
    "\n",
    "    warnings.warn(\n",
    "        f\"Multiple prior‐pkls found in {folder_label!r}; loading all of them and comparing.\",\n",
    "        UserWarning\n",
    "    )\n",
    "    priors_list = [_load_priors(p) for p in files]\n",
    "\n",
    "    compare_keys = [\n",
    "        'fixed_parameters',\n",
    "        'free_parameter_bounds',\n",
    "        'use_posterior_as_priors',\n",
    "    ]\n",
    "    extra_keys = [\n",
    "        'override_free_parameter_bounds_from_posterior',\n",
    "        'muscle_pair_posterior',\n",
    "        'intensity_posterior',\n",
    "        'path_for_posterior_samples',\n",
    "    ]\n",
    "\n",
    "    mismatch_found = False\n",
    "\n",
    "    # compare the standard keys\n",
    "    for key in compare_keys:\n",
    "        vals = [d.get(key) for d in priors_list]\n",
    "        if not all(deep_equal(v, vals[0]) for v in vals[1:]):\n",
    "            mismatch_found = True\n",
    "            warnings.warn(\n",
    "                f\"Mismatch in key {key!r} across prior‐pkls in {folder_label!r}:\\n\"\n",
    "                + \"\\n\".join(\n",
    "                    f\"  • {os.path.basename(files[i])}: {vals[i]!r}\"\n",
    "                    for i in range(len(vals))\n",
    "                ),\n",
    "                UserWarning\n",
    "            )\n",
    "\n",
    "    # if all use posterior-as-priors, compare extra keys too\n",
    "    if all(d.get('use_posterior_as_priors') for d in priors_list):\n",
    "        for key in extra_keys:\n",
    "            vals = [d.get(key) for d in priors_list]\n",
    "            if not all(deep_equal(v, vals[0]) for v in vals[1:]):\n",
    "                mismatch_found = True\n",
    "                warnings.warn(\n",
    "                    f\"Mismatch in key {key!r} across prior‐pkls in {folder_label!r}:\\n\"\n",
    "                    + \"\\n\".join(\n",
    "                        f\"  • {os.path.basename(files[i])}: {vals[i]!r}\"\n",
    "                        for i in range(len(vals))\n",
    "                    ),\n",
    "                    UserWarning\n",
    "                )\n",
    "\n",
    "    if not mismatch_found:\n",
    "        print(f\"✅ All prior‐pkls in {folder_label!r} match on compared keys; using the first one.\")\n",
    "\n",
    "    return priors_list[0]\n",
    "\n",
    "# ----------------- main logic -----------------\n",
    "priors = {}\n",
    "\n",
    "# 1) Look directly in the parent folder\n",
    "pattern_direct = os.path.join(parent_folder_to_load_from, \"*prior*.pkl\")\n",
    "matching_files_direct = glob.glob(pattern_direct)\n",
    "\n",
    "if matching_files_direct:\n",
    "    # Same behavior as before: key is the parent folder path\n",
    "    chosen = _select_prior_from_files(matching_files_direct, parent_folder_to_load_from)\n",
    "    if chosen is not None:\n",
    "        priors[f'{parent_folder_to_load_from}'] = chosen\n",
    "else:\n",
    "    # 2) No priors directly inside parent; scan immediate subfolders\n",
    "    found_any = False\n",
    "    for entry in os.scandir(parent_folder_to_load_from):\n",
    "        if not entry.is_dir():\n",
    "            continue\n",
    "        subfolder = entry.path\n",
    "        pattern_sub = os.path.join(subfolder, \"*prior*.pkl\")\n",
    "        files = glob.glob(pattern_sub)\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        found_any = True\n",
    "        chosen = _select_prior_from_files(files, subfolder)\n",
    "        if chosen is not None:\n",
    "            # Key = subfolder name (as requested)\n",
    "            priors[os.path.basename(subfolder)] = chosen\n",
    "\n",
    "    if not found_any:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No .pkl file with 'prior' in its name found in {parent_folder_to_load_from!r} \"\n",
    "            f\"or its immediate subfolders.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analyses(parent_folder, filename='analysis_output.pkl',\n",
    "                  params_file='sim_parameters.json',\n",
    "                  recursive=False,\n",
    "                  load_hdf5_props=False,              \n",
    "                  hdf5_filename='simulation_output.h5'\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Load all 'filename' pickles under parent_folder, and for each one also\n",
    "    load the accompanying JSON params file into ['sim_parameters'].\n",
    "\n",
    "    If load_hdf5_props=True and an HDF5 file (simulation_output.h5) is present in the\n",
    "    same folder, read group 'motoneurons_properties' and attach it under\n",
    "    data['motoneurons_properties'] as a dict {prop_name: 1D np.ndarray}.\n",
    "    \"\"\"\n",
    "    parent = Path(parent_folder)\n",
    "    analysis_output = {}\n",
    "\n",
    "    if recursive:\n",
    "        pkl_paths = list(parent.rglob(filename))\n",
    "    else:\n",
    "        pkl_paths = [\n",
    "            subdir / filename\n",
    "            for subdir in parent.iterdir()\n",
    "            if subdir.is_dir() and (subdir / filename).exists()\n",
    "        ]\n",
    "\n",
    "    print(\"Loading analysis outputs...\")\n",
    "    for pkl_path in pkl_paths:\n",
    "        key = pkl_path.parent.name\n",
    "\n",
    "        # 1) load the pickle\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # 2) load the JSON params\n",
    "        params_path = pkl_path.parent / params_file\n",
    "        if params_path.exists():\n",
    "            with open(params_path, 'r', encoding='utf-8') as jf:\n",
    "                data['sim_parameters'] = json.load(jf)\n",
    "        else:\n",
    "            data['sim_parameters'] = None\n",
    "            print(f\"Warning: '{params_file}' not found in {pkl_path.parent}\")\n",
    "\n",
    "        # 3) (optional) load HDF5 motoneuron properties\n",
    "        if load_hdf5_props:\n",
    "            h5_path = pkl_path.parent / hdf5_filename\n",
    "            if h5_path.exists():\n",
    "                try:\n",
    "                    with h5py.File(h5_path, 'r') as hf:\n",
    "                        grp = hf.get('motoneurons_properties')\n",
    "                        if grp is not None:\n",
    "                            props = {name: np.asarray(grp[name][()]).squeeze()\n",
    "                                     for name in grp.keys()}\n",
    "                            data['motoneurons_properties'] = props\n",
    "                        else:\n",
    "                            data['motoneurons_properties'] = None\n",
    "                            print(f\"Warning: 'motoneurons_properties' not found in {h5_path}\")\n",
    "                except Exception as e:\n",
    "                    data['motoneurons_properties'] = None\n",
    "                    print(f\"Warning: failed reading {h5_path}: {e}\")\n",
    "            else:\n",
    "                data['motoneurons_properties'] = None  # not present here\n",
    "\n",
    "        analysis_output[key] = data\n",
    "\n",
    "    mode = 'entire hierarchy' if recursive else 'direct subfolders'\n",
    "    count = len(pkl_paths)\n",
    "    print(f\"Loaded {count} file{'s' if count!=1 else ''} by searching {mode} of '{parent_folder}'\")\n",
    "\n",
    "    return analysis_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411232d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_output = load_analyses(\n",
    "    parent_folder_to_load_from,\n",
    "    recursive=True,\n",
    "    load_hdf5_props=load_hdf5_sim_output_for_MNs_electrophysiological_properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70350a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_dataframe(\n",
    "    analysis_output,\n",
    "    sim_params_keys=None,\n",
    "    firing_rate_stats=None,\n",
    "    connectivity_keys=None,\n",
    "    graph_measures_keys=None,\n",
    "    coherence_keys=None,\n",
    "    include_cross_hist=False,\n",
    "    # mean coherence options ---\n",
    "    coherence_band: tuple[float, float] | None = None,   # e.g. (15, 35)\n",
    "    coherence_group_N: int | None = None,                # e.g. 5 (nb of motor units)\n",
    "    coherence_source: str = \"Coherence_total\",           # where to read from in analysis_output['Coherence']\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a DataFrame of MN rows, optionally exploded by Cross_histograms,\n",
    "    and include:\n",
    "      - inhibited_by_ground_truth_receiving\n",
    "      - inhibiting_ground_truth_delivering\n",
    "      - asymmetry_ground_truth_diff\n",
    "      - asymmetry_ground_truth_ratio\n",
    "      - ISI_cov (new column, pulled from data['Firing_rates']['isi_cov_MN'])\n",
    "    \"\"\"\n",
    "    sim_params_keys     = sim_params_keys     or []\n",
    "    firing_rate_stats   = firing_rate_stats   or []\n",
    "    connectivity_keys   = connectivity_keys   or []\n",
    "    graph_measures_keys = graph_measures_keys or []\n",
    "    coherence_keys      = coherence_keys      or []\n",
    "\n",
    "    CROSS_DIRECTIONS = ['inhibited','inhibiting','combined']\n",
    "    FWD_BWD_MEASURES = ['raw_area','corrected_area','z_score','p_val','null_mean','null_std']\n",
    "    ASYM_MEASURES    = [\n",
    "        'raw_area_asym_ratio','corrected_area_asym_ratio',\n",
    "        'raw_area_asym_diff','corrected_area_asym_diff'\n",
    "    ]\n",
    "    OTHER_MEASURES   = [\n",
    "        'sync_height','sync_time',\n",
    "        'delay_forward_IPSP','delay_backward_IPSP',\n",
    "        'r2_full','r2_base','n_spikes',\n",
    "        'hist_plateau_duration','proportion_of_prob_within_plateau_duration',\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "    for sim_name, data in analysis_output.items():\n",
    "        params      = data.get('sim_parameters', {})\n",
    "        total_nb    = params.get('total_nb_motoneurons', 0)\n",
    "        nb_per_pool = params.get('nb_motoneurons_per_pool')\n",
    "        try:\n",
    "            nb_per_pool = int(nb_per_pool)\n",
    "        except Exception:\n",
    "            nb_per_pool = None\n",
    "\n",
    "        fr_dict    = data.get('Firing_rates', {}).get('MN', {})        # e.g. {'mean': { 'MN_0': …, … }, …}\n",
    "        cov_mn_dict = data.get('Firing_rates', {}).get('isi_cov_MN', {})  # e.g. { 'MN_0': 0.31, 'MN_1': NaN, … }\n",
    "        conn       = data.get('Ground_truth_RI_connectivity', {})\n",
    "        graph      = data.get('Graph_theory_connectivity_measures', {})\n",
    "        coh        = data.get('Coherence', {})\n",
    "        cross      = data.get('Cross_histograms', {})\n",
    "\n",
    "        per_rec = conn.get('per_pool_received', {})\n",
    "        per_del = conn.get('per_pool_delivered', {})\n",
    "\n",
    "        # --- Prepare mean coherence in a band for groups of N MUs ---\n",
    "        # We’ll compute per (simulation, pool-pair) and cache results to avoid repeating work.\n",
    "        coh_colname = None\n",
    "        compute_band_mean = None  # function(pool_key: str) -> float\n",
    "\n",
    "        if (coherence_band is not None) and (coherence_group_N is not None) and coh:\n",
    "            fmin, fmax = coherence_band\n",
    "            coh_block = coh.get(coherence_source, {})\n",
    "            freqs = np.asarray(coh_block.get('frequencies', []), dtype=float)\n",
    "\n",
    "            # Build frequency mask safely\n",
    "            if freqs.size == 0:\n",
    "                freq_mask = None\n",
    "            else:\n",
    "                freq_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "                if not np.any(freq_mask):\n",
    "                    freq_mask = None\n",
    "\n",
    "            # Prepare output column name once\n",
    "            coh_colname = f\"coherence_{int(fmin)}-{int(fmax)}hz_{int(coherence_group_N)}MN\"\n",
    "\n",
    "            cache = {}\n",
    "            def compute_band_mean(pool_key_with_dash: str) -> float:\n",
    "                \"\"\"\n",
    "                pool_key_with_dash: e.g. 'pool_0-pool_1' or 'pool_0-pool_0'\n",
    "                returns mean coherence over [fmin,fmax] for group size N, or np.nan\n",
    "                \"\"\"\n",
    "                if freq_mask is None:\n",
    "                    return np.nan\n",
    "                if pool_key_with_dash in cache:\n",
    "                    return cache[pool_key_with_dash]\n",
    "\n",
    "                # Try as-is, then reversed order if missing (for between-pool pairs)\n",
    "                spec = coh_block.get(pool_key_with_dash)\n",
    "                if spec is None and '-' in pool_key_with_dash:\n",
    "                    a, b = pool_key_with_dash.split('-', 1)\n",
    "                    spec = coh_block.get(f\"{b}-{a}\")\n",
    "\n",
    "                if spec is None:\n",
    "                    cache[pool_key_with_dash] = np.nan\n",
    "                    return np.nan\n",
    "\n",
    "                arr_by_N = spec.get(int(coherence_group_N))\n",
    "                if arr_by_N is None:\n",
    "                    cache[pool_key_with_dash] = np.nan\n",
    "                    return np.nan\n",
    "\n",
    "                vals = np.asarray(arr_by_N, dtype=float)\n",
    "                if vals.shape[0] != freqs.shape[0]:\n",
    "                    # shape mismatch → safer to bail out\n",
    "                    cache[pool_key_with_dash] = np.nan\n",
    "                    return np.nan\n",
    "\n",
    "                val = float(np.nanmean(vals[freq_mask]))\n",
    "                cache[pool_key_with_dash] = val\n",
    "                return val\n",
    "        # --- end of coherence per sim and pool pair ---\n",
    "\n",
    "\n",
    "        if include_cross_hist and cross:\n",
    "            # “explode” each cross‐hist entry\n",
    "            for pool_pair, neuron_map in cross.items():\n",
    "                # pool_pair is like \"pool_0<->pool_1\", so pool_B is the second pool\n",
    "                if '<->' not in pool_pair:\n",
    "                    continue # case when cross-histograms are saved: there is another item in the dict that is not of the format 'pool_A<->pool_B'\n",
    "                _, pool_B = pool_pair.split('<->')\n",
    "\n",
    "                for neuron_idx, dir_dict in neuron_map.items():\n",
    "                    # build base row for this MN in this pool_pair\n",
    "                    recv = per_rec.get(pool_B, np.full(total_nb, np.nan))[neuron_idx] \\\n",
    "                           if pool_B in per_rec else np.nan\n",
    "                    delv = per_del.get(pool_B, np.full(total_nb, np.nan))[neuron_idx] \\\n",
    "                           if pool_B in per_del else np.nan\n",
    "\n",
    "                    # pull that MN’s ISI‐cov if it exists, otherwise NaN\n",
    "                    # the keys in isi_cov_MN dict are strings like 'MN_0', 'MN_1', …\n",
    "                    cov_key = f\"MN_{neuron_idx}\"\n",
    "                    isi_cov_mn = cov_mn_dict.get(cov_key, np.nan)\n",
    "\n",
    "                    base = {\n",
    "                        'sim_name':                             sim_name,\n",
    "                        'MN_index':                             int(neuron_idx),\n",
    "                        'pool':                                 (neuron_idx // nb_per_pool) if nb_per_pool else np.nan,\n",
    "                        'idx_within_pool':                      (neuron_idx % nb_per_pool)  if nb_per_pool else np.nan,\n",
    "                        'inhibited_by_ground_truth_receiving':   recv,\n",
    "                        'inhibiting_ground_truth_delivering':    delv,\n",
    "                        'asymmetry_ground_truth_diff':           delv - recv,\n",
    "                        'asymmetry_ground_truth_ratio':          (delv / recv) if (recv not in (0, np.nan)) else np.nan,\n",
    "                        'ISI_cov':                               isi_cov_mn\n",
    "                    }\n",
    "                    # sim parameters\n",
    "                    for key in sim_params_keys:\n",
    "                        base[key] = params.get(key, np.nan)\n",
    "                    # firing‐rate stats (mean, std, etc.) per MN_i\n",
    "                    for stat in firing_rate_stats:\n",
    "                        m = fr_dict.get(stat, {})\n",
    "                        base[f\"Firing_rates_{stat}\"] = m.get(f\"MN_{neuron_idx}\", np.nan)\n",
    "                    # connectivity (e.g. 'MN_delivered_total', 'MN_received_total', etc.)\n",
    "                    for ck in connectivity_keys:\n",
    "                        arr = conn.get(ck)\n",
    "                        base[ck] = (arr[neuron_idx] if (isinstance(arr,(list,np.ndarray)) and neuron_idx < len(arr)) else np.nan)\n",
    "                    # graph‐measures\n",
    "                    for gm in graph_measures_keys:\n",
    "                        arr = graph.get(gm)\n",
    "                        base[f\"Graph_theory_{gm}\"] = (arr[neuron_idx] if (isinstance(arr,(list,np.ndarray)) and neuron_idx < len(arr)) else np.nan)\n",
    "                    # coherence\n",
    "                    for ckey in coherence_keys:\n",
    "                        arr = coh.get(ckey)\n",
    "                        base[f\"Coherence_{ckey}\"] = (arr[neuron_idx] if (isinstance(arr,(list,np.ndarray)) and neuron_idx < len(arr)) else np.nan)\n",
    "\n",
    "                    # per-MN motoneuron properties from HDF5 (if present) ---\n",
    "                    mn_props = data.get('motoneurons_properties') or {}\n",
    "                    if isinstance(mn_props, dict) and len(mn_props):\n",
    "                        for prop_name, arr in mn_props.items():\n",
    "                            try:\n",
    "                                base[f\"MNprop_{prop_name}\"] = float(arr[int(neuron_idx)])\n",
    "                            except Exception:\n",
    "                                base[f\"MNprop_{prop_name}\"] = np.nan\n",
    "\n",
    "                    # now flatten each direction (inhibited/inhibiting/combined)\n",
    "                    for direction in CROSS_DIRECTIONS:\n",
    "                        row = base.copy()\n",
    "                        row['pool_pair'] = pool_pair\n",
    "                        row['direction'] = direction\n",
    "                        dd = dir_dict.get(direction, {})\n",
    "\n",
    "                        # forward/backward measures\n",
    "                        for m in FWD_BWD_MEASURES:\n",
    "                            row[f\"{m}_fwd\"] = dd.get('forward',{}).get(m, np.nan)\n",
    "                            row[f\"{m}_bwd\"] = dd.get('backward',{}).get(m, np.nan)\n",
    "                        # asymmetry\n",
    "                        for m in ASYM_MEASURES:\n",
    "                            row[m] = dd.get('asymmetry',{}).get(m, np.nan)\n",
    "                        # other scalar measures\n",
    "                        for m in OTHER_MEASURES:\n",
    "                            row[m] = dd.get(m, np.nan)\n",
    "                    \n",
    "                        # --- Attach mean coherence for this pool_pair (same for all rows of the pair) ---\n",
    "                        if coh_colname is not None and compute_band_mean is not None:\n",
    "                            # The coherence keys use '-' (dash) not '<->'\n",
    "                            pair_dash = pool_pair.replace('<->', '-')\n",
    "                            row[coh_colname] = compute_band_mean(pair_dash)\n",
    "                        # --- end mean coherence ---\n",
    "\n",
    "                        rows.append(row)\n",
    "\n",
    "        else:\n",
    "            # No cross‐hist → one row per MN (everything else is NaN for cross‐hist columns)\n",
    "            for i in range(total_nb):\n",
    "                # pull that MN’s ISI‐cov if present\n",
    "                cov_key = f\"MN_{i}\"\n",
    "                isi_cov_mn = cov_mn_dict.get(cov_key, np.nan)\n",
    "\n",
    "                row = {\n",
    "                    'sim_name':                        sim_name,\n",
    "                    'MN_index':                        i,\n",
    "                    'pool':                            (i // nb_per_pool) if nb_per_pool else np.nan,\n",
    "                    'idx_within_pool':                 (i % nb_per_pool) if nb_per_pool else np.nan,\n",
    "                    'pool_pair':                       np.nan,\n",
    "                    'direction':                       np.nan,\n",
    "                    'inhibited_by_ground_truth_receiving':   np.nan,\n",
    "                    'inhibiting_ground_truth_delivering':    np.nan,\n",
    "                    'asymmetry_ground_truth_diff':           np.nan,\n",
    "                    'asymmetry_ground_truth_ratio':          np.nan,\n",
    "                    'ISI_cov':                           isi_cov_mn\n",
    "                }\n",
    "                # sim parameters\n",
    "                for key in sim_params_keys:\n",
    "                    row[key] = params.get(key, np.nan)\n",
    "                # firing‐rate stats\n",
    "                for stat in firing_rate_stats:\n",
    "                    m = fr_dict.get(stat, {})\n",
    "                    row[f\"Firing_rates_{stat}\"] = m.get(f\"MN_{i}\", np.nan)\n",
    "                # connectivity, graph, coherence\n",
    "                for ck in connectivity_keys:\n",
    "                    arr = conn.get(ck)\n",
    "                    row[ck] = (arr[i] if (isinstance(arr,(list,np.ndarray)) and i < len(arr)) else np.nan)\n",
    "                for gm in graph_measures_keys:\n",
    "                    arr = graph.get(gm)\n",
    "                    row[f\"Graph_theory_{gm}\"] = (arr[i] if (isinstance(arr,(list,np.ndarray)) and i < len(arr)) else np.nan)\n",
    "                for ckey in coherence_keys:\n",
    "                    arr = coh.get(ckey)\n",
    "                    row[f\"Coherence_{ckey}\"] = (arr[i] if (isinstance(arr,(list,np.ndarray)) and i < len(arr)) else np.nan)\n",
    "\n",
    "                # set all forward/backward and asymmetry/other to NaN\n",
    "                for m in FWD_BWD_MEASURES:\n",
    "                    row[f\"{m}_fwd\"] = np.nan\n",
    "                    row[f\"{m}_bwd\"] = np.nan\n",
    "                for m in ASYM_MEASURES + OTHER_MEASURES:\n",
    "                    row[m] = np.nan\n",
    "\n",
    "                # --- attach mean coherence for the MN's own pool (within-pool) ---\n",
    "                if coh_colname is not None and compute_band_mean is not None and nb_per_pool:\n",
    "                    pool_idx = int(i // nb_per_pool)\n",
    "                    pool_name = f\"pool_{pool_idx}\"\n",
    "                    pair_dash = f\"{pool_name}-{pool_name}\"  # within-pool coherence\n",
    "                    row[coh_colname] = compute_band_mean(pair_dash)\n",
    "                # --- end mean coherence ---\n",
    "\n",
    "                # per-MN motoneuron properties from HDF5 (if present) ---\n",
    "                mn_props = data.get('motoneurons_properties') or {}\n",
    "                if isinstance(mn_props, dict) and len(mn_props):\n",
    "                    for prop_name, arr in mn_props.items():\n",
    "                        try:\n",
    "                            row[f\"MNprop_{prop_name}\"] = float(arr[int(i)])\n",
    "                        except Exception:\n",
    "                            row[f\"MNprop_{prop_name}\"] = np.nan\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64baab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_dataframe(\n",
    "    analysis_output,\n",
    "    sim_params_keys=['total_nb_motoneurons', 'nb_motoneurons_per_pool', 'nb_pools','mean_soma_diameter',\n",
    "                     'excitatory_input_baseline','common_input_std','disynpatic_inhib_connections_desired_MN_MN','common_input_characteristics',\n",
    "                     'independent_to_common_input_ratio', 'frequency_range_of_common_input',\n",
    "                     'between_pool_excitatory_input_correlation',\n",
    "                     'synaptic_IPSP_membrane_or_user_defined_time_constant',\n",
    "                     'synaptic_IPSP_decay_time_constant',\n",
    "                     'MN_RC_synpatic_delay'], # values in the parameter space\n",
    "    firing_rate_stats=['mean','std','max','min'], # the ISI CoVs will also be loaded\n",
    "    connectivity_keys=[\n",
    "      'MN_delivered_total',\n",
    "      'MN_received_total'\n",
    "    ],\n",
    "    graph_measures_keys=['density'],\n",
    "    coherence_keys=None,\n",
    "    include_cross_hist=True,\n",
    "    # --- Set the values below to None if not putting coherence values in the dataframe ---\n",
    "    coherence_band=coh_band,\n",
    "    coherence_group_N=nb_of_MUs_for_COH,\n",
    "    coherence_source=\"Coherence_total\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4886e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_dataframe(df):\n",
    "    \"\"\"\n",
    "    From a DataFrame with columns:\n",
    "      ['direction',\n",
    "       'raw_area_fwd','raw_area_bwd',\n",
    "       'corrected_area_fwd','corrected_area_bwd',\n",
    "       'raw_area_asym_diff','raw_area_asym_ratio',\n",
    "       'corrected_area_asym_diff','corrected_area_asym_ratio', …]\n",
    "    create more explicit mapping columns.\n",
    "\n",
    "    New columns:\n",
    "      - perspective\n",
    "      - inhibited_by_estimation_raw\n",
    "      - inhibiting_estimation_raw\n",
    "      - inhibited_by_estimation_corrected\n",
    "      - inhibiting_estimation_corrected\n",
    "      - asymmetry_estimation_diff_raw\n",
    "      - asymmetry_estimation_ratio_raw\n",
    "      - asymmetry_estimation_diff_corrected\n",
    "      - asymmetry_estimation_ratio_corrected\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1) perspective mapping\n",
    "    df['perspective'] = df['direction'].map({\n",
    "        'inhibited': 'other_MUs_as_ref',\n",
    "        'inhibiting': 'MU_as_ref',\n",
    "        'combined':   'combined'\n",
    "    }).fillna(np.nan)\n",
    "    \n",
    "    # 2) raw‐estimate mapping\n",
    "    is_inh = df['direction'] == 'inhibited'\n",
    "    df['inhibited_by_estimation_raw'] = np.where(\n",
    "        is_inh,\n",
    "        df['raw_area_fwd'],\n",
    "        df['raw_area_bwd']\n",
    "    )\n",
    "    df['inhibiting_estimation_raw'] = np.where(\n",
    "        is_inh,\n",
    "        df['raw_area_bwd'],\n",
    "        df['raw_area_fwd']\n",
    "    )\n",
    "    \n",
    "    # 3) corrected‐estimate mapping\n",
    "    df['inhibited_by_estimation_corrected'] = np.where(\n",
    "        is_inh,\n",
    "        df['corrected_area_fwd'],\n",
    "        df['corrected_area_bwd']\n",
    "    )\n",
    "    df['inhibiting_estimation_corrected'] = np.where(\n",
    "        is_inh,\n",
    "        df['corrected_area_bwd'],\n",
    "        df['corrected_area_fwd']\n",
    "    )\n",
    "\n",
    "    # 4) asymmetry mapping\n",
    "    # start by copying the existing asymmetry columns\n",
    "    df['asymmetry_estimation_diff_raw']      = df['raw_area_asym_diff']\n",
    "    df['asymmetry_estimation_ratio_raw']     = df['raw_area_asym_ratio']\n",
    "    df['asymmetry_estimation_diff_corrected'] = df['corrected_area_asym_diff']\n",
    "    df['asymmetry_estimation_ratio_corrected']= df['corrected_area_asym_ratio']\n",
    "    \n",
    "    # now flip/invert for the 'inhibited' direction\n",
    "    mask = df['direction'] == 'inhibited'\n",
    "    # diff: multiply by -1\n",
    "    df.loc[mask, 'asymmetry_estimation_diff_raw']      *= -1\n",
    "    df.loc[mask, 'asymmetry_estimation_diff_corrected'] *= -1\n",
    "    # ratio: invert\n",
    "    df.loc[mask, 'asymmetry_estimation_ratio_raw']      = 1 / df.loc[mask, 'asymmetry_estimation_ratio_raw']\n",
    "    df.loc[mask, 'asymmetry_estimation_ratio_corrected'] = 1 / df.loc[mask, 'asymmetry_estimation_ratio_corrected']\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc0f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = reorganize_dataframe(df)\n",
    "\n",
    "### Add a new perspective which duplicates the perspective (\"MU_as_ref\" or \"other_MUs_as_ref\") which has the most spikes ###\n",
    "\n",
    "# 1) Max per (sim_name, MN_index, direction), ignoring 'combined'\n",
    "group_max = (\n",
    "    df_new[df_new.perspective != 'combined']\n",
    "      .groupby(['sim_name','MN_index','direction'])['n_spikes']\n",
    "      .max()\n",
    "      .rename('max_noncombined_spikes')\n",
    ")\n",
    "\n",
    "# 2) Merge it back (now aligned on direction too)\n",
    "df_new = df_new.merge(\n",
    "    group_max,\n",
    "    on=['sim_name','MN_index','direction'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3) Flag the winner _per direction_\n",
    "df_new['perspective_with_most_spikes'] = (\n",
    "    (df_new.perspective != 'combined')\n",
    "  & (df_new.n_spikes == df_new.max_noncombined_spikes)\n",
    ")\n",
    "\n",
    "# 4) Extract _both_ winners (one per direction), relabel & append them\n",
    "df_most = df_new[df_new.perspective_with_most_spikes].copy()\n",
    "df_most['perspective'] = 'most_spikes'\n",
    "\n",
    "df_new = pd.concat([df_new, df_most], ignore_index=True)\n",
    "\n",
    "# 5) Drop the helper columns\n",
    "df_new = df_new.drop(columns=['max_noncombined_spikes','perspective_with_most_spikes'])\n",
    "\n",
    "# 6) Set the IPSP time constant from membrane τ_m when requested\n",
    "if load_hdf5_sim_output_for_MNs_electrophysiological_properties:\n",
    "    needed = {\n",
    "        'synaptic_IPSP_membrane_or_user_defined_time_constant',\n",
    "        'MNprop_membrane_time_constant'\n",
    "    }\n",
    "    if needed.issubset(df_new.columns):\n",
    "        # build a robust mask (handles stray spaces / case)\n",
    "        mask = (\n",
    "            df_new['synaptic_IPSP_membrane_or_user_defined_time_constant']\n",
    "            .astype(str).str.strip().str.lower()\n",
    "            .eq('membrane')\n",
    "        )\n",
    "\n",
    "        # assign only where mask is True; preserves existing values elsewhere\n",
    "        df_new.loc[mask, 'synaptic_IPSP_decay_time_constant'] = \\\n",
    "            df_new.loc[mask, 'MNprop_membrane_time_constant']\n",
    "    else:\n",
    "        missing = needed - set(df_new.columns)\n",
    "        print(f\"[info] Skipping IPSP overwrite: missing columns {sorted(missing)}\")\n",
    "\n",
    "# Check what it looks like\n",
    "df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter according to the desired pool_pair types\n",
    "def filter_by_pool_pair(df, mode=\"all\"):\n",
    "    if mode == \"all\":\n",
    "        return df\n",
    "    # split to left/right\n",
    "    pools = df[\"pool_pair\"].str.split(\"<->\", expand=True)\n",
    "    same = pools[0] == pools[1]\n",
    "    if mode == \"within\":\n",
    "        # only poolX<->poolX\n",
    "        return df[same].copy()\n",
    "    elif mode == \"between\":\n",
    "        # only poolX<->poolY with X != Y\n",
    "        return df[~same].copy()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode {mode!r}, must be 'all'|'within'|'between'\")\n",
    "\n",
    "# then:\n",
    "df_new = filter_by_pool_pair(df_new, within_between_or_all_muscle_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de623bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(f\"{parent_folder_to_load_from}\\\\___general_analysis_of_simulations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15bd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking results\n",
    "df_filtered = df_new.copy()\n",
    "df_filtered = df_filtered[df_filtered['n_spikes']>10_000]\n",
    "df_filtered = df_filtered[df_filtered['r2_base']>0.1]\n",
    "df_filtered = df_filtered[df_filtered['r2_full']>0.75]\n",
    "df_filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapping_RI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
