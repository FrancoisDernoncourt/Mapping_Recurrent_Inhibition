{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bffc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.distributions import constraints, MultivariateNormal\n",
    "import random\n",
    "import os\n",
    "import inspect\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pathlib\n",
    "import json\n",
    "import ast\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cmasher as cmr\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.stats import gaussian_kde\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# sbi imports\n",
    "from sbi import utils as sbi_utils\n",
    "from sbi import inference as sbi_inference\n",
    "from sbi.analysis import plot_summary, pairplot, conditional_pairplot\n",
    "from sbi.inference import NPE, ImportanceSamplingPosterior\n",
    "from sbi.utils import RestrictedPrior, get_density_thresholder\n",
    "\n",
    "# for summary‐statistics\n",
    "from scipy.stats import iqr as scipy_iqr\n",
    "\n",
    "# for embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, normalize\n",
    "from sklearn.metrics import silhouette_score\n",
    "from itertools import product\n",
    "\n",
    "# for posterior predictive checks comparisons\n",
    "from scipy.stats import wilcoxon, binomtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS: to be replaced with the folders in which the data is stored on your computer - the script does not need the raw output files but the summary CSVs.\n",
    "simulated_data_path = \"C:\\\\Users\\\\franc\\\\Documents\\\\Mapping_Recurrent_Inhibition_minimal_data_to_run_scripts\\\\Files_to_run_scripts\\\\simulated_data_SBI_training_single_muscles\" # for single muscle\n",
    "# simulated_data_path = \"C:\\\\Users\\\\franc\\\\Documents\\\\Mapping_Recurrent_Inhibition_minimal_data_to_run_scripts\\\\Files_to_run_scripts\\\\simulated_data_SBI_training_between_muscles\" # for muscle pairs\n",
    "experimental_data_path = \"C:\\\\Users\\\\franc\\\\Documents\\\\Mapping_Recurrent_Inhibition_minimal_data_to_run_scripts\\\\Files_to_run_scripts\\\\experimental_data\"\n",
    "experimental_dataframe_to_load = \"experimental_data_dataframe_reorganized_and_filtered_dir_inhibited_persp_other_MUs_as_ref.csv\"\n",
    "# ^ Make sure to plug-in the right csv according to choice of \"perspective_to_use\" and \"direction_to_use\"\n",
    "\n",
    "path_to_save = f\"{simulated_data_path}\\\\$_SBI_inference\"\n",
    "# If a neural estimator was already trained and saved, specify the folder in which it has been saved\n",
    "# # SINGLE MUSCLES CONFIG - if loading trained network and posterior estimates\n",
    "path_to_load = f\"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\Mapping_Recurrent_Inhibition\\\\Simulation_based_inference\\\\saved_posterior_density_estimators\\\\single_muscles_density_estimator\" # Load the trained neural density estimator reported in the paper (single muscle case), available in the repository\n",
    "# # BETWEEN MUSCLES CONFIG - if loading trained network and posterior estimates\n",
    "# path_to_load = f\"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\Mapping_Recurrent_Inhibition\\\\Simulation_based_inference\\\\saved_posterior_density_estimators\\\\paired_muscles_density_estimator\" # Load the trained neural density estimator reported in the paper (single muscle case), available in the repository\n",
    "# NOTE that an existing density estimator may perform very poorly if it has been trained on a set of training data that doesn't reflect the testing data\n",
    "rerun_network_training_and_sampling = False # False # True # If false, will load the posterior-infering network pickle file instead, and the posterior estimates (samples) as a pickle file too\n",
    "rerun_network_training_for_heldout_data = False # False # True # If false, will load the posterior-infering network (trained on 90% of the full training dataset) pickle file instead of training a new one. \n",
    "# It will look for the pickle file 'sbi_check_heldout_sims.pkl' inside the path_to_load folder. \n",
    "# ^ if \"False\", will look for the pickle file in path_to_loads\n",
    "# The network training is fast but if the sampling process for the posterior estimate is very long and appears to run indefinitely,\n",
    "# it means that some experimental observations are out-of-distribution relative to the training data (sbi usually returns a warning)\n",
    "\n",
    "# ### #\n",
    "# /!\\ POSTERIOR PREDICTIVE CHECKS PARAMETERS (e.g., number of parameter sets to sample from posterior and to simulate from) ARE AT THE END OF THE NOTEBOOK\n",
    "# ### #\n",
    "\n",
    "# Creating folder if necessary\n",
    "if not rerun_network_training_and_sampling and not os.path.exists(path_to_load):\n",
    "    raise ValueError(f\"Please specify a valid, existing 'path_to_load'\")\n",
    "elif rerun_network_training_and_sampling:\n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.mkdir(path_to_save)\n",
    "    inference_dirs = [d for d in os.listdir(f\"{path_to_save}\")\n",
    "        if re.fullmatch(r'inference_model_(\\d+)', d)]\n",
    "    if inference_dirs:\n",
    "        existing_idxs = [int(re.fullmatch(r'inference_model_(\\d+)', d).group(1))\n",
    "                        for d in inference_dirs]\n",
    "        inference_idx = max(existing_idxs) + 1\n",
    "    else:\n",
    "        inference_idx = 0\n",
    "    path_to_save = f\"{path_to_save}\\\\inference_model_{inference_idx}\"\n",
    "    os.makedirs(path_to_save, exist_ok=True)\n",
    "else:\n",
    "    path_to_save = path_to_load\n",
    "\n",
    "# ── USER‐SPECIFIED HYPERPARAMETERS ──\n",
    "perspective_to_use = 'other_MUs_as_ref' # 'other_MUs_as_ref' # choose from {'other_MUs_as_ref','MU_as_ref','combined','most_spikes'}\n",
    "direction_to_use   = 'inhibited' # choose from {'inhibited','inhibiting'}\n",
    "raw_or_corrected   = 'raw' # choose from {'raw', 'corrected'}\n",
    "normalize_scale_of_features_for_inference = True\n",
    "\n",
    "# Map direction → column name in the CSV (for simulated data, different column names for the )\n",
    "if direction_to_use == 'inhibited':\n",
    "    inhib_connectivity_colname = f'inhibited_by_estimation_{raw_or_corrected}'\n",
    "elif direction_to_use == 'inhibiting':\n",
    "    inhib_connectivity_colname = f'inhibiting_estimation_{raw_or_corrected}'\n",
    "else:\n",
    "    raise ValueError(\"direction_to_use must be 'inhibited' or 'inhibiting'\")\n",
    "asym_colname = f'asymmetry_diff_{raw_or_corrected}'\n",
    "\n",
    "# Filtering\n",
    "min_r2_for_baseline_curve_fit = {\"simulation\":0.1,\n",
    "                                 \"experiment\": 0.1}\n",
    "min_r2_for_overall_curve_fit = {\"simulation\":0.75,\n",
    "                                 \"experiment\": 0.75}\n",
    "min_nb_spikes = {\"simulation\":10_000,\n",
    "                \"experiment\": 5_000}\n",
    "\n",
    "# Simulation input parameters\n",
    "version_of_sim_with_only_single_pool_input_vals = False # False # if True, older simulation version\n",
    "within_or_between_pools_sbi = 'within' # 'within' # 'between' #'within' 'between'\n",
    "nb_pools = 1 # 1 for SINGLE MUSCLE CONFIG (1 pool simulated for training dataset) # 2 for BETWEEN MUSCLES CONFIG (2 pools simulated for training dataset)\n",
    "# ^ If within, will consider only a single pool and use excitatory_input_baseline[0], disynpatic_inhib_connections_desired_MN_MN[0,0] as well as frequency_range_of_common_input[0]\n",
    "# ^ If between, will consider that there are two pools and will perform sbi only on the following parameters: \n",
    "#       - excitatory_input_baseline[0] and excitatory_input_baseline[1]\n",
    "#       - disynpatic_inhib_connections_desired_MN_MN[0,1] and disynpatic_inhib_connections_desired_MN_MN[1,0]\n",
    "#       - between_pool_excitatory_input_correlation\n",
    "# !!! Please adapt input_sim_parameters !!! #\n",
    "if within_or_between_pools_sbi == 'within':\n",
    "    use_only_same_muscle_pair = True\n",
    "    use_only_different_muscle_pair = False\n",
    "    filter_intensity = [10, 40]\n",
    "    filter_muscles = [] # Leaving empty keeps all muscles\n",
    "elif within_or_between_pools_sbi == 'between':\n",
    "    use_only_different_muscle_pair = True\n",
    "    use_only_same_muscle_pair = False\n",
    "    # The between-pool simulations have their priors coming from the posterior obtained from the within-pool inference from the muscle pair and from the intensity of interest.\n",
    "    # So the mapping of the experimental data should apply only to the muscles and intensity from which the priors have been taken from\n",
    "    filter_intensity = [10, 40] # [10] #[10]\n",
    "    filter_muscles = [] # [\"SOL\",\"GM\"] # [\"VM\",\"VL\"] # Will consider only those muscles for experimental data\n",
    "\n",
    "# Input param to be inferred #########################################\n",
    "# # BETWEEN MUSCLES CONFIG \n",
    "# input_sim_parameters_to_infer = [\"excitatory_input_baseline_self\",\n",
    "#     \"disynpatic_inhib_connections_desired_MN_MN_other_pool\",\n",
    "#     \"between_pool_excitatory_input_correlation\"]\n",
    "# # SINGLE MUSCLES CONFIG\n",
    "input_sim_parameters_to_infer = [\"excitatory_input_baseline\",\n",
    "    \"disynpatic_inhib_connections_desired_MN_MN\",\n",
    "    \"common_input_high_freq_middle_of_range\", # \"common_input_characteristics.Frequency_middle_of_range.pool_0.input_1\"\n",
    "    \"common_input_high_freq_half_width_range\", # \"common_input_characteristics.Frequency_half_width_of_range.pool_0.input_1\"\n",
    "    \"common_input_std\"] # \"common_input_std[0][1]\"\n",
    "\n",
    "# Input parameters not fixed, but still NOT to be inferred #########################################\n",
    "# coming from previously-inferred posterior and used as inference features for training (ground-truth) and inference on experimental data (repeated sampling of the single-muscle posterior)\n",
    "# # BETWEEN MUSCLES CONFIG \n",
    "# input_sim_parameters_as_features = [ # Each row comes from a single 'MN to pool', so always entierly defined by \"self\" VS \"other\"\n",
    "#     \"disynpatic_inhib_connections_desired_MN_MN_self\",\n",
    "#     \"common_input_high_freq_middle_of_range_self\",\n",
    "#     \"common_input_high_freq_half_width_range_self\",\n",
    "#     \"common_input_std_self\"\n",
    "# ]\n",
    "# # SINGLE MUSCLES CONFIG\n",
    "input_sim_parameters_as_features = []\n",
    "\n",
    "# Below: only used if len(input_sim_parameters_as_features) >= 1\n",
    "previously_estimated_posterior_samples_for_experimental_data_SBI = 100 # duplicate each experimental data by this value, and assign to each duplicated row a random sample from the posterior\n",
    "previously_estimated_posterior_results_path = \"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\SBI_motor_neuron_behavior\\\\$$$_Simulation_batch_single_muscle\\\\$_SBI_inference\\\\inference_model_0\"\n",
    "previously_estimated_posterior_each_subject_csv = \"posterior_samples_each_subject_df.csv\"\n",
    "previously_estimated_posterior_subjects_grouped_csv = \"posterior_samples_subjects_grouped_df.csv\"\n",
    "map_strings_of_posterior_estimated_parameters_to_param_used_as_features = {\n",
    "    \"disynpatic_inhib_connections_desired_MN_MN\": \"disynpatic_inhib_connections_desired_MN_MN_self\",\n",
    "    \"common_input_high_freq_middle_of_range\": \"common_input_high_freq_middle_of_range_self\",\n",
    "    \"common_input_high_freq_half_width_range\": \"common_input_high_freq_half_width_range_self\",\n",
    "    \"common_input_std\": \"common_input_std_self\"\n",
    "}\n",
    "# ^ The keys need to be the same as in input_sim_parameters_as_features\n",
    "\n",
    "\n",
    "# The param names below need to exist in 'input_sim_parameters_to_infer' - they are used for plotting\n",
    "# # BETWEEN MUSCLES CONFIG \n",
    "# specific_input_parameters_of_interest = [\"disynpatic_inhib_connections_desired_MN_MN_other_pool\",\n",
    "#     \"between_pool_excitatory_input_correlation\"] # \"common_input_std[0][1]\"\n",
    "# # SINGLE MUSCLES CONFIG\n",
    "specific_input_parameters_of_interest = [\"disynpatic_inhib_connections_desired_MN_MN\",\n",
    "    \"common_input_high_freq_middle_of_range\", # \"common_input_characteristics.Frequency_middle_of_range.pool_0.input_1\"\n",
    "    \"common_input_high_freq_half_width_range\", # \"common_input_characteristics.Frequency_half_width_of_range.pool_0.input_1\"\n",
    "    \"common_input_std\"] # \"common_input_std[0][1]\"\n",
    "specific_input_parameters_of_interest_corresponding_indices = [input_sim_parameters_to_infer.index(p) \n",
    "    for p in specific_input_parameters_of_interest]\n",
    "# ^ This is just to make the index correspondance between specific_input_parameters_of_interest_corresponding_indices and specific_input_parameters_of_interest\n",
    "\n",
    "# Change the pool(s) (and thus the strings) when several pools are considered\n",
    "mapping_from_common_input_characteristics_to_input_param_names = {\n",
    "    \"Frequency_middle_of_range.pool_0.input_1\": \"common_input_high_freq_middle_of_range\",\n",
    "    \"Frequency_half_width_of_range.pool_0.input_1\": \"common_input_high_freq_half_width_range\",\n",
    "}\n",
    "\n",
    "# PRIORS\n",
    "# # SINGLE MUSCLES CONFIG\n",
    "priors_per_parameters_to_infer = { # \"param_name\": [low, high]\n",
    "    \"excitatory_input_baseline\": [20*1e3, 70*1e3], \n",
    "    \"disynpatic_inhib_connections_desired_MN_MN\": [0, 3],\n",
    "    \"common_input_high_freq_middle_of_range\": [2.5, 75],\n",
    "    \"common_input_high_freq_half_width_range\": [2.5, 25],\n",
    "    \"common_input_std\": [0, 7.0*1e3]\n",
    "}\n",
    "# # BETWEEN MUSCLES CONFIG (pairs)\n",
    "# priors_per_parameters_to_infer = { # \"param_name\": [low, high]\n",
    "#     \"excitatory_input_baseline_self\": [20*1e3, 90*1e3], # + 20 relative to the single muscle case with only self-inhibition\n",
    "#     \"disynpatic_inhib_connections_desired_MN_MN_other_pool\": [0, 3],\n",
    "#     \"between_pool_excitatory_input_correlation\": [0, 1]\n",
    "# }\n",
    "\n",
    "# Define summary-statistics functions to be used here\n",
    "summary_funcs = {\n",
    "    'mean'  : np.nanmean,\n",
    "    'median': lambda arr: np.nanmedian(arr),\n",
    "    'sd'    : np.nanstd,\n",
    "    'iqr'   : lambda arr: scipy_iqr(arr, nan_policy='omit')\n",
    "}\n",
    "\n",
    "# Features to use for posterior inference\n",
    "features_for_inference = [inhib_connectivity_colname,\n",
    "            'sync_height',\n",
    "            'IPSP_timing_of_trough',\n",
    "            'Firing_rates_mean']\n",
    "# The feature names below need to exist in 'features_for_inference' - they are used for plotting\n",
    "specific_features_of_interest = [inhib_connectivity_colname,\n",
    "            'sync_height']\n",
    "# In the between-muscles case, also using 'input_sim_parameters_as_features' (ground-truth for training, samples from posterior for inference)\n",
    "features_for_inference += input_sim_parameters_as_features\n",
    "\n",
    "## Remapping colnames from simulation results\n",
    "colname_renames = {}\n",
    "if (direction_to_use == 'inhibited') and (perspective_to_use == 'other_MUs_as_ref'):\n",
    "    colname_renames = {\"delay_forward_IPSP\": \"IPSP_timing_of_trough\"}\n",
    "elif (direction_to_use == 'inhibited') and (perspective_to_use == 'other_MUs_as_ref'):\n",
    "    colname_renames = {\"delay_backward_IPSP\": \"IPSP_timing_of_trough\"}\n",
    "elif (direction_to_use == 'inhibited') and (perspective_to_use == 'MU_as_ref'):\n",
    "    colname_renames = {\"delay_backward_IPSP\": \"IPSP_timing_of_trough\"}\n",
    "elif (direction_to_use == 'inhibiting') and (perspective_to_use == 'MU_as_ref'):\n",
    "    colname_renames = {\"delay_forward_IPSP\": \"IPSP_timing_of_trough\"}\n",
    "\n",
    "### For SBI\n",
    "sbi_density_estimator = \"maf\" # \"maf\" (masked autoregressive flow) is default; \"mdn\" (mixture density network) avoid leakages at the priors' boundaries\n",
    "num_posterior_samples = {\"simulation\":10_000,\n",
    "                         \"experiment\": 10_000,\n",
    "                         \"experiment_with_posterior_estimates_as_features\": 100} # 1_000} # Note that the total number of samples per condition will be multiplied by 'previously_estimated_posterior_samples_for_experimental_data_SBI'\n",
    "best_posterior_estimate_method = \"logp\" # \"logp\" or \"knn\". The 'correct' way is logp and it is the most accurate, but it can be extremely long if the model cannot appropriately reproduce the epxerimental data - otherwise it's very fast\n",
    "network_training_hyperparameters = {\n",
    "    \"num_atoms\": 10, # default is 10\n",
    "    \"training_batch_size\": 200, # default is 200\n",
    "    \"learning_rate\": 0.0005, # default is 0.0005\n",
    "    \"validation_fraction\": 0.1, # default is 10%\n",
    "    \"max_num_epochs\": 2000, # Stop training after X epoches (upper bound)\n",
    "    \"stop_after_epochs\": 20 # Train for at least X epochs\n",
    "}\n",
    "# # Check tutorials online:\n",
    "# # https://sbi-dev.github.io/sbi/v0.24.0/tutorials/03_density_estimators/\n",
    "# # https://sbi-dev.github.io/sbi/v0.24.0/tutorials/09_sampler_interface/\n",
    "# # https://sbi-dev.github.io/sbi/v0.24.0/tutorials/15_importance_sampled_posteriors/\n",
    "# # https://sbi-dev.github.io/sbi/v0.24.0/tutorials/17_plotting_functionality/\n",
    "# # https://sbi-dev.github.io/sbi/0.22/tutorial/07_conditional_distributions/ # https://sbi-dev.github.io/sbi/v0.24.0/tutorials/05_conditional_distributions/ \n",
    "# sampling_algorithm = \"vi\" # \"direct\" (default) ; \"mcmc\" (Markov Chain Monte Carlo) ; \"vi\" (variational inference) ; \"rejection\" ; \"is\" (importance sampling)\n",
    "# # a sampling method may be necessary if sampling_algorithm == 'mcmc', 'vi' or 'si'\n",
    "# # ValueError: You passed `sample_with='rejection' but you did not specify a `proposal` in `rejection_sampling_parameters`. Until sbi v0.22.0, this was interpreted as directly sampling from the posterior. As of sbi v0.23.0, you instead have to use `sample_with='direct'` to do so.\n",
    "\n",
    "# Mapping from simulation colname to experiment colnames\n",
    "colname_map = {\n",
    "    \"ISI_cov\": \"cov_mean\",\n",
    "    \"Firing_rates_mean\":\"firing_rates_mean\"\n",
    "} # The rest of the colnames should have the correct mapping already\n",
    "\n",
    "muscle_colors_dict = {\n",
    "    \"FDI<->FDI\": \"#00D6D6\",\n",
    "    \"TA<->TA\":   \"#2CA02C\",\n",
    "    \"VL<->VL\":   \"#D62728\",\n",
    "    \"VL<->VM\":   \"#D65927\",\n",
    "    \"VM<->VM\":   \"#FFC400\",\n",
    "    \"VM<->VL\":   \"#FFA600\",\n",
    "    \"GM<->GM\":   \"#339DFF\",\n",
    "    \"GM<->SOL\":   \"#339DFF\",\n",
    "    \"SOL<->SOL\": \"#9467BD\",\n",
    "    \"SOL<->GM\": \"#9467BD\"\n",
    "}\n",
    "muscle_colormaps_dict = {\n",
    "    \"FDI<->FDI\":    cmr.cosmic,\n",
    "    \"TA<->TA\":      cmr.nuclear,\n",
    "    \"VL<->VL\":      cmr.ember,\n",
    "    \"VL<->VM\":      cmr.ember,\n",
    "    \"VM<->VM\":      cmr.amber,\n",
    "    \"VM<->VL\":      cmr.amber,\n",
    "    \"GM<->GM\":      cmr.bubblegum,\n",
    "    \"GM<->SOL\":     cmr.bubblegum,\n",
    "    \"SOL<->SOL\":    cmr.freeze,\n",
    "    \"SOL<->GM\":     cmr.freeze    \n",
    "}\n",
    "subjects_color_dict = {\n",
    "    'DeFr':\"#FFC400\",\n",
    "    'HuFr':\"#0091F8\",\n",
    "    'KaPa':\"#2CA02C\",\n",
    "    'LeCl':\"#D62728\",\n",
    "    'LoTi':\"#9467BD\",\n",
    "    'MeJu':\"#00D6D6\"\n",
    "}\n",
    "\n",
    "param_colors_dict = { # Should be the same paramas as in input_sim_parameters\n",
    "    \"disynpatic_inhib_connections_desired_MN_MN\": \"#1F77B4\",\n",
    "    \"common_input_std\": \"#D62728\"\n",
    "}\n",
    "\n",
    "# ---- Helper to help with Numpy and PyTorch conversions\n",
    "def to_np(x):\n",
    "    \"\"\"Convert torch.Tensor or array-like to a NumPy array without using tensor.numpy().\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        # Go through Python lists → NumPy, avoids PyTorch's NumPy bridge entirely\n",
    "        return np.asarray(x.detach().cpu().tolist(), dtype=float)\n",
    "    else:\n",
    "        return np.asarray(x, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model parameters as JSON #######################\n",
    "# Only if this is a new inference run\n",
    "if rerun_network_training_and_sampling:\n",
    "    # Build a JSON‐safe representation of summary_funcs:\n",
    "    serializable_summary_funcs = {}\n",
    "    for name, func in summary_funcs.items():\n",
    "        try:\n",
    "            func_name = func.__name__\n",
    "        except AttributeError:\n",
    "            # lambdas typically have the name \"<lambda>\"\n",
    "            func_name = \"<lambda>\"\n",
    "        serializable_summary_funcs[name] = func_name\n",
    "\n",
    "    # Now collect everything into one dict:\n",
    "    cfg = {\n",
    "        \"perspective_to_use\":                perspective_to_use,\n",
    "        \"direction_to_use\":                  direction_to_use,\n",
    "        \"min_r2_for_baseline_curve_fit\":     min_r2_for_baseline_curve_fit,\n",
    "        \"min_r2_for_overall_curve_fit\":      min_r2_for_overall_curve_fit,\n",
    "        \"min_nb_spikes\":                     min_nb_spikes,\n",
    "        \"within_or_between_pools_sbi\":       within_or_between_pools_sbi,\n",
    "        \"use_only_same_muscle_pair\":         use_only_same_muscle_pair,\n",
    "        'use_only_different_muscle_pair':    use_only_different_muscle_pair,\n",
    "        'filter_intensity':                  filter_intensity,\n",
    "        'filter_muscles':                    filter_muscles,\n",
    "        \"input_sim_parameters_to_infer\":     input_sim_parameters_to_infer,\n",
    "        \"summary_funcs\":                     serializable_summary_funcs,\n",
    "        \"features_for_inference\":            features_for_inference,\n",
    "        \"input_sim_parameters_as_features\": input_sim_parameters_as_features,\n",
    "        \"specific_features_of_interest\":     specific_features_of_interest,\n",
    "        \"sbi_density_estimator\":             sbi_density_estimator,\n",
    "        \"num_posterior_samples\":             num_posterior_samples,\n",
    "        \"version_of_sim_with_only_single_pool_input_vals\": version_of_sim_with_only_single_pool_input_vals\n",
    "    }\n",
    "\n",
    "    # Finally, write it out to disk:\n",
    "    with open(f\"{path_to_save}\\\\_model_hyperparameters.json\", \"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data frames - simulation\n",
    "df_simulation = pd.read_csv(f\"{simulated_data_path}\\\\___general_analysis_of_simulations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data frames - experiment\n",
    "df_experiment = pd.read_csv(f\"{experimental_data_path}\\\\{experimental_dataframe_to_load}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data frame\n",
    "df_simulation = df_simulation[df_simulation['r2_base']>min_r2_for_baseline_curve_fit['simulation']]\n",
    "df_simulation = df_simulation[df_simulation['r2_full']>min_r2_for_overall_curve_fit['simulation']]\n",
    "df_simulation = df_simulation[df_simulation['n_spikes']>min_nb_spikes['simulation']]\n",
    "df_experiment = df_experiment[df_experiment['r2_base']>min_r2_for_baseline_curve_fit['experiment']]\n",
    "df_experiment = df_experiment[df_experiment['r2_full']>min_r2_for_overall_curve_fit['experiment']]\n",
    "df_experiment = df_experiment[df_experiment['n_spikes']>min_nb_spikes['experiment']]\n",
    "# Split “SOL<->GM” → [“SOL”, “GM”], then check equality\n",
    "left  = df_experiment[\"muscle_pair\"].str.split(\"<->\").str[0]\n",
    "right = df_experiment[\"muscle_pair\"].str.split(\"<->\").str[1]\n",
    "left_sim = df_simulation[\"pool_pair\"].str.split(\"<->\").str[0]\n",
    "right_sim = df_simulation[\"pool_pair\"].str.split(\"<->\").str[1]\n",
    "if len(filter_muscles) > 0:\n",
    "    df_experiment = df_experiment[df_experiment['muscle_of_MU'].isin(filter_muscles)]\n",
    "if len(filter_intensity) > 0:\n",
    "    df_experiment = df_experiment[df_experiment['intensity'].isin(filter_intensity)]\n",
    "if use_only_same_muscle_pair:\n",
    "    df_experiment = df_experiment[left == right].copy()\n",
    "if use_only_different_muscle_pair:\n",
    "    df_experiment = df_experiment[left != right].copy()\n",
    "    df_simulation = df_simulation[left_sim != right_sim].copy()\n",
    "\n",
    "# Rename columns to get matching names\n",
    "# Invert it so we go “experiment → simulation”:\n",
    "inv_map = { exp_name: sim_name for sim_name, exp_name in colname_map.items() }\n",
    "# Now rename only those columns in df_experiment:\n",
    "df_experiment = df_experiment.rename(columns=inv_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342243ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some new columns to the simulated dataframe to \"explode\" array parameters into the relevant single float values (one per row)\n",
    "# new_cols = [\n",
    "#     \"excitatory_input_baseline_self\",\n",
    "#     \"disynpatic_inhib_connections_desired_MN_MN_self\",\n",
    "#     \"disynpatic_inhib_connections_desired_MN_MN_other_pool\",\n",
    "#     \"between_pool_excitatory_input_correlation\",\n",
    "#     \"common_input_high_freq_middle_of_range_self\",\n",
    "#     \"common_input_high_freq_half_width_range_self\",\n",
    "#     \"common_input_std_self\"\n",
    "# ]\n",
    "\n",
    "# Just for testing (making the data frame much smaller)\n",
    "# df_simulation_test = df_simulation.copy()\n",
    "# df_simulation_test = df_simulation_test.iloc[0:100]\n",
    "# df_simulation_test\n",
    "\n",
    "# ---------- robust parsers ----------\n",
    "_num_re = re.compile(r'[-+]?(?:\\d*\\.\\d+|\\d+)(?:[eE][-+]?\\d+)?')\n",
    "\n",
    "def parse_vec_len2(s):\n",
    "    \"\"\"Return np.ndarray shape (2,) from a string like '[x y]' (no commas), else None.\"\"\"\n",
    "    if isinstance(s, np.ndarray) and s.size == 2:\n",
    "        return s.astype(float)\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    nums = _num_re.findall(str(s))\n",
    "    if len(nums) == 2:\n",
    "        return np.array([float(v) for v in nums], dtype=float)\n",
    "    return None\n",
    "\n",
    "def parse_mat_2x2(s):\n",
    "    \"\"\"Return np.ndarray shape (2,2) from a string like '[[a b]\\\\n [c d]]', else None.\"\"\"\n",
    "    if isinstance(s, np.ndarray) and s.size == 4:\n",
    "        a = np.asarray(s, dtype=float)\n",
    "        return a.reshape(2, 2)\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    nums = _num_re.findall(str(s))\n",
    "    if len(nums) == 4:\n",
    "        return np.array([float(v) for v in nums], dtype=float).reshape(2, 2)\n",
    "    return None\n",
    "\n",
    "def parse_dict_str(s):\n",
    "    \"\"\"Parse your dict string with single quotes into a real dict.\"\"\"\n",
    "    if isinstance(s, dict):\n",
    "        return s\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    try:\n",
    "        return ast.literal_eval(str(s))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- per-row extractor (row is a Series) ---\n",
    "\n",
    "def extract_new_fields_row(row, direction_to_use='inhibited'):\n",
    "    \"\"\"\n",
    "    direction_to_use:\n",
    "      'inhibited'  -> other_pool -> self (receiving inhibition)\n",
    "      'inhibiting' -> self -> other_pool (delivering inhibition)\n",
    "    Assumes pools are {0,1}.\n",
    "    \"\"\"\n",
    "    pool_i  = int(row['pool'])\n",
    "    other_i = 1 - pool_i\n",
    "\n",
    "    # 1) baseline excitatory input (vector length 2, index by pool)\n",
    "    ex_base = parse_vec_len2(row.get('excitatory_input_baseline'))\n",
    "    ex_self = float(ex_base[pool_i]) if ex_base is not None else np.nan\n",
    "\n",
    "    # 2) disynaptic inhibition matrix (2x2: from_pool, to_pool)\n",
    "    disyn = parse_mat_2x2(row.get('disynpatic_inhib_connections_desired_MN_MN'))\n",
    "    if disyn is not None:\n",
    "        dis_self = float(disyn[pool_i, pool_i])\n",
    "        dis_other = (float(disyn[other_i, pool_i]) if direction_to_use == 'inhibited'\n",
    "                     else float(disyn[pool_i, other_i]))\n",
    "    else:\n",
    "        dis_self = np.nan\n",
    "        dis_other = np.nan\n",
    "\n",
    "    # 3) common_input_characteristics dict\n",
    "    cic = parse_dict_str(row.get('common_input_characteristics')) or {}\n",
    "    pk  = f'pool_{pool_i}'\n",
    "    mid  = cic.get('Frequency_middle_of_range', {}).get(pk, {}).get('input_1', np.nan)\n",
    "    half = cic.get('Frequency_half_width_of_range', {}).get(pk, {}).get('input_1', np.nan)\n",
    "\n",
    "    # 4) common_input_std (2x2: pool x input_index) — want input_1 for current pool\n",
    "    ci_std = parse_mat_2x2(row.get('common_input_std'))\n",
    "    ci_self = float(ci_std[pool_i, 1]) if ci_std is not None else np.nan\n",
    "\n",
    "    # 5) scalar passthrough\n",
    "    between_corr = row.get('between_pool_excitatory_input_correlation', np.nan)\n",
    "\n",
    "    return pd.Series({\n",
    "        \"excitatory_input_baseline_self\": ex_self,\n",
    "        \"disynpatic_inhib_connections_desired_MN_MN_self\": dis_self,\n",
    "        \"disynpatic_inhib_connections_desired_MN_MN_other_pool\": dis_other,\n",
    "        \"between_pool_excitatory_input_correlation\": between_corr,\n",
    "        \"common_input_high_freq_middle_of_range_self\": mid,\n",
    "        \"common_input_high_freq_half_width_range_self\": half,\n",
    "        \"common_input_std_self\": ci_self,\n",
    "    })\n",
    "\n",
    "# --- run with a progress bar on your (possibly filtered) frame ---\n",
    "\n",
    "# pick the frame to process (your 0:100 slice in tests)\n",
    "df_sim = df_simulation.copy()\n",
    "df_sim['pool'] = df_sim['pool'].astype(int)\n",
    "\n",
    "tqdm.pandas(desc=\"Extracting fields\", mininterval=0.5)\n",
    "new_cols = df_sim.progress_apply(\n",
    "    lambda r: extract_new_fields_row(r, direction_to_use=direction_to_use),\n",
    "    axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "# attach to original (aligned by index)\n",
    "df_simulation = pd.concat([df_simulation, new_cols], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bd070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulation # checking the result of the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ebe07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulation data frame\n",
    "# Parse the \"common_input_characteristics\" column here directly, not later - because it may relate to several free parameters (instead of 1 like the other string parameters which need to be parsed)\n",
    "if \"common_input_characteristics\" in df_simulation.columns:\n",
    "    # 1) Parse each string into an actual dict\n",
    "    dicts = df_simulation['common_input_characteristics'].apply(ast.literal_eval)\n",
    "    # 2) Use json_normalize to flatten, joining nested keys with dots\n",
    "    flat = pd.json_normalize(dicts, sep='.')\n",
    "    cols_to_take = [col for col in mapping_from_common_input_characteristics_to_input_param_names if col in flat.columns]\n",
    "    selected_cols = flat[cols_to_take].rename(columns=mapping_from_common_input_characteristics_to_input_param_names)\n",
    "    df_simulation = pd.concat([df_simulation, selected_cols], axis=1)\n",
    "###\n",
    "# Add the renamed IPSP delay/timing column\n",
    "for col_previous_name, col_to_add in colname_renames.items():\n",
    "    if col_previous_name in df_simulation.columns:\n",
    "        df_simulation[col_to_add] = df_simulation[col_previous_name]\n",
    "# Make sure 'sim_name', 'perspective', etc. exist for the simulation data frame\n",
    "required_columns = ['sim_name',\n",
    "    'perspective'] + input_sim_parameters_to_infer + features_for_inference + input_sim_parameters_as_features\n",
    "for col in required_columns:\n",
    "    if col not in df_simulation.columns:\n",
    "        raise KeyError(f\"Required column '{col}' not found in CSV.\")\n",
    "\n",
    "# Filter to just the chosen perspective\n",
    "# for simulation\n",
    "df_simulation = df_simulation[df_simulation['perspective'] == perspective_to_use].copy()\n",
    "# for experiment = a bit trickier when using different muscles as pairs, because of the choice to use the \"pair direction\" that maximizes the number of spikes (for example VM<->VL instead of both VM<->VL and VL<->VM)\n",
    "df_experiment = df_experiment[df_experiment['perspective'] == perspective_to_use].copy()\n",
    "\n",
    "# Choose the direction for experimental data (only experimental data, because stored as separate columns for the simulated data)\n",
    "df_experiment = df_experiment[df_experiment['direction']==direction_to_use]\n",
    "if raw_or_corrected == 'raw':\n",
    "    df_experiment[inhib_connectivity_colname] = df_experiment['raw_area']\n",
    "elif raw_or_corrected == 'corrected':\n",
    "    df_experiment[inhib_connectivity_colname] = df_experiment['corrected_area']\n",
    "\n",
    "# If there are any NaNs in output‐column, you might want to drop them:\n",
    "df_simulation = df_simulation[~df_simulation[inhib_connectivity_colname].isna() & ~df_simulation['sync_height'].isna()].copy()\n",
    "df_experiment = df_experiment[~df_experiment[inhib_connectivity_colname].isna() & ~df_experiment['sync_height'].isna()].copy()\n",
    "\n",
    "# ── SCALE THE TWO COLUMNS BEFORE COMPUTING SUMMARIES ──\n",
    "df_simulation[inhib_connectivity_colname] = df_simulation[inhib_connectivity_colname] * (-100)  # make inhibition positive % \n",
    "df_experiment[inhib_connectivity_colname] = df_experiment[inhib_connectivity_colname] * (-100)  # make inhibition positive % \n",
    "\n",
    "df_simulation['sync_height']  = df_simulation['sync_height']  * 100     # make sync_height in % units\n",
    "df_experiment['sync_height']  = df_experiment['sync_height']  * 100     # make sync_height in % units\n",
    "\n",
    "df_simulation['IPSP_timing_of_trough'] = df_simulation['IPSP_timing_of_trough'] * 1000 # convert to ms\n",
    "df_experiment['IPSP_timing_of_trough'] = df_experiment['IPSP_timing_of_trough'] * 1000 # convert to ms\n",
    "\n",
    "df_simulation[f'asymmetry_estimation_diff_{raw_or_corrected}'] = df_simulation[f'asymmetry_estimation_diff_{raw_or_corrected}'] * 100\n",
    "df_experiment[f'asymmetry_diff_{raw_or_corrected}'] = df_experiment[f'asymmetry_diff_{raw_or_corrected}'] / 100 # seems to be already multiplied by 100 twice in this data frame, so dividing by 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulation # checking the result of the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment # checking the result of the previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) A helper to parse ANY JSON‐style array string into a np.array\n",
    "def parse_array(s: str, shape):\n",
    "    \"\"\"\n",
    "    Strip out brackets, commas, newlines, then read all the numbers\n",
    "    and reshape to `shape`.  If it doesn’t fit, return an array of NaNs.\n",
    "    \"\"\"\n",
    "    # replace any bracket or comma with a space\n",
    "    flat = np.fromstring(re.sub(r\"[\\[\\],]\", \" \", s), sep=\" \")\n",
    "    try:\n",
    "        return flat.reshape(shape)\n",
    "    except Exception:\n",
    "        return np.full(shape, np.nan)\n",
    "\n",
    "df = df_simulation  # your DataFrame\n",
    "\n",
    "# 2)  parse the raw array columns with the shapes we expect\n",
    "nb_of_common_inputs_in_simulations = 2 # There is actually a unique common input, but there are two \"spaces\" for common input in the parameters\n",
    "if version_of_sim_with_only_single_pool_input_vals:\n",
    "    df['excitatory_input_baseline_array'] = (\n",
    "        df['excitatory_input_baseline']\n",
    "          .map(lambda v: np.full(nb_pools, v, dtype=float))\n",
    "    )\n",
    "    df['common_input_std_array'] = (\n",
    "        df['common_input_std']\n",
    "        .map(lambda s: parse_array(s, (1, nb_of_common_inputs_in_simulations)))\n",
    "    )\n",
    "    df['frequency_range_of_common_input_array'] = (\n",
    "        df['frequency_range_of_common_input']\n",
    "        .map(lambda s: parse_array(s, (1, nb_of_common_inputs_in_simulations, 2)))\n",
    "    )\n",
    "else: # New simulation version, with a new dim per input\n",
    "    # Here I am hard coding \"2\" as the number of pools nb_pools (even if only one pool was simulated - this is because of how the simulation parameters are organized)\n",
    "    df['excitatory_input_baseline_array'] = (\n",
    "        df['excitatory_input_baseline']\n",
    "        .map(lambda s: parse_array(s, (nb_pools,))) # .map(lambda s: parse_array(s, (nb_pools,)))\n",
    "    )\n",
    "    df['common_input_std_array'] = (\n",
    "        df['common_input_std']\n",
    "        .map(lambda s: parse_array(s, (2, nb_of_common_inputs_in_simulations))) # .map(lambda s: parse_array(s, (nb_pools, nb_of_common_inputs_in_simulations)))\n",
    "    )\n",
    "    df['frequency_range_of_common_input_array'] = (\n",
    "        df['frequency_range_of_common_input']\n",
    "        .map(lambda s: parse_array(s, (2, nb_of_common_inputs_in_simulations, 2))) # .map(lambda s: parse_array(s, (nb_pools, nb_of_common_inputs_in_simulations, 2)))\n",
    "    )\n",
    "\n",
    "# disynpatic_inhib_connections_desired_MN_MN_array did not change across simulation versions\n",
    "df['disynpatic_inhib_connections_desired_MN_MN_array'] = (\n",
    "    df['disynpatic_inhib_connections_desired_MN_MN']\n",
    "    .map(lambda s: parse_array(s, (2, 2))) # hard-coding the number of pools here, because 2x2 all the time\n",
    ")\n",
    "\n",
    "# 3) Selector for “within” vs “between” for baseline excitatory input\n",
    "def select_excit_baseline(arr, within_between, pool_pair, idx_between):\n",
    "    # bail if parsing failed → scalar nan\n",
    "    if not hasattr(arr, \"__getitem__\"):\n",
    "        return float(arr)\n",
    "    # now safe to index arr[0] or arr[idx]\n",
    "    if within_between == 'within':\n",
    "        return float(arr[0])\n",
    "    else:\n",
    "        return float(arr[idx_between[0]] if pool_pair=='pool_0<->pool_1'\n",
    "                     else arr[idx_between[1]])\n",
    "    \n",
    "# 3.1) Selector for second (high freq) common input STD\n",
    "def select_high_freq_input_std(arr, within_between, pool_pair, idx_between):\n",
    "    # bail if parsing failed → scalar nan\n",
    "    if not hasattr(arr, \"__getitem__\"):\n",
    "        return float(arr)\n",
    "    # now safe to index arr[0] or arr[idx]\n",
    "    if within_between == 'within':\n",
    "        return float(arr[0][1]) # arr[0] for first input, arr[1] for second input\n",
    "    else:\n",
    "        return float(arr[idx_between[0]][1] if pool_pair=='pool_0<->pool_1'\n",
    "                     else arr[idx_between[1]][1])\n",
    "\n",
    "# 3b) Guarded slicers\n",
    "def slice_cis(arr):\n",
    "    \"\"\"Return arr[:,0] if arr is array, else return arr.\"\"\"\n",
    "    if hasattr(arr, \"__getitem__\"):\n",
    "        try:\n",
    "            return arr[:,0]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return arr\n",
    "\n",
    "def slice_freq(arr):\n",
    "    \"\"\"Return arr[:,0,1] if arr is array, else return arr.\"\"\"\n",
    "    if hasattr(arr, \"__getitem__\"):\n",
    "        try:\n",
    "            return arr[:,0,1]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return arr\n",
    "\n",
    "# 4) Build scalar columns, using our guarded slicers\n",
    "\n",
    "# excitatory_input_baseline (vector → select_simple does its own slicing)\n",
    "df['excitatory_input_baseline'] = df.apply(\n",
    "    lambda row: select_excit_baseline(\n",
    "        row['excitatory_input_baseline_array'],\n",
    "        within_or_between_pools_sbi,\n",
    "        row['pool_pair'],\n",
    "        idx_between=(0,1)\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# common_input_std: first slice out [:,0], *then* feed to select_simple\n",
    "df['common_input_std'] = df.apply(\n",
    "    lambda row: select_high_freq_input_std(\n",
    "        row['common_input_std_array'],\n",
    "        within_or_between_pools_sbi,\n",
    "        row['pool_pair'],\n",
    "        idx_between=(0,1)\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# 5) disynaptic selector\n",
    "def select_disyn(arr, pool_pair, perspective, direction):\n",
    "    inhib_self = np.nan\n",
    "    inhib_according_to_direction = np.nan\n",
    "    if not hasattr(arr, \"shape\") or len(arr.shape) != 2:\n",
    "        print(\"Incorrect array shape\")\n",
    "    if pool_pair == 'pool_0<->pool_0':\n",
    "        inhib_self = float(arr[0,0])\n",
    "        inhib_according_to_direction = inhib_self\n",
    "    elif pool_pair == 'pool_1<->pool_1':\n",
    "        inhib_self = float(arr[1,1])\n",
    "        inhib_according_to_direction = inhib_self\n",
    "    elif pool_pair == 'pool_0<->pool_1':\n",
    "        inhib_self = float(arr[0,0])\n",
    "        if perspective == 'MU_as_ref': # I'm keeping this to not be too confused if I come back to this, but \"direction\" already encodes the perspective taken, so it's actually the same whether the direction if 'MU_as_ref' or 'other_MUs_as_ref'\n",
    "            if direction == 'inhibiting':\n",
    "                inhib_according_to_direction = float(arr[0,1])\n",
    "            elif direction == 'inhibited':\n",
    "                inhib_according_to_direction = float(arr[1,0])\n",
    "            else:\n",
    "                raise ValueError(\"direction should be either 'inhibiting' or 'inhibited'\")\n",
    "        elif perspective == 'other_MUs_as_ref':# I'm keeping this to not be too confused if I come back to this, but \"direction\" already encodes the perspective taken, so it's actually the same whether the direction if 'MU_as_ref' or 'other_MUs_as_ref'\n",
    "            if direction == 'inhibiting':\n",
    "                inhib_according_to_direction = float(arr[0,1])\n",
    "            elif direction == 'inhibited':\n",
    "                inhib_according_to_direction = float(arr[1,0])\n",
    "            else:\n",
    "                raise ValueError(\"direction should be either 'inhibiting' or 'inhibited'\")\n",
    "        else:\n",
    "            raise ValueError(\"perspective should be either 'MU_as_ref' or 'other_MUs_as_ref'\")\n",
    "    elif pool_pair == 'pool_1<->pool_0':\n",
    "        inhib_self = float(arr[1,1])\n",
    "        if perspective == 'MU_as_ref': # I'm keeping this to not be too confused if I come back to this, but \"direction\" already encodes the perspective taken, so it's actually the same whether the direction if 'MU_as_ref' or 'other_MUs_as_ref'\n",
    "            if direction == 'inhibiting':\n",
    "                inhib_according_to_direction = float(arr[1,0])\n",
    "            elif direction == 'inhibited':\n",
    "                inhib_according_to_direction = float(arr[0,1])\n",
    "            else:\n",
    "                raise ValueError(\"direction should be either 'inhibiting' or 'inhibited'\")\n",
    "        elif perspective == 'other_MUs_as_ref': # I'm keeping this to not be too confused if I come back to this, but \"direction\" already encodes the perspective taken, so it's actually the same whether the direction if 'MU_as_ref' or 'other_MUs_as_ref'\n",
    "            if direction == 'inhibiting':\n",
    "                inhib_according_to_direction = float(arr[1,0])\n",
    "            elif direction == 'inhibited':\n",
    "                inhib_according_to_direction = float(arr[0,1])\n",
    "            else:\n",
    "                raise ValueError(\"direction should be either 'inhibiting' or 'inhibited'\")\n",
    "        else:\n",
    "            raise ValueError(\"perspective should be either 'MU_as_ref' or 'other_MUs_as_ref'\")\n",
    "    else:\n",
    "        raise ValueError(\"Only two pools, pool_0 and pool_1 are accepted as input\")\n",
    "    return inhib_self, inhib_according_to_direction\n",
    "\n",
    "# define the two new column names\n",
    "out_cols = [\n",
    "    'disynpatic_inhib_connections_desired_MN_MN_self',\n",
    "    'disynpatic_inhib_connections_desired_MN_MN'\n",
    "]\n",
    "df[out_cols]  = df.apply(\n",
    "    lambda row: select_disyn(\n",
    "        row['disynpatic_inhib_connections_desired_MN_MN_array'],\n",
    "        row['pool_pair'],\n",
    "        row['perspective'],\n",
    "        row['direction']\n",
    "    ), axis=1,\n",
    "    result_type='expand'\n",
    ")\n",
    "\n",
    "# 6) Now you have both “_array” columns and the scalar versions:\n",
    "#    excitatory_input_baseline_array, excitatory_input_baseline\n",
    "#    common_input_std_array, common_input_std\n",
    "#    frequency_range_of_common_input_array, frequency_range_of_common_input\n",
    "#    disynpatic_inhib_connections_desired_MN_MN_array, disynpatic_inhib_connections_desired_MN_MN\n",
    "\n",
    "# Drop rows where any column value in 'specific_input_parameters_of_interest' is NaN:\n",
    "# df_simulation = df.dropna(subset=specific_input_parameters_of_interest).copy()\n",
    "\n",
    "# Drop rows where any column value in 'input_sim_parameters_to_infer' is NaN:\n",
    "df_simulation = df.dropna(subset=input_sim_parameters_to_infer).copy()\n",
    "\n",
    "# Remove duplicated columns (some messy code somewhere I believe, as some of the input_sim_parameters_to_infer columns can be duplicated)\n",
    "df_simulation = df_simulation.loc[:,~df_simulation.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04d048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulation # check result of prevous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize each feature relative to the training data (simulation)\n",
    "\n",
    "def apply_standardization(df, stats, cols):\n",
    "    \"\"\"\n",
    "    In-place Z-score: (x - μ)/σ for each col in cols,\n",
    "    using stats.loc[col,'μ'] and stats.loc[col,'σ'].\n",
    "    \"\"\"\n",
    "    for feat in cols:\n",
    "        μ, σ = stats.loc[feat, ['μ','σ']]\n",
    "        df[feat] = (df[feat] - μ) / σ\n",
    "\n",
    "def destandardize(df, stats, cols, name_map=None): # to be able to reverse the operation\n",
    "    \"\"\"\n",
    "    In-place inverse of Z-score standardization:\n",
    "        x = z * σ + μ\n",
    "\n",
    "    df:      DataFrame to modify\n",
    "    stats:   DataFrame with index as feature names and columns ['μ','σ']\n",
    "    cols:    columns in df to destandardize (post-renaming)\n",
    "    name_map: optional dict mapping df column name -> stats index name\n",
    "              (useful if you renamed columns after computing stats)\n",
    "    \"\"\"\n",
    "    name_map = name_map or {}\n",
    "    for col in cols:\n",
    "        key = name_map.get(col, col)\n",
    "        if key not in stats.index:\n",
    "            raise KeyError(f\"Stats for feature '{key}' not found in stats.index.\")\n",
    "        μ, σ = stats.loc[key, ['μ','σ']]\n",
    "        if np.isclose(σ, 0.0):\n",
    "            # nothing was scaled; just shift back by μ if needed\n",
    "            df[col] = df[col] + μ\n",
    "        else:\n",
    "            df[col] = df[col] * σ + μ\n",
    "\n",
    "if normalize_scale_of_features_for_inference:\n",
    "    # compute per‐feature mean & std on the simulated data (from prior)\n",
    "    norm_stats = ( # norm stats will record the original meand and std, to reverse the trasnform using destandardize() when needed\n",
    "        df_simulation[features_for_inference]\n",
    "        .agg(['mean','std'])\n",
    "        .transpose()\n",
    "        .rename(columns={'mean':'μ','std':'σ'})\n",
    "    )\n",
    "\n",
    "    # standardize the simulations:\n",
    "    apply_standardization(df_simulation, norm_stats, features_for_inference)\n",
    "\n",
    "    # standardize the real data ##########################\n",
    "    # Make sure all features exist in the experimental data frame\n",
    "    def add_placeholder_columns(df: pd.DataFrame, placeholders: dict, *, inplace: bool = True):\n",
    "        \"\"\"\n",
    "        Add columns that do not already exist in df and fill them with the\n",
    "        provided placeholder values. Existing columns are left untouched.\n",
    "\n",
    "        placeholders: {col_name: scalar_value}\n",
    "        \"\"\"\n",
    "        target = df if inplace else df.copy()\n",
    "        to_add = [c for c in placeholders if c not in target.columns]\n",
    "        for c in to_add:\n",
    "            target[c] = placeholders[c]\n",
    "        return target\n",
    "    # --- build placeholders from μ (means) of the simulation, for your selected features ---\n",
    "    # norm_stats = df_simulation[features_for_inference].agg(['mean','std']).T.rename(columns={'mean':'μ','std':'σ'})\n",
    "    # If names match:\n",
    "    placeholders = {\n",
    "        feat: float(norm_stats.loc[feat, 'μ'])\n",
    "        for feat in features_for_inference\n",
    "        if feat in norm_stats.index\n",
    "    }\n",
    "    # --- add only missing columns to df_experiment, filled with μ as placeholders ---\n",
    "    add_placeholder_columns(df_experiment, placeholders, inplace=True)\n",
    "    # (optional) quick report\n",
    "    added = [c for c in placeholders if c in df_experiment.columns]\n",
    "    print(f\"Added/ensured {len(added)} placeholder columns in df_experiment.\")\n",
    "    # Apply standardization to real data\n",
    "    apply_standardization(df_experiment, norm_stats, features_for_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8a48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just checking\n",
    "df_experiment[features_for_inference]\n",
    "plt.figure()\n",
    "plt.scatter(x=df_experiment[features_for_inference[0]], y=df_experiment[features_for_inference[1]], alpha=0.1)\n",
    "plt.title(\"Experimental data\")\n",
    "plt.xlabel(f\"{features_for_inference[0]} (standardized relative to training data)\")\n",
    "plt.ylabel(f\"{features_for_inference[1]} (standardized relative to training data)\")\n",
    "\n",
    "df_simulation[features_for_inference]\n",
    "plt.figure()\n",
    "plt.scatter(x=df_simulation[features_for_inference[0]], y=df_simulation[features_for_inference[1]], alpha=0.005)\n",
    "plt.title(\"Simulated training data\")\n",
    "plt.xlabel(f\"{features_for_inference[0]} (standardized relative to training data)\")\n",
    "plt.ylabel(f\"{features_for_inference[1]} (standardized relative to training data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── BUILD SUMMARY STATISTICS FOR SIMULATED DATA = EACH (sim_name, pool_pair) & EACH FEATURE ──\n",
    "\n",
    "all_series = []\n",
    "\n",
    "# group by both sim_name and pool_pair\n",
    "group_cols = [\"sim_name\", \"pool_pair\"] # \"pool\"] # \"pool_pair\"]\n",
    "\n",
    "for feat in features_for_inference:\n",
    "    grp = df_simulation.groupby(group_cols)[feat]\n",
    "\n",
    "    if feat not in input_sim_parameters_as_features:\n",
    "        for summary_key, summary_fx in summary_funcs.items():\n",
    "            series = (\n",
    "                grp\n",
    "                .apply(lambda arr: summary_fx(arr))\n",
    "                # name the series \"{feature}_{stat}\"\n",
    "                .rename(f\"{feat}_{summary_key}\")\n",
    "            )\n",
    "            all_series.append(series)\n",
    "    else:\n",
    "        all_series.append(grp.mean())\n",
    "\n",
    "# concat them into one DataFrame; it will have a MultiIndex (sim_name, pool_pair)\n",
    "df_simulation_summary = (\n",
    "    pd.concat(all_series, axis=1)\n",
    "      .reset_index()   # bring sim_name & pool_pair back as columns\n",
    ")\n",
    "\n",
    "# ── MERGE BACK INPUT PARAMETERS FOR EACH (sim_name, pool_pair) ──\n",
    "param_cols = group_cols + input_sim_parameters_to_infer # + ['disynpatic_inhib_connections_desired_MN_MN_self']\n",
    "\n",
    "param_df = (\n",
    "    df_simulation[param_cols]\n",
    "      .drop_duplicates(subset=group_cols)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "df_simulation_summary = df_simulation_summary.merge(\n",
    "    param_df,\n",
    "    on=group_cols,\n",
    "    how=\"inner\",\n",
    ")\n",
    "# df_simulation_summary has one row per sim_name × pool_pair × feature‐stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed3807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulation_summary # checking result of previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818733a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── BUILD SUMMARY STATISTICS FOR EXPERIMENTAL DATA = EACH subject/muscle pair/intensity & EACH FEATURE ──\n",
    "# We will compute “summary_funcs” for each feature in features_for_inference.\n",
    "group_keys = [\"subject\", \"muscle_pair\", \"intensity\"]\n",
    "\n",
    "# Prepare a list to collect all the “per‐feature” Series objects:\n",
    "exp_all_series = []\n",
    "for feat in features_for_inference:\n",
    "    # GroupBy object keyed by subject, muscle_pair, and intensity\n",
    "    grp = df_experiment.groupby(group_keys)[feat]\n",
    "\n",
    "    if feat not in input_sim_parameters_as_features: \n",
    "        for summary_key, summary_fx in summary_funcs.items():\n",
    "            series = (\n",
    "                grp\n",
    "                .apply(lambda arr: summary_fx(arr))\n",
    "                # name the series \"{feature}_{stat}\"\n",
    "                .rename(f\"{feat}_{summary_key}\")\n",
    "            )\n",
    "            exp_all_series.append(series)\n",
    "    else:\n",
    "        exp_all_series.append(grp.mean())\n",
    "\n",
    "# ── Concatenate them into a single DataFrame (multi‐index → columns) ──\n",
    "df_experiment_summary = pd.concat(exp_all_series, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff2c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment_summary # checking result of previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8604a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── BUILD SUMMARY STATISTICS FOR EXPERIMENTAL DATA = EACH muscle pair/intensity & EACH FEATURE ──\n",
    "# Consider all subjects together\n",
    "# We will compute “summary_funcs” for each feature in features_for_inference.\n",
    "group_keys = [\"muscle_pair\", \"intensity\"]\n",
    "\n",
    "# Prepare a list to collect all the “per‐feature” Series objects:\n",
    "exp_all_series = []\n",
    "for feat in features_for_inference:\n",
    "    # GroupBy object keyed by muscle_pair, and intensity\n",
    "    grp = df_experiment.groupby(group_keys)[feat]\n",
    "\n",
    "    # For each summary statistic in summary_funcs:\n",
    "    if feat not in input_sim_parameters_as_features: \n",
    "        for summary_key, summary_fx in summary_funcs.items():\n",
    "            series = (\n",
    "                grp\n",
    "                .apply(lambda arr: summary_fx(arr))\n",
    "                # name the series \"{feature}_{stat}\"\n",
    "                .rename(f\"{feat}_{summary_key}\")\n",
    "            )\n",
    "            exp_all_series.append(series)\n",
    "    else:\n",
    "        exp_all_series.append(grp.mean())\n",
    "\n",
    "# ── Concatenate them into a single DataFrame (multi‐index → columns) ──\n",
    "df_experiment_summary_grouped_subjects = pd.concat(exp_all_series, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment_summary_grouped_subjects # checking result of previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ebded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visually check coverage of training data (summaries) relative to experimental observations (summaries)\n",
    "def plot_pairgrid_experiment_vs_sim(\n",
    "    df_experiment_summary,\n",
    "    df_simulation_summary,\n",
    "    *,\n",
    "    features: list[str],\n",
    "    stat: str,\n",
    "    muscle_colors_dict: dict[str,str],\n",
    "    savepath: str | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    For a given `stat` (e.g. \"mean\", \"sd\", \"median\", \"iqr\") and a list of `features`,\n",
    "    build an (N × N) grid (N = len(features)):\n",
    "      - diagonal (i == j): KDE density plots of df_experiment_summary[f\"{feature}_{stat}\"],\n",
    "        one curve per muscle_pair (low‐alpha fill, high‐alpha outline), plus a dashed‐black\n",
    "        KDE of simulated values for that same feature‐stat.\n",
    "      - off‐diagonal (i != j): scatter of experimental\n",
    "            x = df_experiment_summary[f\"{features[j]}_{stat}\"]\n",
    "            y = df_experiment_summary[f\"{features[i]}_{stat}\"]\n",
    "        colored by muscle_pair; overlay the convex‐hull outline of simulated points:\n",
    "            x_sim = df_simulation_summary[f\"{features[j]}_{stat}\"]\n",
    "            y_sim = df_simulation_summary[f\"{features[i]}_{stat}\"]\n",
    "    One combined legend (muscle_pair→color) is drawn only in the top‐left diagonal cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_experiment_summary : pandas.DataFrame\n",
    "        Must contain columns:\n",
    "          - \"muscle_pair\" (string)\n",
    "          - For each feat in `features`, a column named f\"{feat}_{stat}\"\n",
    "\n",
    "    df_simulation_summary : pandas.DataFrame\n",
    "        Must contain, for each feat in `features`, a column named f\"{feat}_{stat}`.\n",
    "\n",
    "    features : list[str]\n",
    "        e.g. [\"inhibiting_estimation_raw\", \"sync_height\", \"ISI_cov\", ...]\n",
    "\n",
    "    stat : str\n",
    "        One of the summary‐stats, e.g. \"mean\", \"sd\", \"median\", \"iqr\".\n",
    "\n",
    "    muscle_colors_dict : dict[str,str]\n",
    "        Maps each `muscle_pair` string to a matplotlib color (hex or name).\n",
    "\n",
    "    savepath : str or None\n",
    "        If non‐None, the figure is saved to `savepath`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # remove input_sim_parameters_as_features from features if present\n",
    "    features = [f for f in features if f not in input_sim_parameters_as_features]\n",
    "    n_feats = len(features)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_feats,\n",
    "        ncols=n_feats,\n",
    "        figsize=(3*n_feats, 3*n_feats),\n",
    "        squeeze=False,\n",
    "        sharex=False,\n",
    "        sharey=False,\n",
    "    )\n",
    "\n",
    "    # For each (i,j) in the N×N grid:\n",
    "    for i_row, feat_i in enumerate(features):\n",
    "        for j_col, feat_j in enumerate(features):\n",
    "            ax = axes[i_row][j_col]\n",
    "            col_i = f\"{feat_i}_{stat}\"\n",
    "            col_j = f\"{feat_j}_{stat}\"\n",
    "\n",
    "            # Extract experimental arrays (as floats) and muscle labels:\n",
    "            df_experiment_summary_intensity10 = df_experiment_summary[df_experiment_summary[\"intensity\"] == 10]\n",
    "            df_experiment_summary_intensity40 = df_experiment_summary[df_experiment_summary[\"intensity\"] == 40]\n",
    "            exp_i_total = df_experiment_summary[col_i].to_numpy(dtype=float)\n",
    "            exp_j_total = df_experiment_summary[col_j].to_numpy(dtype=float)\n",
    "            exp_i_10 = df_experiment_summary_intensity10[col_i].to_numpy(dtype=float)\n",
    "            exp_j_10 = df_experiment_summary_intensity10[col_j].to_numpy(dtype=float)\n",
    "            exp_i_40 = df_experiment_summary_intensity40[col_i].to_numpy(dtype=float)\n",
    "            exp_j_40 = df_experiment_summary_intensity40[col_j].to_numpy(dtype=float)\n",
    "            muscles = df_experiment_summary[\"muscle_pair\"].astype(str)\n",
    "\n",
    "            # DIAGONAL: KDE density plots of experiment (one per muscle) + sim KDE\n",
    "            if i_row == j_col:\n",
    "                # 1) Gather all non‐NaN experimental and simulated values for this cell:\n",
    "                exp_vals = exp_i_total[~np.isnan(exp_i_total)]\n",
    "                sim_vals = df_simulation_summary[col_i].to_numpy(dtype=float)\n",
    "                sim_vals = sim_vals[~np.isnan(sim_vals)]\n",
    "\n",
    "                # If there is data at all, define a common x‐grid spanning both:\n",
    "                all_vals = np.concatenate([exp_vals, sim_vals]) if sim_vals.size>0 else exp_vals\n",
    "                if all_vals.size>0:\n",
    "                    vmin, vmax = np.nanmin(all_vals), np.nanmax(all_vals)\n",
    "                    span = vmax - vmin\n",
    "                    if span == 0:\n",
    "                        # Single‐value fallback\n",
    "                        x_grid = np.linspace(vmin - 1, vmax + 1, 200)\n",
    "                    else:\n",
    "                        x_grid = np.linspace(vmin - 0.05*span, vmax + 0.05*span, 300)\n",
    "\n",
    "                    # 2) Plot one KDE per muscle:\n",
    "                    for m, color in muscle_colors_dict.items():\n",
    "                        mask_m = (muscles == m)\n",
    "                        data_m = exp_i_total[mask_m]\n",
    "                        data_m = data_m[~np.isnan(data_m)]\n",
    "                        if data_m.size > 1:\n",
    "                            try:\n",
    "                                kde_m = gaussian_kde(data_m)\n",
    "                                y_m = kde_m(x_grid)\n",
    "                                ax.fill_between(\n",
    "                                    x_grid, y_m,\n",
    "                                    facecolor=color,\n",
    "                                    alpha=0.3\n",
    "                                )\n",
    "                                ax.plot(\n",
    "                                    x_grid, y_m,\n",
    "                                    color=color,\n",
    "                                    linewidth=1.2,\n",
    "                                    alpha=1.0\n",
    "                                )\n",
    "                            except Exception:\n",
    "                                # If KDE fails (e.g. singular data), skip\n",
    "                                pass\n",
    "\n",
    "                    # 3) Overlay simulated KDE in dashed black:\n",
    "                    if sim_vals.size > 1:\n",
    "                        try:\n",
    "                            kde_sim = gaussian_kde(sim_vals)\n",
    "                            y_sim = kde_sim(x_grid)\n",
    "                            ax.plot(\n",
    "                                x_grid, y_sim,\n",
    "                                \"--k\",\n",
    "                                linewidth=1.5,\n",
    "                                alpha=1.0,\n",
    "                                label=\"Sim KDE\"\n",
    "                            )\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                ax.set_xlabel(f\"{feat_i} ({stat})\")\n",
    "                ax.set_ylabel(\"Density\")\n",
    "                ax.set_title(f\"{feat_i} ({stat})\")\n",
    "                ax.grid(True, linestyle=\"--\", alpha=0.2)\n",
    "\n",
    "                # Only add legend once (top‐left diagonal cell)\n",
    "                if i_row == 0 and j_col == 0:\n",
    "                    from matplotlib.lines import Line2D\n",
    "                    legend_handles = []\n",
    "                    # muscle patches\n",
    "                    present_pairs = sorted(df_experiment_summary[\"muscle_pair\"].unique())\n",
    "                    for m, color in muscle_colors_dict.items():\n",
    "                        if m in present_pairs:\n",
    "                            legend_handles.append(\n",
    "                                Line2D(\n",
    "                                    [0], [0],\n",
    "                                    marker=\"s\",\n",
    "                                    color=\"w\",\n",
    "                                    markerfacecolor=color,\n",
    "                                    label=m,\n",
    "                                    markersize=8,\n",
    "                                    markeredgecolor=\"k\",\n",
    "                                )\n",
    "                            )\n",
    "                    # “Sim KDE” line\n",
    "                    legend_handles.append(\n",
    "                        Line2D(\n",
    "                            [0], [0],\n",
    "                            linestyle=\"--\",\n",
    "                            color=\"k\",\n",
    "                            linewidth=1.5,\n",
    "                            label=\"Sim KDE\"\n",
    "                        )\n",
    "                    )\n",
    "                    ax.legend(\n",
    "                        handles=legend_handles,\n",
    "                        loc=\"upper right\",\n",
    "                        fontsize=\"small\",\n",
    "                        framealpha=0.7,\n",
    "                        title=\"muscle_pair\"\n",
    "                    )\n",
    "\n",
    "             # OFF‐DIAGONAL: first draw sim‐density background, then scatter + hull\n",
    "            else:\n",
    "                # 1) Gather the sim points (x_sim,y_sim) for this cell\n",
    "                sim_x = df_simulation_summary[col_j].to_numpy(dtype=float)\n",
    "                sim_y = df_simulation_summary[col_i].to_numpy(dtype=float)\n",
    "                valid_sim = (~np.isnan(sim_x)) & (~np.isnan(sim_y))\n",
    "\n",
    "                # 2) If there are ≥2 valid simulated points, build a 2D KDE:\n",
    "                if np.count_nonzero(valid_sim) > 1:\n",
    "                    pts_sim = np.vstack([sim_x[valid_sim], sim_y[valid_sim]]).T\n",
    "\n",
    "                    try:\n",
    "                        kde2d = gaussian_kde(pts_sim.T)\n",
    "                        # Determine the grid limits from the sim‐points themselves:\n",
    "                        x_min, x_max = np.nanmin(sim_x[valid_sim]), np.nanmax(sim_x[valid_sim])\n",
    "                        y_min, y_max = np.nanmin(sim_y[valid_sim]), np.nanmax(sim_y[valid_sim])\n",
    "\n",
    "                        # Expand slightly so we see a margin:\n",
    "                        x_span = x_max - x_min if (x_max > x_min) else 1.0\n",
    "                        y_span = y_max - y_min if (y_max > y_min) else 1.0\n",
    "                        x_min -= 0.03*x_span;  x_max += 0.03*x_span\n",
    "                        y_min -= 0.03*y_span;  y_max += 0.03*y_span\n",
    "\n",
    "                        # Build a 100×100 grid over [x_min,x_max]×[y_min,y_max]\n",
    "                        xi = np.linspace(x_min, x_max, 100)\n",
    "                        yi = np.linspace(y_min, y_max, 100)\n",
    "                        xi_mesh, yi_mesh = np.meshgrid(xi, yi)\n",
    "                        grid_coords = np.vstack([xi_mesh.ravel(), yi_mesh.ravel()])\n",
    "\n",
    "                        # Evaluate KDE on the grid:\n",
    "                        zi = kde2d(grid_coords).reshape(xi_mesh.shape)\n",
    "\n",
    "                        # 3) Normalize zi to [0,1]:\n",
    "                        zi_min, zi_max = zi.min(), zi.max()\n",
    "                        if zi_max > zi_min:\n",
    "                            zi_norm = (zi - zi_min) / (zi_max - zi_min)\n",
    "                        else:\n",
    "                            zi_norm = np.zeros_like(zi)\n",
    "\n",
    "                        # 4) Now display it with imshow, using cmap=\"gray\":\n",
    "                        ax.imshow(\n",
    "                            zi_norm,\n",
    "                            origin=\"lower\",\n",
    "                            extent=(x_min, x_max, y_min, y_max),\n",
    "                            cmap=\"gray\",\n",
    "                            aspect=\"auto\",\n",
    "                            alpha=0.5,\n",
    "                            zorder=0\n",
    "                        )\n",
    "\n",
    "                        # Set axis limits so scatter/hull match:\n",
    "                        ax.set_xlim(x_min, x_max)\n",
    "                        ax.set_ylim(y_min, y_max)\n",
    "\n",
    "                    except Exception:\n",
    "                        # If KDE fails, simply skip the heatmap background\n",
    "                        pass\n",
    "\n",
    "                # 5) Now plot the experimental scatter on top:\n",
    "                # for intensity = 10%\n",
    "                x_exp = exp_j_10\n",
    "                y_exp = exp_i_10\n",
    "                colors = df_experiment_summary_intensity10[\"muscle_pair\"].map(muscle_colors_dict).fillna(\"gray\")\n",
    "                mask_xy = (~np.isnan(x_exp)) & (~np.isnan(y_exp))\n",
    "                if np.any(mask_xy):\n",
    "                    ax.scatter(\n",
    "                        x_exp[mask_xy],\n",
    "                        y_exp[mask_xy],\n",
    "                        c=colors[mask_xy],\n",
    "                        marker=\"o\",\n",
    "                        edgecolor=\"k\",\n",
    "                        alpha=0.7,\n",
    "                        s=30,\n",
    "                        zorder=2\n",
    "                    )\n",
    "                # for intensity = 40%\n",
    "                x_exp = exp_j_40\n",
    "                y_exp = exp_i_40\n",
    "                colors = df_experiment_summary_intensity40[\"muscle_pair\"].map(muscle_colors_dict).fillna(\"gray\")\n",
    "                mask_xy = (~np.isnan(x_exp)) & (~np.isnan(y_exp))\n",
    "                if np.any(mask_xy):\n",
    "                    ax.scatter(\n",
    "                        x_exp[mask_xy],\n",
    "                        y_exp[mask_xy],\n",
    "                        c=colors[mask_xy],\n",
    "                        marker=\"^\",\n",
    "                        edgecolor=\"k\",\n",
    "                        alpha=0.7,\n",
    "                        s=30,\n",
    "                        zorder=2\n",
    "                    )\n",
    "\n",
    "                # 6) Overlay the convex‐hull of simulated points (as black dashed outline)\n",
    "                valid_sim2 = valid_sim  # same mask\n",
    "                if np.count_nonzero(valid_sim2) >= 3:\n",
    "                    pts_sim2 = np.vstack([sim_x[valid_sim2], sim_y[valid_sim2]]).T\n",
    "                    try:\n",
    "                        hull = ConvexHull(pts_sim2)\n",
    "                        hull_pts = pts_sim2[hull.vertices]\n",
    "                        hull_loop = np.vstack([hull_pts, hull_pts[0]])\n",
    "                        ax.plot(\n",
    "                            hull_loop[:,0],\n",
    "                            hull_loop[:,1],\n",
    "                            \"--k\",\n",
    "                            lw=1.2,\n",
    "                            zorder=3\n",
    "                        )\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                ax.set_xlabel(f\"{feat_j} ({stat})\")\n",
    "                ax.set_ylabel(f\"{feat_i} ({stat})\")\n",
    "                # ax.grid(True, linestyle=\"--\", alpha=0.2)\n",
    "\n",
    "    if normalize_scale_of_features_for_inference:\n",
    "        fig.suptitle(\"Standardized values relative to the chosen features of the experimental data\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if savepath is not None:\n",
    "        plt.savefig(savepath, dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats_names = summary_funcs.keys()\n",
    "for stat in summary_stats_names:\n",
    "    plot_pairgrid_experiment_vs_sim(\n",
    "        df_experiment_summary,\n",
    "        df_simulation_summary,\n",
    "        features=features_for_inference,\n",
    "        stat=stat,\n",
    "        muscle_colors_dict=muscle_colors_dict,\n",
    "        savepath=f\"{path_to_save}\\\\pairgrid_{stat}.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a0d973",
   "metadata": {},
   "source": [
    "# Evaluating inference on held-out simulated data (posterior estimates VS ground truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537eab7f",
   "metadata": {},
   "source": [
    "### Training model (90% of the training data; 10% used as test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11affc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Build Torch tensors & prior for SBI ──\n",
    "# Collect the “feature‐summary” column names in exactly the same order:\n",
    "summary_colnames = []\n",
    "for feat in features_for_inference:\n",
    "    if feat in input_sim_parameters_as_features:\n",
    "        summary_colnames += [f\"{feat}\"]\n",
    "    else:\n",
    "        for summary_stat in summary_funcs.keys():\n",
    "            summary_colnames +=  [f\"{feat}_{summary_stat}\"]\n",
    "theta_colnames = input_sim_parameters_to_infer\n",
    "# The commented lines cause error with recent numpy versions\n",
    "# sim_obs     = torch.from_numpy(df_simulation_summary[summary_colnames].to_numpy(dtype=float)).float()\n",
    "# theta_raw = torch.from_numpy(df_simulation_summary[theta_colnames].to_numpy(dtype=float)).float()\n",
    "sim_obs = torch.tensor(\n",
    "    df_simulation_summary[summary_colnames].values.tolist(),\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "theta_raw = torch.tensor(\n",
    "    df_simulation_summary[theta_colnames].values.tolist(),\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# BOUNDARIES OF PRIORS\n",
    "low_original = []\n",
    "high_original = []\n",
    "low_unit = []\n",
    "high_unit = []\n",
    "# ── Build low/high tensors from priors_per_parameters dict ──\n",
    "low_original  = torch.tensor([priors_per_parameters_to_infer[name][0] for name in theta_colnames],\n",
    "                     dtype=torch.float32)\n",
    "high_original = torch.tensor([priors_per_parameters_to_infer[name][1] for name in theta_colnames],\n",
    "                     dtype=torch.float32)\n",
    "# ── Define normalization ↔︎ denormalization helpers ──\n",
    "def theta_to_unit(theta):\n",
    "    \"\"\"Map from original θ-space into [0,1]^d.\"\"\"\n",
    "    return (theta - low_original) / (high_original - low_original)\n",
    "def unit_to_theta(theta_unit):\n",
    "    \"\"\"Map from [0,1]^d back to original θ-space.\"\"\"\n",
    "    return theta_unit * (high_original - low_original) + low_original\n",
    "# ── Define a uniform prior on [0,1]^d ──\n",
    "low_unit = theta_to_unit(low_original) # If everything works well, set all of them to 0 (because 0 is the normalized min value)\n",
    "high_unit = theta_to_unit(high_original) # If everything works well, set all of them to 1 (because 1 is the normalized max value)\n",
    "# ── Normalize your training θ’s ──\n",
    "theta_unit = theta_to_unit(theta_raw)\n",
    "prior = sbi_utils.BoxUniform(low=torch.zeros_like(low_unit),\n",
    "                             high=torch.ones_like(high_unit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0355dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun_network_training_for_heldout_data:\n",
    "    # PARAMETERS AND SELECTION OF HELD OUT SAMPLES\n",
    "    held_out_proportion = 0.1 # 12_000 examples in the dataset, so training on 10_800 and testing on 1_200\n",
    "    sim_samples_nb = df_simulation_summary.shape[0]\n",
    "    rng = np.random.default_rng(seed=random.randint(0,int(1e5))) # random selector\n",
    "    hold_out_idx = rng.choice(sim_samples_nb,\n",
    "                                size=np.round(held_out_proportion*sim_samples_nb).astype(int),\n",
    "                                replace=False)\n",
    "    hold_out_idx = np.sort(hold_out_idx)\n",
    "    training_idx = np.setdiff1d(np.arange(sim_samples_nb), hold_out_idx)\n",
    "\n",
    "    # Converts numpy int to Python list (this makes Pytorch happy)\n",
    "    hold_out_idx = hold_out_idx.tolist()\n",
    "    training_idx = training_idx.tolist()\n",
    "\n",
    "    # Separate held-out (testing) and training data\n",
    "    theta_ground_truth_hold_out = theta_unit[hold_out_idx]\n",
    "    theta_ground_truth_training = theta_unit[training_idx]\n",
    "    obs_hold_out = sim_obs[hold_out_idx]\n",
    "    obs_training = sim_obs[training_idx]\n",
    "\n",
    "    # Train the SBI network - SNPE\n",
    "    inference_net_held_out_data = sbi_inference.SNPE(prior=prior,\n",
    "                                                    density_estimator=sbi_density_estimator,\n",
    "                                                    show_progress_bars=True)\n",
    "    inference_net_held_out_data.append_simulations(theta_ground_truth_training, obs_training)\n",
    "    inference_net_held_out_data.train(\n",
    "            num_atoms                  = network_training_hyperparameters['num_atoms'],       # default is 10\n",
    "            force_first_round_loss     = True,    # start fresh\n",
    "            training_batch_size        = network_training_hyperparameters['training_batch_size'],     # default is 200\n",
    "            learning_rate              = network_training_hyperparameters['learning_rate'],  # default is 0.0005\n",
    "            validation_fraction        = network_training_hyperparameters['validation_fraction'],      # default is 10%\n",
    "            max_num_epochs             = network_training_hyperparameters['max_num_epochs'],    # train up to 1000 epochs\n",
    "            stop_after_epochs          = network_training_hyperparameters['stop_after_epochs'],      # but at least train 20\n",
    "            show_train_summary         = True\n",
    "        )\n",
    "    \n",
    "    # Saving in a dictionary the relevant data for checking the performance of the network trained on heldout data\n",
    "    heldout_data_sbi_check = {\n",
    "        \"neural_density_estimator_network\": inference_net_held_out_data,\n",
    "        \"theta_ground_truth_hold_out\": theta_ground_truth_hold_out,\n",
    "        \"obs_hold_out\": obs_hold_out,\n",
    "        \"theta_ground_truth_training\": theta_ground_truth_training,\n",
    "        \"obs_training\": obs_training,\n",
    "        \"hold_out_idx\": hold_out_idx,\n",
    "        \"training_idx\": training_idx,\n",
    "        \"held_out_proportion\": held_out_proportion\n",
    "    }\n",
    "    sbi_check_heldout_sims_pickle_path = f\"{path_to_save}\\\\sbi_check_heldout_sims.pkl\"\n",
    "    with open(sbi_check_heldout_sims_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(heldout_data_sbi_check, f)\n",
    "    print(f\"✅ Neural density estimator trained on heldout data saved to '{sbi_check_heldout_sims_pickle_path}'\")\n",
    "else:\n",
    "    # Load previously trained network\n",
    "    sbi_check_heldout_sims_pickle_path = f\"{path_to_load}\\\\sbi_check_heldout_sims.pkl\"\n",
    "    with open(sbi_check_heldout_sims_pickle_path, \"rb\") as f:\n",
    "        heldout_data_sbi_check = pickle.load(f)\n",
    "    print(f\"✅  Neural density estimator trained on heldout data loaded from '{sbi_check_heldout_sims_pickle_path}'\")\n",
    "    inference_net_held_out_data = heldout_data_sbi_check[\"neural_density_estimator_network\"]\n",
    "    theta_ground_truth_hold_out = heldout_data_sbi_check[\"theta_ground_truth_hold_out\"]\n",
    "    obs_hold_out = heldout_data_sbi_check[\"obs_hold_out\"] # re-assigning, as the normalization of units may not have been performed in the loaded data\n",
    "    theta_ground_truth_training = heldout_data_sbi_check[\"theta_ground_truth_training\"]\n",
    "    obs_training = heldout_data_sbi_check[\"obs_training\"]\n",
    "    hold_out_idx_raw = heldout_data_sbi_check[\"hold_out_idx\"]\n",
    "    training_idx_raw = heldout_data_sbi_check[\"training_idx\"]\n",
    "    held_out_proportion = heldout_data_sbi_check[\"held_out_proportion\"] if \"held_out_proportion\" in heldout_data_sbi_check else len(hold_out_idx_raw) / (len(hold_out_idx_raw) + len(training_idx_raw))\n",
    "    # Normalize any np.array / np.int64 into plain Python int lists\n",
    "    hold_out_idx   = [int(i) for i in (hold_out_idx_raw.tolist()   if hasattr(hold_out_idx_raw, \"tolist\")   else hold_out_idx_raw)]\n",
    "    training_idx   = [int(i) for i in (training_idx_raw.tolist()   if hasattr(training_idx_raw, \"tolist\")   else training_idx_raw)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fdeafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Consistency checks and auto-alignment of sim_obs to saved obs_* ---\n",
    "\n",
    "# 1) Basic checks: do θ and obs match the current tensors?\n",
    "theta_match = torch.allclose(\n",
    "    theta_ground_truth_hold_out,\n",
    "    theta_unit[hold_out_idx]\n",
    ")\n",
    "\n",
    "obs_match = torch.allclose(\n",
    "    obs_hold_out,\n",
    "    sim_obs[hold_out_idx]\n",
    ")\n",
    "\n",
    "print(\"theta match:\", theta_match)\n",
    "print(\"obs match :\", obs_match)\n",
    "\n",
    "# 3) If obs don't match, infer the training-time transform and align sim_obs.\n",
    "#    We assume an affine, feature-wise transform of the form:\n",
    "#        Y ≈ (X - shift) / scale\n",
    "#    where:\n",
    "#        X = current sim_obs[training_idx_py]\n",
    "#        Y = saved obs_training (the canonical training space)\n",
    "if not obs_match:\n",
    "    print(\n",
    "        \"→ obs_hold_out and sim_obs[hold_out_idx] differ in scale; \"\n",
    "        \"attempting to align sim_obs to the saved training/held-out scale.\"\n",
    "    )\n",
    "\n",
    "    def describe_stats(name: str, X: torch.Tensor):\n",
    "        \"\"\"Print rough summary stats to see who looks standardized.\"\"\"\n",
    "        mean = X.mean(dim=0)\n",
    "        std  = X.std(dim=0, unbiased=False)\n",
    "        mean_abs = float(mean.abs().mean())\n",
    "        std_mean = float(std.mean())\n",
    "        print(f\"{name}: mean(|μ|) ≈ {mean_abs:.3f}, mean(σ) ≈ {std_mean:.3f}\")\n",
    "        return mean, std\n",
    "\n",
    "    # Use the TRAINING subset to infer the transform that was used originally.\n",
    "    # X_train: current representation (raw or differently scaled)\n",
    "    # Y_train: saved representation (what SNPE actually saw during training)\n",
    "    X_train = sim_obs[training_idx]   # shape [N_train, D]\n",
    "    Y_train = obs_training               # shape [N_train, D]\n",
    "\n",
    "    X_mean, X_std = describe_stats(\"sim_obs[training_idx]\", X_train)\n",
    "    Y_mean, Y_std = describe_stats(\"obs_training\",          Y_train)\n",
    "\n",
    "    # Infer per-feature scale and shift assuming:\n",
    "    #   Y ≈ (X - shift) / scale  ⇒  X ≈ scale * Y + shift\n",
    "    eps   = 1e-12\n",
    "    scale = X_std / (Y_std + eps)              # element-wise [D]\n",
    "    shift = X_mean - scale * Y_mean           # element-wise [D]\n",
    "\n",
    "    # Apply the same transform to ALL rows in sim_obs so that:\n",
    "    #   sim_obs_aligned ≈ Y when restricted to training/held-out indices\n",
    "    sim_obs = (sim_obs - shift) / (scale + eps)\n",
    "\n",
    "    # Re-check match specifically on the held-out set\n",
    "    obs_match_aligned = torch.allclose(\n",
    "        obs_hold_out,\n",
    "        sim_obs[hold_out_idx],\n",
    "        atol=1e-5,\n",
    "        rtol=1e-4,\n",
    "    )\n",
    "    print(\"obs match after alignment:\", obs_match_aligned)\n",
    "\n",
    "# At this point:\n",
    "# - θ space is already consistent via theta_to_unit / unit_to_theta.\n",
    "# - sim_obs has been rescaled (if needed) so that its held-out rows match\n",
    "#   the obs_hold_out that the loaded SNPE model expects.\n",
    "# - All later uses of sim_obs with this loaded model will now be in the\n",
    "#   correct observation space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training results\n",
    "plt.plot(figsize=(8,6))\n",
    "plt.plot(inference_net_held_out_data.summary['training_loss'],\n",
    "            label=\"Training loss\", linewidth=3, color='blue', alpha=0.5)\n",
    "plt.plot(inference_net_held_out_data.summary['validation_loss'],\n",
    "            label=\"Validation loss\", linewidth=3, color='red', alpha=0.5)\n",
    "plt.xlabel(\"epochs_trained\")\n",
    "plt.ylabel(f\"Loss - negative log-probability\\n(neg-log-prob of observation,\\ngiven inferred generative parameters)\")\n",
    "plt.title(f\"Training results on SBI neural net\\nwith {(1-held_out_proportion)*100}% of the simulated observations\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{path_to_save}\\\\network_training_loss_inference_check.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e1576",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DIFFERENCE & DISTANCE FROM POSTERIOR (VECTORS SAMPLED FROM POSTERIOR) TO GROUND TRUTH\n",
    "\n",
    "posterior_net_held_out_data = inference_net_held_out_data.build_posterior()\n",
    "# Infer the posterior probability distribution over theta for each held-out sim\n",
    "# -> Get the probability distribution by sampling the posterior\n",
    "# Compute, for each sample, the distance (for each theta, and in theta-space) between the sample and the ground-truth.\n",
    "# Then, get the mean and SD of distance (again, for each theta and for the overall theta-space)\n",
    "n_held_out_sims = len(hold_out_idx)\n",
    "n_params_to_infer = theta_unit.shape[-1]\n",
    "n_posterior_draws = num_posterior_samples[\"simulation\"]\n",
    "\n",
    "estimate_to_ground_truth_diff_for_each_theta_mean = torch.zeros(n_held_out_sims, n_params_to_infer)\n",
    "estimate_to_ground_truth_diff_for_each_theta_std = torch.zeros(n_held_out_sims, n_params_to_infer)\n",
    "estimate_to_ground_truth_abs_diff_for_each_theta_mean = torch.zeros(n_held_out_sims, n_params_to_infer)\n",
    "estimate_to_ground_truth_abs_diff_for_each_theta_std = torch.zeros(n_held_out_sims, n_params_to_infer)\n",
    "estimate_to_ground_truth_euc_dist_in_theta_space_mean = torch.zeros(n_held_out_sims)\n",
    "estimate_to_ground_truth_euc_dist_in_theta_space_std = torch.zeros(n_held_out_sims)\n",
    "\n",
    "num_posterior_samples[\"simulation\"] = 1_000 # 10_000\n",
    "print(f\"Posterior estimation of simulated samples that were kept out of the training dataset ({len(hold_out_idx)} samples)\\nDrawing {num_posterior_samples[\"simulation\"]} samples from posterior for each simulation\")\n",
    "posterior_samples_thetas_for_held_out_sims = {}\n",
    "posterior_log_probs_for_held_out_sims = {}\n",
    "ground_truth_thetas_for_held_out_sims = {}\n",
    "\n",
    "for iter in range(n_held_out_sims):\n",
    "    sim_id = hold_out_idx[iter]  # original global index, for dict keys\n",
    "\n",
    "    # # 0) Make fewer iterations for testing purpose\n",
    "    # if iter > 100:\n",
    "    #     break\n",
    "    # ^ CAREFUL !!!! IF this is not uncommented, the average distance will be much smaller because the arrays are initialized with zeros !!!\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"    Iteration {iter}/{len(hold_out_idx)}\")\n",
    "\n",
    "    # 1) Grab ground truth and observation *from saved tensors*\n",
    "    theta_ground_truth = theta_ground_truth_hold_out[iter].unsqueeze(0)    # [1, D]\n",
    "    obs_hold_out_current_sim = obs_hold_out[iter].unsqueeze(0)            # [1, Dx]\n",
    "\n",
    "    # 2) Draw posterior samples\n",
    "    theta_samples = posterior_net_held_out_data.sample(\n",
    "        (num_posterior_samples[\"simulation\"],),\n",
    "        x=obs_hold_out_current_sim,\n",
    "        show_progress_bars=False,\n",
    "    )  # [N, D]\n",
    "\n",
    "    logp_sample = posterior_net_held_out_data.log_prob(\n",
    "        theta_samples, x=obs_hold_out_current_sim\n",
    "    )  # [N]\n",
    "\n",
    "    posterior_samples_thetas_for_held_out_sims[sim_id] = theta_samples\n",
    "    posterior_log_probs_for_held_out_sims[sim_id]      = logp_sample\n",
    "    ground_truth_thetas_for_held_out_sims[sim_id]      = theta_ground_truth\n",
    "\n",
    "    # 3) Per-parameter diffs\n",
    "    diffs = theta_samples - theta_ground_truth               # [N, D]\n",
    "    abs_diffs = diffs.abs()\n",
    "\n",
    "    estimate_to_ground_truth_diff_for_each_theta_mean[iter, :]     = diffs.mean(dim=0)\n",
    "    estimate_to_ground_truth_diff_for_each_theta_std[iter, :]      = diffs.std(dim=0)\n",
    "    estimate_to_ground_truth_abs_diff_for_each_theta_mean[iter, :] = abs_diffs.mean(dim=0)\n",
    "    estimate_to_ground_truth_abs_diff_for_each_theta_std[iter, :]  = abs_diffs.std(dim=0)\n",
    "\n",
    "    # 4) Euclidean distance in θ-space\n",
    "    euc_dists = torch.norm(diffs, dim=1)   # [N]\n",
    "    estimate_to_ground_truth_euc_dist_in_theta_space_mean[iter] = euc_dists.mean()\n",
    "    estimate_to_ground_truth_euc_dist_in_theta_space_std[iter]  = euc_dists.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3772721",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE CHECK # 1\n",
    "# Comparing posterior distribution (right) centered on ground truth (difference of 0 = ground truth) for each parameter to infer, and compared to prior distribution (left; flat)\n",
    "\n",
    "# ===========================================\n",
    "# Centered prior/posterior overlays with:\n",
    "# - progress bars\n",
    "# - optional subsampling of held-out sims\n",
    "# - global y-scale across all panels\n",
    "# - configurable x-limits\n",
    "# - aggregate mean ± SD bands\n",
    "# - info gain (KL) and shrinkage summaries; per-parameter info gain (1D KL to Uniform[0,1]\n",
    "# - RED curve: best-guess offsets KDE/HIST across sims\n",
    "# ===========================================\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------- CONFIG --------\n",
    "PLOT_KIND = 'kde'           # 'hist' or 'kde'\n",
    "SUBSAMPLE_SIMS = 100 # 100       # e.g., 300 (or None for all held-out sims)\n",
    "SUBSAMPLE_SEED = 0\n",
    "XLIM = (-1, 1) # (-0.6, 0.6)          # shared x-range for all panels\n",
    "FIGSIZE = (12, 20)\n",
    "ALPHA_LINE = 0.05  # line alpha\n",
    "NBINS = 101                 # hist bins\n",
    "KDE_POINTS = 401            # points for x-grid (kde)\n",
    "KDE_BW = 0.15               # KDE bandwidth on centered axis # (lower = more precise, less smooth)\n",
    "LINEWIDTH_THIN = 2.0\n",
    "LINEWIDTH_THICK = 2.0\n",
    "AGGREGATE_ON_ALL = True     # mean±SD bands computed from ALL held-out sims (not just displayed)\n",
    "SHOW_BEST_GUESS_CURVE = True\n",
    "SHOW_KL_TEXT = True\n",
    "SAVEFIG_PATH = path_to_save\n",
    "\n",
    "# ---- Names & IDs ----\n",
    "param_names = input_sim_parameters_to_infer if 'input_sim_parameters_to_infer' in globals() \\\n",
    "             else [f\"θ[{j}]\" for j in range(theta_unit.shape[-1])]\n",
    "\n",
    "# normalize keys to plain int\n",
    "all_ids = sorted(int(k) for k in posterior_samples_thetas_for_held_out_sims.keys())\n",
    "assert len(all_ids) > 0, \"No held-out sims found.\"\n",
    "\n",
    "theta_true_by_sim_display = {}\n",
    "theta_true_by_sim_all = {}\n",
    "\n",
    "# choose which sims to DRAW (display subset)\n",
    "if SUBSAMPLE_SIMS is not None and SUBSAMPLE_SIMS < len(all_ids):\n",
    "    rng = np.random.default_rng(SUBSAMPLE_SEED)\n",
    "    sel_ids = list(rng.choice(all_ids, size=SUBSAMPLE_SIMS, replace=False))\n",
    "else:\n",
    "    sel_ids = all_ids\n",
    "\n",
    "D = len(param_names)\n",
    "first_key = all_ids[0]\n",
    "n_draws_per_sim = int(posterior_samples_thetas_for_held_out_sims[first_key].shape[0])\n",
    "\n",
    "def sample_prior(n, d):\n",
    "    \"\"\"Sample from the prior as a NumPy array of shape (n, d).\"\"\"\n",
    "    if 'prior' in globals():\n",
    "        with torch.no_grad():\n",
    "            samples = prior.sample((n,))   # torch tensor [n, d]\n",
    "        return to_np(samples)              # safe conversion to NumPy\n",
    "    # Fallback: uniform on [0, 1]^d\n",
    "    rng = np.random.default_rng(0)\n",
    "    return rng.uniform(0.0, 1.0, size=(n, d))\n",
    "\n",
    "def kde_curve(samples, xgrid, bw):\n",
    "    x = np.clip(samples, XLIM[0], XLIM[1])\n",
    "    if x.size == 0:\n",
    "        return np.zeros_like(xgrid)\n",
    "    inv = 1.0 / np.sqrt(2*np.pi*bw*bw)\n",
    "    diffs = xgrid[None, :] - x[:, None]            # [n, M]\n",
    "    y = inv * np.exp(-0.5 * (diffs / bw)**2)       # [n, M]\n",
    "    return y.mean(axis=0)\n",
    "\n",
    "def hist_curve(samples, edges):\n",
    "    x = np.clip(samples, XLIM[0], XLIM[1])\n",
    "    counts, _ = np.histogram(x, bins=edges, density=True)\n",
    "    return counts  # length = len(edges)-1\n",
    "\n",
    "def kl_to_uniform_1d(samples, M=256, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Approximate KL(p || Uniform[0,1]) for 1D posterior samples in [0,1].\n",
    "    Uses discrete bins with mass p_k (sum p_k = 1), prior mass q_k = 1/M.\n",
    "    KL = sum_k p_k * log(p_k / q_k) = sum p_k * log p_k + log M.\n",
    "    \"\"\"\n",
    "    s = np.clip(samples, 0.0, 1.0)\n",
    "    counts, _ = np.histogram(s, bins=M, range=(0.0, 1.0))\n",
    "    p = counts.astype(np.float64)\n",
    "    Z = p.sum()\n",
    "    if Z <= 0:\n",
    "        return 0.0\n",
    "    p /= Z\n",
    "    p = np.clip(p, eps, 1.0)\n",
    "    return float(np.sum(p * np.log(p)) + np.log(M))\n",
    "\n",
    "def flat_prior_curve_centered(theta_true_j, xgrid, edges=None):\n",
    "    \"\"\"\n",
    "    Prior is Uniform[0,1]. Centering by θ_true makes support [a,b]=[-θ_true, 1-θ_true] on the centered axis.\n",
    "    We return a flat line at density=1 on [a,b], and NaN outside (so nanmean/nanstd work).\n",
    "    If edges is given (hist mode), xgrid are the LEFT edges and we assign 1.0 to bins that overlap [a,b).\n",
    "    \"\"\"\n",
    "    a = -float(theta_true_j)\n",
    "    b = 1.0 - float(theta_true_j)\n",
    "\n",
    "    # clip to plotting window\n",
    "    a = max(a, XLIM[0]); b = min(b, XLIM[1])\n",
    "    if a >= b:\n",
    "        # no overlap with visible window\n",
    "        return np.full_like(xgrid, np.nan, dtype=float)\n",
    "\n",
    "    if edges is None:\n",
    "        # KDE/line mode: xgrid are points\n",
    "        y = np.full_like(xgrid, np.nan, dtype=float)\n",
    "        mask = (xgrid >= a) & (xgrid <= b)\n",
    "        y[mask] = 1.0\n",
    "        return y\n",
    "    else:\n",
    "        # HIST mode: xgrid = edges[:-1] (left edges); mark bins that overlap any part of [a,b)\n",
    "        left = edges[:-1]\n",
    "        right = edges[1:]\n",
    "        overlap = (left < b) & (right > a)\n",
    "        y = np.full(left.shape, np.nan, dtype=float)\n",
    "        y[overlap] = 1.0\n",
    "        return y\n",
    "\n",
    "\n",
    "# ---- Collect centered arrays (DISPLAY subset) ----\n",
    "posterior_centered_disp = [[] for _ in range(D)]\n",
    "prior_centered_disp     = [[] for _ in range(D)]\n",
    "\n",
    "# For bands & best-guess & KL we’ll use ALL sims (robust)\n",
    "posterior_centered_all = [[] for _ in range(D)] if AGGREGATE_ON_ALL else posterior_centered_disp\n",
    "prior_centered_all     = [[] for _ in range(D)] if AGGREGATE_ON_ALL else prior_centered_disp\n",
    "\n",
    "# best-guess offsets per parameter (across ALL sims)\n",
    "best_guess_offsets_by_param = [[] for _ in range(D)]\n",
    "# per-parameter KL list across sims\n",
    "kl_list_by_param = [[] for _ in range(D)]\n",
    "\n",
    "print(f\"Collecting centered samples for display (sims={len(sel_ids)}) \"\n",
    "      f\"{'(aggregate on ALL held-out sims)' if AGGREGATE_ON_ALL else '(aggregate on displayed sims)'}\")\n",
    "\n",
    "# display subset\n",
    "for sim_id in tqdm(sel_ids, desc=\"Centering (display subset)\"):\n",
    "    theta_true = to_np(ground_truth_thetas_for_held_out_sims[sim_id]).reshape(-1)\n",
    "    theta_true_by_sim_display[sim_id] = theta_true\n",
    "    post = to_np(posterior_samples_thetas_for_held_out_sims[sim_id])              # [n_draws, D]\n",
    "    prior_s = sample_prior(n_draws_per_sim, post.shape[1])\n",
    "    centered_post = post - theta_true[None, :]\n",
    "    centered_prior = prior_s - theta_true[None, :]\n",
    "    for j in range(D):\n",
    "        posterior_centered_disp[j].append(centered_post[:, j])\n",
    "        prior_centered_disp[j].append(centered_prior[:, j])\n",
    "\n",
    "# aggregate scope (ALL sims) + best-guess offsets + 1D KLs\n",
    "for sim_id in tqdm(all_ids, desc=\"Aggregate scope (bands, best-guess, KL)\"):\n",
    "    theta_true = to_np(ground_truth_thetas_for_held_out_sims[sim_id]).reshape(-1)\n",
    "    theta_true_by_sim_all[sim_id] = theta_true\n",
    "    post = to_np(posterior_samples_thetas_for_held_out_sims[sim_id])              # [n_draws, D]\n",
    "    prior_s = sample_prior(n_draws_per_sim, post.shape[1])\n",
    "\n",
    "    # centered arrays\n",
    "    cp = post - theta_true[None, :]\n",
    "    cpr = prior_s - theta_true[None, :]\n",
    "    if AGGREGATE_ON_ALL:\n",
    "        for j in range(D):\n",
    "            posterior_centered_all[j].append(cp[:, j])\n",
    "            prior_centered_all[j].append(cpr[:, j])\n",
    "\n",
    "    # best-guess (argmax logp) and offsets\n",
    "    if sim_id in posterior_log_probs_for_held_out_sims:\n",
    "        logp = to_np(posterior_log_probs_for_held_out_sims[sim_id]).reshape(-1)\n",
    "        idx_best = int(np.argmax(logp))\n",
    "        best = post[idx_best, :]\n",
    "    else:\n",
    "        best = post.mean(axis=0)\n",
    "    offset = best - theta_true                             # [D]\n",
    "    for j in range(D):\n",
    "        best_guess_offsets_by_param[j].append(float(offset[j]))\n",
    "\n",
    "    # 1D KL per parameter (posterior vs Uniform[0,1])\n",
    "    for j in range(D):\n",
    "        kl_list_by_param[j].append(kl_to_uniform_1d(post[:, j], M=256))\n",
    "\n",
    "# convert lists to arrays\n",
    "for j in range(D):\n",
    "    best_guess_offsets_by_param[j] = np.asarray(best_guess_offsets_by_param[j], dtype=float)\n",
    "    kl_list_by_param[j] = np.asarray(kl_list_by_param[j], dtype=float)\n",
    "\n",
    "# ---- Shrinkage metrics per parameter (computed across ALL held-out sims) ----\n",
    "# Prior on [0,1]: prior variance = 1/12, prior 90% ETI width = 0.90 (equal-tailed)\n",
    "PRIOR_VAR = 1.0 / 12.0\n",
    "PRIOR_CI90_WIDTH = 0.90\n",
    "EPS = 1e-12\n",
    "\n",
    "ci90_shrink_mean = np.zeros(D, dtype=float)\n",
    "ci90_shrink_sd   = np.zeros(D, dtype=float)\n",
    "var_shrink_mean  = np.zeros(D, dtype=float)\n",
    "var_shrink_sd    = np.zeros(D, dtype=float)\n",
    "\n",
    "# also convert KL nats -> bits for display\n",
    "kl_bits_list_by_param = [ arr / np.log(2.0) for arr in kl_list_by_param ]\n",
    "\n",
    "for j in range(D):\n",
    "    # gather posterior samples per sim for param j (ALL sims for robustness)\n",
    "    widths = []\n",
    "    vsh    = []\n",
    "    for sid in all_ids:\n",
    "        post = to_np(posterior_samples_thetas_for_held_out_sims[sid])[:, j]  # [n_draws]\n",
    "        # 90% equal-tailed credible interval width\n",
    "        lo, hi = np.quantile(post, [0.05, 0.95])\n",
    "        width = max(hi - lo, EPS)\n",
    "        widths.append(width)\n",
    "        # variance shrinkage\n",
    "        v = float(np.var(post, ddof=0))\n",
    "        v = max(v, EPS)\n",
    "        vsh.append(PRIOR_VAR / v)\n",
    "\n",
    "    widths = np.asarray(widths, dtype=float)\n",
    "    vsh    = np.asarray(vsh, dtype=float)\n",
    "\n",
    "    # \"shrinkage\" = prior width / posterior width (prior width = 0.90 on [0,1])\n",
    "    ci_sh = PRIOR_CI90_WIDTH / widths\n",
    "    ci90_shrink_mean[j] = float(np.mean(ci_sh))\n",
    "    ci90_shrink_sd[j]   = float(np.std(ci_sh, ddof=0))\n",
    "\n",
    "    var_shrink_mean[j]  = float(np.mean(vsh))\n",
    "    var_shrink_sd[j]    = float(np.std(vsh, ddof=0))\n",
    "\n",
    "\n",
    "# ---- Build common x-grids for plotting & precompute curves with progress bars ----\n",
    "if PLOT_KIND == 'hist':\n",
    "    edges = np.linspace(XLIM[0], XLIM[1], NBINS + 1)\n",
    "    xgrid = edges[:-1]\n",
    "else:\n",
    "    edges = None\n",
    "    xgrid = np.linspace(XLIM[0], XLIM[1], KDE_POINTS)\n",
    "\n",
    "# Caches of curves for display subset\n",
    "curves_prior  = [[] for _ in range(D)]   # each: list of y arrays (one per sim)\n",
    "curves_post   = [[] for _ in range(D)]\n",
    "# Aggregate bands (mean ± SD across sims' curves)\n",
    "band_prior    = [None for _ in range(D)] # dict with 'mean','std'\n",
    "band_post     = [None for _ in range(D)]\n",
    "# Best-guess density curve (one per param)\n",
    "best_guess_curve = [None for _ in range(D)]\n",
    "\n",
    "print(\"Computing density curves (display subset)…\")\n",
    "for j in range(D):\n",
    "    for sid in tqdm(sel_ids, desc=f\"[{param_names[j]}] prior curves\", leave=False):\n",
    "        theta_true = theta_true_by_sim_display[sid]\n",
    "        if PLOT_KIND == 'hist':\n",
    "            y = flat_prior_curve_centered(theta_true[j], xgrid, edges)   # step-ready\n",
    "        else:\n",
    "            y = flat_prior_curve_centered(theta_true[j], xgrid, None)    # line-ready\n",
    "        curves_prior[j].append(y)\n",
    "    for arr in tqdm(posterior_centered_disp[j], desc=f\"[{param_names[j]}] post curves\", leave=False):\n",
    "        y = hist_curve(arr, edges) if PLOT_KIND == 'hist' else kde_curve(arr, xgrid, KDE_BW)\n",
    "        curves_post[j].append(y)\n",
    "\n",
    "print(\"Computing aggregate mean ± SD bands & best-guess curves…\")\n",
    "for j in range(D):\n",
    "\n",
    "    if AGGREGATE_ON_ALL:\n",
    "        sim_ids_for_band = all_ids\n",
    "    else:\n",
    "        sim_ids_for_band = sel_ids\n",
    "\n",
    "    Ys_prior = []\n",
    "    for sid in tqdm(sim_ids_for_band, desc=f\"[{param_names[j]}] band prior\", leave=False):\n",
    "        theta_true = (theta_true_by_sim_all if AGGREGATE_ON_ALL else theta_true_by_sim_display)[sid]\n",
    "        if PLOT_KIND == 'hist':\n",
    "            y = flat_prior_curve_centered(theta_true[j], xgrid, edges)\n",
    "        else:\n",
    "            y = flat_prior_curve_centered(theta_true[j], xgrid, None)\n",
    "        Ys_prior.append(y)\n",
    "\n",
    "    Ys_post = []\n",
    "    for arr in tqdm((posterior_centered_all[j] if AGGREGATE_ON_ALL else posterior_centered_disp[j]),\n",
    "                    desc=f\"[{param_names[j]}] band post\", leave=False):\n",
    "        Ys_post.append(hist_curve(arr, edges) if PLOT_KIND == 'hist' else kde_curve(arr, xgrid, KDE_BW))\n",
    "\n",
    "    # Use nan-aggregates for prior band (since we inserted NaNs outside support)\n",
    "    if len(Ys_prior) > 0:\n",
    "        YP = np.vstack(Ys_prior)\n",
    "        band_prior[j] = dict(mean=np.nanmean(YP, axis=0), std=np.nanstd(YP, axis=0))\n",
    "    else:\n",
    "        band_prior[j] = dict(mean=None, std=None)\n",
    "\n",
    "    if len(Ys_post) > 0:\n",
    "        YQ = np.vstack(Ys_post)\n",
    "        mu_post = YQ.mean(axis=0)\n",
    "        sd_post = YQ.std(axis=0, ddof=0)\n",
    "        band_post[j] = dict(mean=mu_post, std=sd_post)\n",
    "    else:\n",
    "        band_post[j] = dict(mean=None, std=None)\n",
    "\n",
    "    # best-guess offsets density (red curve)\n",
    "    if SHOW_BEST_GUESS_CURVE and best_guess_offsets_by_param[j].size > 0:\n",
    "        offsets = np.clip(best_guess_offsets_by_param[j], XLIM[0], XLIM[1])\n",
    "        if PLOT_KIND == 'hist':\n",
    "            best_guess_curve[j] = hist_curve(offsets, edges)\n",
    "        else:\n",
    "            best_guess_curve[j] = kde_curve(offsets, xgrid, KDE_BW)\n",
    "\n",
    "# ---- Global y-limits across ALL panels (include red curves & bands) ----\n",
    "ymax = 0.0\n",
    "for j in range(D):\n",
    "    for y in curves_prior[j]:  ymax = max(ymax, float(np.nanmax(y)))\n",
    "    for y in curves_post[j]:   ymax = max(ymax, float(np.nanmax(y)))\n",
    "    if band_prior[j]['mean'] is not None:\n",
    "        ymax = max(ymax, float(np.nanmax(band_prior[j]['mean'] + band_prior[j]['std'])))\n",
    "    if band_post[j]['mean'] is not None:\n",
    "        ymax = max(ymax, float(np.nanmax(band_post[j]['mean'] + band_post[j]['std'])))\n",
    "    if best_guess_curve[j] is not None:\n",
    "        ymax = max(ymax, float(np.nanmax(best_guess_curve[j])))\n",
    "ymax *= 1.05\n",
    "ylims = (0.0, ymax if np.isfinite(ymax) and ymax > 0 else 1.0)\n",
    "\n",
    "# ---- Plot ----\n",
    "fig, axes = plt.subplots(D, 2, figsize=FIGSIZE, sharex=True, sharey=True)\n",
    "if D == 1: axes = np.array([axes])\n",
    "\n",
    "for j in range(D):\n",
    "    # LEFT: PRIOR\n",
    "    axL = axes[j, 0]\n",
    "    for y in curves_prior[j]:\n",
    "        if PLOT_KIND == 'hist' or PLOT_KIND == 'kde': # always hist for prior\n",
    "            axL.step(xgrid, y, where='post', alpha=ALPHA_LINE, lw=LINEWIDTH_THIN, color='grey')\n",
    "        else:\n",
    "            axL.plot(xgrid, y, alpha=ALPHA_LINE, lw=LINEWIDTH_THIN, color='grey')\n",
    "    if band_prior[j]['mean'] is not None:\n",
    "        mu = band_prior[j]['mean']; sd = band_prior[j]['std']\n",
    "        if PLOT_KIND == 'hist' or PLOT_KIND == 'kde': # always hist for prior\n",
    "            axL.step(xgrid, mu, where='post', lw=LINEWIDTH_THICK, color='grey', alpha=0.8, label='Mean density')\n",
    "            axL.fill_between(xgrid, np.clip(mu - sd, 0, None), mu + sd, step='post',\n",
    "                             color='grey', alpha=0.12, label='±1 SD (across sims)')\n",
    "        else:\n",
    "            axL.plot(xgrid, mu, lw=LINEWIDTH_THICK, color='grey', alpha=0.8, label='Mean density')\n",
    "            axL.fill_between(xgrid, np.clip(mu - sd, 0, None), mu + sd,\n",
    "                             color='grey', alpha=0.12, label='±1 SD (across sims)')\n",
    "    axL.axvline(0.0, ls=':', lw=1.2, color='k')\n",
    "    axL.set_ylabel(param_names[j])\n",
    "    axL.set_xlim(*XLIM); axL.set_ylim(*ylims)\n",
    "    if j == 0:\n",
    "        axL.set_title(\"Prior (centered)\")\n",
    "        axL.legend(loc='upper right', fontsize=8, frameon=False)\n",
    "\n",
    "    # RIGHT: POSTERIOR\n",
    "    axR = axes[j, 1]\n",
    "    for y in curves_post[j]:\n",
    "        if PLOT_KIND == 'hist':\n",
    "            axR.step(xgrid, y, where='post', alpha=ALPHA_LINE, lw=LINEWIDTH_THIN, color='purple')\n",
    "        else:\n",
    "            axR.plot(xgrid, y, alpha=ALPHA_LINE, lw=LINEWIDTH_THIN, color='purple')\n",
    "    if band_post[j]['mean'] is not None:\n",
    "        mu = band_post[j]['mean']; sd = band_post[j]['std']\n",
    "        if PLOT_KIND == 'hist':\n",
    "            axR.step(xgrid, mu, where='post', lw=LINEWIDTH_THICK, color='purple', alpha=0.9, label='Mean density')\n",
    "            axR.fill_between(xgrid, np.clip(mu - sd, 0, None), mu + sd, step='post',\n",
    "                             color='purple', alpha=0.2, label='±1 SD (across sims)')\n",
    "        else:\n",
    "            axR.plot(xgrid, mu, lw=LINEWIDTH_THICK, color='purple', alpha=0.9, label='Mean density')\n",
    "            axR.fill_between(xgrid, np.clip(mu - sd, 0, None), mu + sd,\n",
    "                             color='purple', alpha=0.2, label='±1 SD (across sims)')\n",
    "\n",
    "    # RED: best-guess offsets density\n",
    "    if SHOW_BEST_GUESS_CURVE and best_guess_curve[j] is not None:\n",
    "        if PLOT_KIND == 'hist':\n",
    "            axR.step(xgrid, best_guess_curve[j], where='post', lw=LINEWIDTH_THICK, color='red', alpha=0.6, label='Best guess (distribution of difference relative to ground truth)')\n",
    "        else:\n",
    "            axR.plot(xgrid, best_guess_curve[j], lw=LINEWIDTH_THICK, color='red', alpha=0.6, label=f'Best guess\\n(distribution of difference\\nrelative to ground truth)')\n",
    "\n",
    "    axR.axvline(0.0, ls=':', lw=1.2, color='k')\n",
    "    axR.set_xlim(*XLIM); axR.set_ylim(*ylims)\n",
    "    if j == 0:\n",
    "        axR.set_title(\"Posterior (centered)\")\n",
    "        axR.legend(loc='upper right', fontsize=8, frameon=False)\n",
    "\n",
    "    if SHOW_KL_TEXT:\n",
    "        # KL: use bits for display\n",
    "        kl_bits = kl_bits_list_by_param[j]\n",
    "        kl_mean_bits = float(kl_bits.mean()) if kl_bits.size else np.nan\n",
    "        kl_sd_bits   = float(kl_bits.std(ddof=0)) if kl_bits.size else np.nan\n",
    "\n",
    "        # Add shrinkage summaries (mean ± SD across sims)\n",
    "        ci_mu, ci_sd = ci90_shrink_mean[j], ci90_shrink_sd[j]\n",
    "        vs_mu, vs_sd = var_shrink_mean[j],  var_shrink_sd[j]\n",
    "\n",
    "        axR.text(0.02, 0.95,\n",
    "                \"Info gain (from uniform to marginal posterior):\\n\"\n",
    "                f\"  KL divergence between uniform[0,1] and posterior\\n  (entropy of posterior)\\n  = {kl_mean_bits:.3f} ± {kl_sd_bits:.3f} bits\\n\"\n",
    "                f\" \\nShrinkage (X-fold decrease from prior to posterior):\\n\"\n",
    "                f\"  90% confidence interval  = {ci_mu:.2f} ×  ± {ci_sd:.2f}\\n\"\n",
    "                f\"  Variance   = {vs_mu:.2f} ×  ± {vs_sd:.2f}\",\n",
    "                transform=axR.transAxes, va='top', ha='left',\n",
    "                color='black', fontsize=8,\n",
    "                bbox=dict(facecolor='white', alpha=0.6, edgecolor=None))\n",
    "\n",
    "\n",
    "for ax in axes[-1, :]:\n",
    "    ax.set_xlabel(\"Centered value (θ − θ_true)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVEFIG_PATH:\n",
    "    plt.savefig(f\"{SAVEFIG_PATH}\\\\ Accuracy_posterior\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dad6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE CHECK # 2.1\n",
    "# Comparing posterior distribution to ground truth, this time for each held out simulation (distribution density as color scale on the y axis, ground-truth as black line)\n",
    "\n",
    "# ============================================================\n",
    "# Posterior density maps (light theme + custom white→color cmap)\n",
    "#  - Per-parameter 2D density (columns = held-out sims, y∈[0,1])\n",
    "#  - Ground-truth line overlay\n",
    "#  - Best-guess accuracy metrics (MAE/RMSE/R²) + expected baselines (Uniform[0,1])\n",
    "#  - Option to SHARE the density color scale across all subplots\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_rgb\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "NBINS_Y         = 100              # vertical bins (resolution in y)\n",
    "BASE_COLOR      = \"purple\" # \"#2a6f97\"        # choose any color; 0-density is white → high density = this color\n",
    "SHARE_VSCALE    = True             # if True, all subplots share the same color scale (comparable intensities)\n",
    "VSCALE_QUANTILE = 0.999            # when sharing, take this global quantile as vmax (robust to outliers); 1.0 = max\n",
    "FIG_W           = 10\n",
    "FIG_H_PER_ROW   = 4\n",
    "COLORBAR_MODE   = \"shared\"         # \"shared\" (one colorbar for all) or \"per-axes\"\n",
    "SAVEFIG         = path_to_save  # or None\n",
    "\n",
    "# Pick held-out index\n",
    "HELDOUT_PICK = 19 # Choose the held out simulation to highlight\n",
    "heldout_idx = int(hold_out_idx[HELDOUT_PICK] if isinstance(HELDOUT_PICK, int) else HELDOUT_PICK)\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "def _to_np(x):\n",
    "    \"\"\"Convert torch.Tensor or array-like to a NumPy array without using tensor.numpy().\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        # Go through Python lists → NumPy, avoids PyTorch's NumPy bridge entirely\n",
    "        return np.asarray(x.detach().cpu().tolist(), dtype=float)\n",
    "    else:\n",
    "        return np.asarray(x, dtype=float)\n",
    "\n",
    "# Light-theme matplotlib defaults\n",
    "plt.rcParams.update({\n",
    "    \"axes.facecolor\": \"white\",\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"savefig.facecolor\": \"white\",\n",
    "    \"text.color\": \"black\",\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.labelcolor\": \"black\",\n",
    "    \"xtick.color\": \"black\",\n",
    "    \"ytick.color\": \"black\",\n",
    "})\n",
    "\n",
    "# Custom colormap: white → BASE_COLOR (linear)\n",
    "def white_to_color_cmap(hex_color=\"#2a6f97\", steps=256):\n",
    "    rgb = to_rgb(hex_color)\n",
    "    return LinearSegmentedColormap.from_list(\"white_to_color\", [(1,1,1), rgb], N=steps)\n",
    "\n",
    "CMAP = white_to_color_cmap(BASE_COLOR, 256)\n",
    "\n",
    "# Resolve names and held-out IDs\n",
    "param_names = input_sim_parameters_to_infer if 'input_sim_parameters_to_infer' in globals() else \\\n",
    "              [f\"θ[{j}]\" for j in range(next(iter(posterior_samples_thetas_for_held_out_sims.values())).shape[1])]\n",
    "heldout_ids = sorted(int(k) for k in posterior_samples_thetas_for_held_out_sims.keys())\n",
    "N = len(heldout_ids)\n",
    "D = len(param_names)\n",
    "assert N > 0 and D > 0, \"No held-out sims or parameters found.\"\n",
    "\n",
    "# Collect GT, best-guess, and posterior samples\n",
    "GT = np.zeros((N, D), dtype=np.float32)\n",
    "BEST_GUESS = np.zeros((N, D), dtype=np.float32)\n",
    "SAMPLES_PER_SIM = []\n",
    "\n",
    "for i, sid in enumerate(heldout_ids):\n",
    "    theta_true = _to_np(ground_truth_thetas_for_held_out_sims[sid]).reshape(-1)\n",
    "    post = _to_np(posterior_samples_thetas_for_held_out_sims[sid])  # [n_draws, D]\n",
    "    GT[i, :] = theta_true\n",
    "    if sid in posterior_log_probs_for_held_out_sims:\n",
    "        logp = _to_np(posterior_log_probs_for_held_out_sims[sid]).reshape(-1)\n",
    "        idx  = int(np.argmax(logp))\n",
    "        BEST_GUESS[i, :] = post[idx, :]\n",
    "    else:\n",
    "        BEST_GUESS[i, :] = post.mean(axis=0)\n",
    "    SAMPLES_PER_SIM.append(post)\n",
    "\n",
    "# Metric helpers\n",
    "def mae(a, b):  return float(np.mean(np.abs(a - b)))\n",
    "def rmse(a, b): return float(np.sqrt(np.mean((a - b)**2)))\n",
    "def r2(pred, true):\n",
    "    ss_res = np.sum((pred - true)**2)\n",
    "    ss_tot = np.sum((true - true.mean())**2)\n",
    "    return float(1.0 - ss_res/ss_tot) if ss_tot > 0 else np.nan\n",
    "\n",
    "# Precompute images (so we can share color scale if requested)\n",
    "y_edges = np.linspace(0.0, 1.0, NBINS_Y + 1)\n",
    "images_per_param = []   # list of [NBINS_Y, N]\n",
    "gt_sorted_per_param = []  # x alignments\n",
    "\n",
    "for j, pname in enumerate(param_names):\n",
    "    gt_j = GT[:, j]\n",
    "    order = np.argsort(gt_j)\n",
    "    gt_sorted = gt_j[order]\n",
    "    img = np.zeros((NBINS_Y, N), dtype=np.float32)\n",
    "    for col, idx in enumerate(tqdm(order, desc=f\"Building density for {pname}\", leave=False)):\n",
    "        samples = SAMPLES_PER_SIM[idx][:, j]\n",
    "        samples = np.clip(samples, 0.0, 1.0)\n",
    "        counts, _ = np.histogram(samples, bins=y_edges)\n",
    "        col_pdf = counts.astype(np.float32)\n",
    "        Z = col_pdf.sum()\n",
    "        if Z > 0:\n",
    "            col_pdf /= Z  # column-wise PDF\n",
    "        img[:, col] = col_pdf\n",
    "    images_per_param.append(img)\n",
    "    gt_sorted_per_param.append(gt_sorted)\n",
    "\n",
    "# Choose shared vmin/vmax\n",
    "if SHARE_VSCALE:\n",
    "    all_vals = np.concatenate([img.ravel() for img in images_per_param])\n",
    "    vmin = 0.0\n",
    "    vmax = float(np.quantile(all_vals, VSCALE_QUANTILE)) if VSCALE_QUANTILE < 1.0 else float(all_vals.max())\n",
    "    if vmax <= 0:\n",
    "        vmax = 1.0\n",
    "else:\n",
    "    vmin, vmax = 0.0, None  # autoscale per-axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE CHECK # 2.2\n",
    "# Comparing posterior distribution to ground truth, this time for each held out simulation (distribution density as color scale on the y axis, ground-truth as black line)\n",
    "\n",
    "# ---- Plot (with markers for HELDOUT_PICK) ----\n",
    "fig, axes = plt.subplots(D, 1, figsize=(FIG_W, max(3.0, D*FIG_H_PER_ROW)), squeeze=False)\n",
    "mappables = []\n",
    "\n",
    "# map heldout_idx (sim id) -> its row index i_pick in arrays\n",
    "try:\n",
    "    i_pick = heldout_ids.index(heldout_idx)\n",
    "except ValueError:\n",
    "    i_pick = None\n",
    "    print(f\"Warning: heldout_idx={heldout_idx} not found in heldout_ids; markers will be skipped.\")\n",
    "\n",
    "for j, pname in enumerate(param_names):\n",
    "    ax = axes[j, 0]\n",
    "    img = images_per_param[j]\n",
    "\n",
    "    im = ax.imshow(\n",
    "        img, aspect='auto', origin='lower',\n",
    "        extent=[0, N-1, 0.0, 1.0],\n",
    "        cmap=CMAP, vmin=vmin, vmax=vmax,\n",
    "        interpolation=None,\n",
    "    )\n",
    "    mappables.append(im)\n",
    "\n",
    "    # Ground-truth line (y = sorted GT)\n",
    "    gt_j = GT[:, j]\n",
    "    order_j = np.argsort(gt_j)\n",
    "    gt_sorted = gt_j[order_j]\n",
    "    ax.plot(np.arange(N), gt_sorted, color='black', lw=1.4, alpha=0.9, label=\"Ground truth\")\n",
    "\n",
    "    # Marker for the chosen held-out sim (column depends on sorting for THIS parameter)\n",
    "    if i_pick is not None:\n",
    "        # column where this sim lands after sorting by parameter j\n",
    "        col_pos = int(np.where(order_j == i_pick)[0][0])\n",
    "        y_val = gt_sorted[col_pos]\n",
    "        ax.scatter(\n",
    "            [col_pos], [y_val],\n",
    "            s=65, marker='D',\n",
    "            facecolor='none', edgecolor='black', linewidth=1.2, zorder=5,\n",
    "            label=None\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0, N-1); ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_ylabel(pname)\n",
    "    if j == 0:\n",
    "        ax.set_title(\"Posterior density across held-out simulations (light theme; white→low, color→high)\")\n",
    "\n",
    "    # Best-guess metrics vs unsorted GT (metrics independent of sort)\n",
    "    mae_best  = float(np.mean(np.abs(BEST_GUESS[:, j] - GT[:, j])))\n",
    "    rmse_best = float(np.sqrt(np.mean((BEST_GUESS[:, j] - GT[:, j])**2)))\n",
    "    ss_res = np.sum((BEST_GUESS[:, j] - GT[:, j])**2)\n",
    "    ss_tot = np.sum((GT[:, j] - GT[:, j].mean())**2)\n",
    "    r2_best  = float(1.0 - ss_res/ss_tot) if ss_tot > 0 else np.nan\n",
    "    # Pearson correlation between posterior mode and ground truth\n",
    "    if GT[:, j].std(ddof=0) > 0 and BEST_GUESS[:, j].std(ddof=0) > 0 and GT.shape[0] > 1:\n",
    "        pear_r, _ = pearsonr(BEST_GUESS[:, j], GT[:, j])\n",
    "        pear_r = float(pear_r)\n",
    "    else:\n",
    "        pear_r = np.nan\n",
    "\n",
    "\n",
    "    # Expected baselines under Uniform[0,1]\n",
    "    a = GT[:, j]\n",
    "    E_abs = a**2 - a + 0.5                         # E|U-a|\n",
    "    E_sq  = (0.5 - a)**2 + (1.0/12.0)              # E[(U-a)^2]\n",
    "    exp_mae_prior  = float(np.mean(E_abs))\n",
    "    exp_rmse_prior = float(np.sqrt(np.mean(E_sq)))\n",
    "    muY, varY = float(a.mean()), float(a.var(ddof=0))\n",
    "    exp_r2_prior = (1.0 - (varY + 1.0/12.0 + (muY - 0.5)**2) / varY) if varY > 0 else np.nan\n",
    "\n",
    "    txt = (f\"Best-guess vs Ground Truth:\\n\"\n",
    "           f\"  Mean Abs. Error  = {mae_best:.3f}\\n\"\n",
    "           f\"  RMSE = {rmse_best:.3f}\\n\"\n",
    "           f\"  R²   = {r2_best:.3f}\\n\"\n",
    "           f\"  Pearson r        = {pear_r:.3f}\\n\"\n",
    "           f\"Baseline (Uniform prior, expected):\\n\"\n",
    "           f\"  E[MAE]  = {exp_mae_prior:.3f}\\n\"\n",
    "           f\"  E[RMSE] = {exp_rmse_prior:.3f}\\n\"\n",
    "           f\"  E[R²]   = {exp_r2_prior:.3f}\")\n",
    "    ax.text(0.01, 0.99, txt, transform=ax.transAxes, va='top', ha='left',\n",
    "            fontsize=8, color='black',\n",
    "            bbox=dict(facecolor='white', alpha=0.75, edgecolor='none'))\n",
    "\n",
    "    if j == 0:\n",
    "        ax.legend(loc='lower right', fontsize=8, frameon=False)\n",
    "\n",
    "ax.set_xlabel(\"Held-out simulations (sorted by ground truth per row)\")\n",
    "\n",
    "# Colorbar(s)\n",
    "if COLORBAR_MODE == \"shared\":\n",
    "    cbar = fig.colorbar(mappables[-1], ax=axes.ravel().tolist(), fraction=0.035, pad=0.02)\n",
    "    cbar.set_label(\"Column PDF (density)\")\n",
    "else:\n",
    "    for ax, im in zip(axes.ravel(), mappables):\n",
    "        cbar = fig.colorbar(im, ax=ax, fraction=0.035, pad=0.02)\n",
    "        cbar.set_label(\"Column PDF\")\n",
    "\n",
    "if SAVEFIG:\n",
    "    plt.savefig(f\"{SAVEFIG}\\\\Accuracy_posterior_density_light.png\", dpi=300)\n",
    "    plt.savefig(f\"{SAVEFIG}\\\\Accuracy_posterior_density_light.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE CHECK # 2.3\n",
    "# Comparing posterior distribution to ground truth, specifically at the highlighted heldout idx (corresponds to a \"slice\" of the plots generated in FOGURE CHECK # 2.2)\n",
    "\n",
    "# ===========================================\n",
    "# “Slice” figure for HELDOUT_PICK\n",
    "#  - One panel per parameter\n",
    "#  - KDE of posterior samples for that sim\n",
    "#  - Vertical lines: GT (black solid), Best-guess (colored dashed)\n",
    "# ===========================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgb\n",
    "\n",
    "SLICE_BASE_COLOR = BASE_COLOR    # reuse color from above for consistency\n",
    "SLICE_BW         = 0.04          # KDE bandwidth in parameter units [0,1]\n",
    "SLICE_POINTS     = 401           # grid resolution\n",
    "SLICE_XLIM       = (0.0, 1.0)    # parameters are normalized to [0,1]\n",
    "FIGSIZE_SLICE    = (10, max(3.0, 2.2*D))\n",
    "\n",
    "def kde_1d(samples, xgrid, bw):\n",
    "    x = np.clip(np.asarray(samples, float), SLICE_XLIM[0], SLICE_XLIM[1])\n",
    "    if x.size == 0:\n",
    "        return np.zeros_like(xgrid)\n",
    "    inv = 1.0 / np.sqrt(2*np.pi*bw*bw)\n",
    "    dif = xgrid[None, :] - x[:, None]\n",
    "    y = inv * np.exp(-0.5 * (dif / bw)**2)\n",
    "    # normalize to integrate to ~1 over the grid\n",
    "    y_mean = y.mean(axis=0)\n",
    "    dx = np.diff(xgrid).mean()\n",
    "    area = (y_mean * dx).sum()\n",
    "    return (y_mean / area) if area > 0 else y_mean\n",
    "\n",
    "# Map heldout_idx to its array row index i_pick (if not done already)\n",
    "try:\n",
    "    i_pick = heldout_ids.index(heldout_idx)\n",
    "except ValueError:\n",
    "    raise RuntimeError(f\"heldout_idx={heldout_idx} not found in heldout_ids.\")\n",
    "\n",
    "# Build figure\n",
    "xgrid = np.linspace(SLICE_XLIM[0], SLICE_XLIM[1], SLICE_POINTS)\n",
    "fig, axes = plt.subplots(D, 1, figsize=FIGSIZE_SLICE, sharex=True)\n",
    "\n",
    "if D == 1:\n",
    "    axes = np.array([axes])\n",
    "\n",
    "curve_color = SLICE_BASE_COLOR\n",
    "for j, pname in enumerate(param_names):\n",
    "    ax = axes[j]\n",
    "\n",
    "    # Posterior samples for this sim & parameter\n",
    "    samples_j = SAMPLES_PER_SIM[i_pick][:, j]\n",
    "    dens = kde_1d(samples_j, xgrid, SLICE_BW)\n",
    "\n",
    "    # Lines\n",
    "    ax.plot(xgrid, dens, color=curve_color, lw=2.0, alpha=0.95, label=\"Posterior (KDE)\")\n",
    "    ax.axvline(GT[i_pick, j], color='black', lw=1.5, label=\"Ground truth\")\n",
    "    ax.axvline(BEST_GUESS[i_pick, j], color=curve_color, lw=1.5, ls='--', label=\"Best guess\")\n",
    "\n",
    "    ax.set_ylabel(pname)\n",
    "    ax.set_xlim(*SLICE_XLIM)\n",
    "    # pretty y-limit with a bit of headroom\n",
    "    ymax = float(dens.max()) if np.isfinite(dens).all() else 1.0\n",
    "    ax.set_ylim(0, ymax * 1.08)\n",
    "\n",
    "axes[-1].set_xlabel(\"Parameter value (normalized)\")\n",
    "\n",
    "# Top legend (single, clean)\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=\"upper right\", frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVEFIG:\n",
    "    plt.savefig(f\"{SAVEFIG}\\\\Accuracy_slice_KDE.png\", dpi=300)\n",
    "    plt.savefig(f\"{SAVEFIG}\\\\Accuracy_slice_KDE.svg\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE CHECK # 3\n",
    "# Comparing posterior distribution to ground truth in feature-space (and comparing it to prior), with different projection styles\n",
    "\n",
    "# %% SBI held-out check — projection (with full D axes) + 2D scatter (fixed contours)\n",
    "\n",
    "# ------------------- TUNING KNOBS -------------------\n",
    "HELDOUT_PICK = 19 # Held out idx to look at\n",
    "# Normalization for projection & scatter: \"original\", \"unit\", or \"zscore\"\n",
    "proj_normalization    = \"unit\"\n",
    "scatter_normalization = \"unit\"\n",
    "\n",
    "prior_color = \"#888888\"\n",
    "heldout_condition_color = \"#C2388D\"\n",
    "\n",
    "# --- Figure 1 (D->2 arrow projection, show ALL basis axes) ---\n",
    "PROJ_MODE    = \"manual\"                 # \"mix2\" | \"random\" | \"axes\" | \"manual\"\n",
    "# Plane definition if PROJ_MODE == \"mix2\":\n",
    "# u_raw = cos(θ) e_i + sin(θ) e_j  ; v_raw = cos(φ) e_k + sin(φ) e_l\n",
    "MIX2_U = (0, 1, 35.0)                 # (i, j, angle_deg)\n",
    "MIX2_V = (0, 2, -20.0)                # (k, l, angle_deg)\n",
    "# Fallbacks for other modes\n",
    "ORIENT_SEED    = 7\n",
    "ORIENT_AXES    = (0, 1)               # for PROJ_MODE==\"axes\"\n",
    "# ORIENT_B_MANUAL = None                # np.ndarray shape (D,2) if PROJ_MODE==\"manual\"\n",
    "ORIENT_B_MANUAL = np.array([\n",
    "    [ 1.00,  0.00],   # θ1\n",
    "    [ 0.31,  0.95],   # θ2\n",
    "    [-0.81,  0.59],   # θ3\n",
    "    [ 0.62, -0.78],   # θ4\n",
    "    [-0.96,  0.28],   # θ5\n",
    "])\n",
    "\n",
    "# How many arrows to show (subset to avoid clutter)\n",
    "n_post_proj  = 20\n",
    "n_prior_proj = 50\n",
    "\n",
    "# Arrow styling\n",
    "arrow_lw_gt       = 3.0\n",
    "arrow_lw_post     = 2\n",
    "arrow_lw_prior    = 2\n",
    "arrow_alpha_gt    = 1.0\n",
    "arrow_alpha_post  = 0.2\n",
    "arrow_alpha_prior = 0.2\n",
    "# Small arrowheads (relative — the plane is unit-ish)\n",
    "arrow_head_width  = 0.02\n",
    "arrow_head_length = 0.03\n",
    "\n",
    "# Basis (parameter) axis arrows (projected) — black\n",
    "basis_axis_color = \"#000000\"\n",
    "basis_axis_lw    = 1.4\n",
    "basis_axis_alpha = 0.8\n",
    "basis_head_w     = 0.018\n",
    "basis_head_l     = 0.035\n",
    "\n",
    "# Limits padding for projection figure\n",
    "proj_axis_pad_frac = 0.06\n",
    "label_params_on_axes = True           # show θ_i labels near arrow tips\n",
    "\n",
    "# --- Figure 2 (2D scatter) ---\n",
    "param_x = \"common_input_std\"\n",
    "param_y = \"disynpatic_inhib_connections_desired_MN_MN\"\n",
    "n_post_scatter  = 2500\n",
    "n_prior_scatter = 11500\n",
    "dot_size_prior  = 30\n",
    "dot_alpha_prior = 0.05\n",
    "dot_size_post   = 30\n",
    "dot_alpha_post  = 0.1\n",
    "cross_size      = 130\n",
    "cross_edge_lw   = 1.2\n",
    "scatter_axis_pad_frac = 0.08\n",
    "\n",
    "# 2D KDE (posterior) — mass-contour lines 30/60/90%\n",
    "kde_levels_mass = (0.3, 0.6, 0.9)\n",
    "kde_grid        = 200\n",
    "kde_lw          = 1.4\n",
    "kde_color       = \"#000000\"\n",
    "kde_bw_scale    = None                # None→Scott; or float\n",
    "\n",
    "# Save paths\n",
    "path_to_save = globals().get(\"path_to_save\", \".\")\n",
    "os.makedirs(path_to_save, exist_ok=True)\n",
    "FIG1_PATH = os.path.join(path_to_save, \"SBI_check_projection_theta.svg\")\n",
    "FIG2_PATH = os.path.join(path_to_save, \"SBI_check_scatter_theta.svg\")\n",
    "\n",
    "# Helper to avoid numpy arrays (not supported by new Pytorch versions)\n",
    "def to_np(x):\n",
    "    \"\"\"Convert torch.Tensor or array-like to a NumPy array without using tensor.numpy().\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return np.asarray(x.detach().cpu().tolist(), dtype=float)\n",
    "    else:\n",
    "        return np.asarray(x, dtype=float)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Context (same as before)\n",
    "theta_colnames = list(globals()[\"theta_colnames\"])\n",
    "D = len(theta_colnames)\n",
    "if \"theta_raw\" not in globals():\n",
    "    theta_raw = torch.tensor(\n",
    "        df_simulation_summary[theta_colnames].to_numpy(dtype=float).tolist(),\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "# Prior bounds (original scale)\n",
    "if \"low_original\" not in globals() or \"high_original\" not in globals():\n",
    "    low_original  = torch.tensor([priors_per_parameters_to_infer[name][0] for name in theta_colnames], dtype=torch.float32)\n",
    "    high_original = torch.tensor([priors_per_parameters_to_infer[name][1] for name in theta_colnames], dtype=torch.float32)\n",
    "\n",
    "def theta_to_unit(theta):\n",
    "    return (theta - low_original) / (high_original - low_original)\n",
    "\n",
    "def unit_to_theta(theta_unit):\n",
    "    return theta_unit * (high_original - low_original) + low_original\n",
    "\n",
    "theta_mu = theta_raw.mean(dim=0)\n",
    "theta_sd = theta_raw.std(dim=0, unbiased=False).clamp_min(1e-12)\n",
    "\n",
    "# Pick held-out index\n",
    "heldout_idx = int(hold_out_idx[HELDOUT_PICK] if isinstance(HELDOUT_PICK, int) else HELDOUT_PICK)\n",
    "\n",
    "# Observation vector for this held-out\n",
    "x_obs = obs_hold_out[HELDOUT_PICK] # sim_obs[heldout_idx]\n",
    "\n",
    "# Ground truth θ\n",
    "theta_gt_original = theta_raw[heldout_idx]\n",
    "theta_gt_unit     = theta_to_unit(theta_gt_original)\n",
    "\n",
    "# Posterior samples (unit)\n",
    "N_POST = int(max(n_post_proj, n_post_scatter))\n",
    "with torch.no_grad():\n",
    "    post_samples_unit = posterior_net_held_out_data.sample((N_POST,), x=x_obs)\n",
    "\n",
    "# Subsets\n",
    "rng = np.random.default_rng(123)\n",
    "idx_post_proj   = rng.choice(post_samples_unit.shape[0], size=min(n_post_proj, post_samples_unit.shape[0]), replace=False)\n",
    "idx_post_scatt  = rng.choice(post_samples_unit.shape[0], size=min(n_post_scatter, post_samples_unit.shape[0]), replace=False)\n",
    "\n",
    "N_PRIOR_PROJ   = min(n_prior_proj,   len(df_simulation_summary))\n",
    "N_PRIOR_SCATT  = min(n_prior_scatter, len(df_simulation_summary))\n",
    "idx_prior_proj  = rng.choice(len(df_simulation_summary), size=N_PRIOR_PROJ,  replace=False)\n",
    "idx_prior_scatt = rng.choice(len(df_simulation_summary), size=N_PRIOR_SCATT, replace=False)\n",
    "\n",
    "# make them safe for PyTorch indexing\n",
    "idx_post_proj   = idx_post_proj.tolist()\n",
    "idx_post_scatt  = idx_post_scatt.tolist()\n",
    "idx_prior_proj  = idx_prior_proj.tolist()\n",
    "idx_prior_scatt = idx_prior_scatt.tolist()\n",
    "\n",
    "prior_subset_original_proj  = theta_raw[idx_prior_proj]\n",
    "prior_subset_original_scatt = theta_raw[idx_prior_scatt]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def make_plane_mix2(D, mix_u, mix_v):\n",
    "    # mix = (i, j, angle_deg)\n",
    "    def vec_from_mix(i, j, ang_deg):\n",
    "        ang = np.deg2rad(float(ang_deg))\n",
    "        v = np.zeros((D,), dtype=float)\n",
    "        v[i] = np.cos(ang); v[j] = np.sin(ang)\n",
    "        return v\n",
    "    u_raw = vec_from_mix(*mix_u)\n",
    "    v_raw = vec_from_mix(*mix_v)\n",
    "    u = u_raw / (np.linalg.norm(u_raw) + 1e-12)\n",
    "    v = v_raw - (u @ v_raw) * u\n",
    "    nv = np.linalg.norm(v)\n",
    "    if nv < 1e-8:\n",
    "        # fallback: pick next canonical axis not collinear\n",
    "        for k in range(D):\n",
    "            cand = np.zeros(D); cand[k] = 1.0\n",
    "            cand = cand - (u @ cand) * u\n",
    "            nv2 = np.linalg.norm(cand)\n",
    "            if nv2 > 1e-8:\n",
    "                v = cand / nv2\n",
    "                break\n",
    "    else:\n",
    "        v = v / nv\n",
    "    return np.column_stack([u, v])  # (D,2)\n",
    "\n",
    "def make_projection_basis(D, mode=\"mix2\", seed=7, axes=(0,1), B_manual=None, mix_u=None, mix_v=None):\n",
    "    if mode == \"mix2\":\n",
    "        if mix_u is None or mix_v is None:\n",
    "            raise ValueError(\"Provide MIX2_U and MIX2_V when PROJ_MODE='mix2'.\")\n",
    "        return make_plane_mix2(D, mix_u, mix_v)\n",
    "    elif mode == \"axes\":\n",
    "        u = np.zeros((D,)); v = np.zeros((D,))\n",
    "        u[axes[0]] = 1.0; v[axes[1]] = 1.0\n",
    "        return np.column_stack([u, v])\n",
    "    elif mode == \"manual\":\n",
    "        B = np.asarray(B_manual, float)\n",
    "        if B.shape != (D,2): raise ValueError(\"ORIENT_B_MANUAL must be (D,2)\")\n",
    "        Q, _ = np.linalg.qr(B)\n",
    "        return Q[:, :2]\n",
    "    else:\n",
    "        A = np.random.default_rng(seed).normal(size=(D,2))\n",
    "        Q, _ = np.linalg.qr(A)\n",
    "        return Q[:, :2]\n",
    "\n",
    "def project(V, B):  # V: (..., D), B: (D,2)\n",
    "    return V @ B\n",
    "\n",
    "def axis_limits_with_padding(xy, pad_frac=0.06):\n",
    "    xmin, xmax = np.nanmin(xy[:,0]), np.nanmax(xy[:,0])\n",
    "    ymin, ymax = np.nanmin(xy[:,1]), np.nanmax(xy[:,1])\n",
    "    dx, dy = xmax - xmin, ymax - ymin\n",
    "    xmin -= dx*pad_frac; xmax += dx*pad_frac\n",
    "    ymin -= dy*pad_frac; ymax += dy*pad_frac\n",
    "    if dx <= 0: xmin -= 1; xmax += 1\n",
    "    if dy <= 0: ymin -= 1; ymax += 1\n",
    "    return (xmin, xmax, ymin, ymax)\n",
    "\n",
    "def draw_arrow(ax, start, end, lw=1.5, color=\"#000\", alpha=1.0,\n",
    "               head_w=0.02, head_len=0.04, z=1):\n",
    "    x0, y0 = start; x1, y1 = end\n",
    "    dx, dy = (x1 - x0), (y1 - y0)\n",
    "    ax.arrow(x0, y0, dx, dy,\n",
    "             head_width=head_w, head_length=head_len,\n",
    "             length_includes_head=True, linewidth=lw,\n",
    "             facecolor=color, edgecolor=color, alpha=alpha, zorder=z)\n",
    "\n",
    "def to_mode(arr_torch, mode):\n",
    "    if mode == \"original\":\n",
    "        return to_np(arr_torch)\n",
    "    if mode == \"unit\":\n",
    "        return to_np(theta_to_unit(arr_torch))\n",
    "    if mode == \"zscore\":\n",
    "        return to_np((arr_torch - theta_mu) / theta_sd)\n",
    "    raise ValueError(\"mode must be 'original'|'unit'|'zscore'\")\n",
    "\n",
    "\n",
    "# 2D KDE + mass levels\n",
    "def kde2d_grid(x, y, xlim, ylim, grid=200, bw_scale=None):\n",
    "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "    x = x[np.isfinite(x)]; y = y[np.isfinite(y)]\n",
    "    if x.size < 2:\n",
    "        gx = np.linspace(*xlim, grid); gy = np.linspace(*ylim, grid)\n",
    "        XX, YY = np.meshgrid(gx, gy); return XX, YY, np.zeros_like(XX)\n",
    "    gx = np.linspace(*xlim, grid); gy = np.linspace(*ylim, grid)\n",
    "    XX, YY = np.meshgrid(gx, gy)\n",
    "    sx = np.std(x, ddof=1); sy = np.std(y, ddof=1)\n",
    "    if sx <= 0 or sy <= 0:\n",
    "        sx = (xlim[1]-xlim[0]) * 1e-3 or 1e-6\n",
    "        sy = (ylim[1]-ylim[0]) * 1e-3 or 1e-6\n",
    "    n = x.size; h_scott = n ** (-1/6)\n",
    "    bx = sx * (h_scott if bw_scale is None else float(bw_scale))\n",
    "    by = sy * (h_scott if bw_scale is None else float(bw_scale))\n",
    "    dens = np.zeros_like(XX, dtype=float)\n",
    "    chunk = max(1, int(2e5 // XX.size))\n",
    "    for s in range(0, n, chunk):\n",
    "        e = min(n, s+chunk)\n",
    "        dx = (XX[...,None] - x[None,None,s:e]) / bx\n",
    "        dy = (YY[...,None] - y[None,None,s:e]) / by\n",
    "        dens += np.exp(-0.5*(dx*dx + dy*dy)).sum(axis=2)\n",
    "    dens /= (n * (2*np.pi*bx*by))\n",
    "    return XX, YY, dens\n",
    "\n",
    "def mass_contour_levels(dens, levels_mass=(0.3,0.6,0.9), XX=None, YY=None):\n",
    "    flat = dens.ravel()\n",
    "    dx = (XX[0,1] - XX[0,0]); dy = (YY[1,0] - YY[0,0])\n",
    "    area = dx*dy\n",
    "    order = np.argsort(flat)[::-1]\n",
    "    flat_sorted = flat[order]\n",
    "    mass_cum = np.cumsum(flat_sorted) * area\n",
    "    thr = []\n",
    "    for m in levels_mass:\n",
    "        idx = np.searchsorted(mass_cum, m, side=\"left\")\n",
    "        idx = np.clip(idx, 0, flat_sorted.size-1)\n",
    "        thr.append(float(flat_sorted[idx]))\n",
    "    # Matplotlib requires strictly increasing levels\n",
    "    thr = np.unique(np.sort(thr))\n",
    "    return thr.tolist()\n",
    "\n",
    "# ------------- Build datasets in chosen normalization -------------\n",
    "# Projection data\n",
    "gt_vec = {\n",
    "    \"original\": theta_gt_original,\n",
    "    \"unit\":     theta_gt_unit,\n",
    "    \"zscore\":   (theta_gt_original - theta_mu) / theta_sd,\n",
    "}[proj_normalization]\n",
    "gt_vec = to_np(gt_vec)\n",
    "\n",
    "post_proj = {\n",
    "    \"original\": unit_to_theta(post_samples_unit[idx_post_proj]),\n",
    "    \"unit\":     post_samples_unit[idx_post_proj],\n",
    "    \"zscore\":   (unit_to_theta(post_samples_unit[idx_post_proj]) - theta_mu)/theta_sd\n",
    "}[proj_normalization]\n",
    "prior_proj = {\n",
    "    \"original\": prior_subset_original_proj,\n",
    "    \"unit\":     theta_to_unit(prior_subset_original_proj),\n",
    "    \"zscore\":   (prior_subset_original_proj - theta_mu)/theta_sd\n",
    "}[proj_normalization]\n",
    "post_proj_np  = to_np(post_proj)\n",
    "prior_proj_np = to_np(prior_proj)\n",
    "\n",
    "# Scatter data\n",
    "def extract_xy_any(obj, xname, yname, mode=\"original\"):\n",
    "    ix = theta_colnames.index(xname)\n",
    "    iy = theta_colnames.index(yname)\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        if mode == \"original\":\n",
    "            X = to_np(obj[:, ix])\n",
    "            Y = to_np(obj[:, iy])\n",
    "        elif mode == \"unit\":\n",
    "            U = theta_to_unit(obj)\n",
    "            X = to_np(U[:, ix])\n",
    "            Y = to_np(U[:, iy])\n",
    "        elif mode == \"zscore\":\n",
    "            X = to_np((obj[:, ix] - theta_mu[ix]) / theta_sd[ix])\n",
    "            Y = to_np((obj[:, iy] - theta_mu[iy]) / theta_sd[iy])\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'original'|'unit'|'zscore'\")\n",
    "        return X, Y\n",
    "    arr = np.asarray(obj, dtype=float)\n",
    "    return arr[:, ix], arr[:, iy]\n",
    "\n",
    "post_scatt_original = unit_to_theta(post_samples_unit[idx_post_scatt])\n",
    "prior_scatt_original= prior_subset_original_scatt\n",
    "\n",
    "def to_xy(obj_torch, mode):\n",
    "    return extract_xy_any(obj_torch, param_x, param_y, mode=mode)\n",
    "\n",
    "px_post, py_post = to_xy(post_scatt_original,  scatter_normalization)\n",
    "px_prior,py_prior= to_xy(prior_scatt_original, scatter_normalization)\n",
    "gt_point_original = theta_gt_original.unsqueeze(0)\n",
    "gx, gy = to_xy(gt_point_original, scatter_normalization)\n",
    "gx, gy = float(gx[0]), float(gy[0])\n",
    "\n",
    "# ----- FIGURE 1: D→2 projection with basis axes -----\n",
    "B = make_projection_basis(\n",
    "    D, mode=PROJ_MODE, seed=ORIENT_SEED, axes=ORIENT_AXES,\n",
    "    B_manual=ORIENT_B_MANUAL, mix_u=MIX2_U, mix_v=MIX2_V\n",
    ")  # (D,2)\n",
    "\n",
    "gt_xy     = project(gt_vec,           B).reshape(1,2)\n",
    "post_xy   = project(post_proj_np,     B)\n",
    "prior_xy  = project(prior_proj_np,    B)\n",
    "\n",
    "# Basis axes: project each canonical basis e_i\n",
    "E = np.eye(D)\n",
    "E_proj = E @ B  # (D,2)\n",
    "\n",
    "stack_all = np.vstack([gt_xy, post_xy, prior_xy, E_proj])\n",
    "xmin,xmax,ymin,ymax = axis_limits_with_padding(stack_all, pad_frac=proj_axis_pad_frac)\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(7.4, 6.2))\n",
    "# Draw basis axes (black), from origin\n",
    "for i in range(D):\n",
    "    end = E_proj[i]\n",
    "    draw_arrow(ax1, (0,0), (end[0], end[1]),\n",
    "               lw=basis_axis_lw, color=basis_axis_color, alpha=basis_axis_alpha,\n",
    "               head_w=basis_head_w, head_len=basis_head_l, z=3)\n",
    "    if label_params_on_axes:\n",
    "        ax1.text(end[0]*1.05, end[1]*1.05, f\"θ:{theta_colnames[i]}\",\n",
    "                 fontsize=9, color=basis_axis_color, alpha=basis_axis_alpha)\n",
    "\n",
    "# Prior arrows\n",
    "for p in prior_xy:\n",
    "    draw_arrow(ax1, (0,0), (p[0], p[1]), lw=arrow_lw_prior, color=prior_color,\n",
    "               alpha=arrow_alpha_prior, head_w=arrow_head_width, head_len=arrow_head_length, z=1)\n",
    "# Posterior arrows\n",
    "for p in post_xy:\n",
    "    draw_arrow(ax1, (0,0), (p[0], p[1]), lw=arrow_lw_post, color=heldout_condition_color,\n",
    "               alpha=arrow_alpha_post, head_w=arrow_head_width, head_len=arrow_head_length, z=2)\n",
    "# Ground-truth arrow\n",
    "draw_arrow(ax1, (0,0), (gt_xy[0,0], gt_xy[0,1]), lw=arrow_lw_gt, color=heldout_condition_color,\n",
    "           alpha=arrow_alpha_gt, head_w=arrow_head_width, head_len=arrow_head_length, z=5)\n",
    "\n",
    "# ax1.axhline(0, color=\"0.85\", lw=1)\n",
    "# ax1.axvline(0, color=\"0.85\", lw=1)\n",
    "ax1.set_xlim(xmin, xmax); ax1.set_ylim(ymin, ymax)\n",
    "ax1.set_aspect(\"equal\", adjustable=\"box\")\n",
    "ax1.set_title(f\"Held-out θ projection ({proj_normalization} space) • plane={PROJ_MODE}\")\n",
    "ax1.set_xlabel(\"proj u\"); ax1.set_ylabel(\"proj v\")\n",
    "ax1.set_box_aspect(1)\n",
    "# plt.tight_layout()\n",
    "plt.savefig(FIG1_PATH, dpi=180)\n",
    "plt.savefig(FIG1_PATH.replace(\".svg\",\".png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ----- FIGURE 2: 2D scatter + fixed mass-contours (sorted levels) -----\n",
    "def axis_limits_with_padding_xy(px_prior, py_prior, px_post, py_post, gx, gy, pad_frac=0.08):\n",
    "    xy_all = np.vstack([\n",
    "        np.column_stack([px_prior, py_prior]),\n",
    "        np.column_stack([px_post,  py_post]),\n",
    "        np.array([[gx, gy]])\n",
    "    ])\n",
    "    return axis_limits_with_padding(xy_all, pad_frac=pad_frac)\n",
    "\n",
    "xmin,xmax,ymin,ymax = axis_limits_with_padding_xy(px_prior, py_prior, px_post, py_post, gx, gy, pad_frac=scatter_axis_pad_frac)\n",
    "\n",
    "fig2, ax2 = plt.subplots(figsize=(7.2, 6.2))\n",
    "ax2.scatter(px_prior, py_prior, s=dot_size_prior, alpha=dot_alpha_prior,\n",
    "            color=prior_color, edgecolors='none', label=\"prior\")\n",
    "ax2.scatter(px_post, py_post, s=dot_size_post, alpha=dot_alpha_post,\n",
    "            color=heldout_condition_color, edgecolors='none', label=\"posterior\")\n",
    "ax2.scatter([gx], [gy], s=cross_size, marker='X', color=heldout_condition_color,\n",
    "            edgecolor='k', linewidth=cross_edge_lw, zorder=8, label=\"ground truth\")\n",
    "\n",
    "# Posterior mass-contours (30/60/90%), with increasing levels (fix)\n",
    "XX, YY, dens = kde2d_grid(px_post, py_post, (xmin, xmax), (ymin, ymax), grid=kde_grid, bw_scale=kde_bw_scale)\n",
    "thr = mass_contour_levels(dens, kde_levels_mass, XX=XX, YY=YY)  # sorted & unique\n",
    "if len(thr) >= 1 and np.isfinite(thr).all():\n",
    "    ax2.contour(XX, YY, dens, levels=thr, colors=kde_color, linewidths=kde_lw)\n",
    "\n",
    "ax2.set_xlim(xmin, xmax); ax2.set_ylim(ymin, ymax)\n",
    "ax2.set_xlabel(param_x + f\" ({scatter_normalization})\")\n",
    "ax2.set_ylabel(param_y + f\" ({scatter_normalization})\")\n",
    "ax2.set_aspect(\"equal\", adjustable=\"box\")\n",
    "ax2.legend(frameon=False, loc=\"best\")\n",
    "ax2.set_title(\"Posterior vs prior (2D slice) with posterior mass-contours\")\n",
    "ax2.set_box_aspect(1)\n",
    "# plt.tight_layout()\n",
    "plt.savefig(FIG2_PATH, dpi=180)\n",
    "plt.savefig(FIG2_PATH.replace(\".svg\",\".png\"), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  \", FIG1_PATH)\n",
    "print(\"  \", FIG2_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE CHECK # 4\n",
    "# Posterior density estimator calibration\n",
    "# 4.1 = calculating calibration\n",
    "\n",
    "# === Alpha-star step functions (HPD via log_prob), plus marginal alpha* per parameter ===\n",
    "# Needs (for JOINT):\n",
    "# - posterior_log_probs_for_held_out_sims: {sim_id: torch.Tensor [n_draws]}   (log p(theta_samples | x))\n",
    "# - ground_truth_thetas_for_held_out_sims: {sim_id: torch.Tensor [1,D] or [D]}\n",
    "# Optional (faster):\n",
    "# - posterior_true_log_prob_for_held_out_sims: {sim_id: float or 1D tensor}\n",
    "# Fallback if recompute needed:\n",
    "# - posterior_net_held_out_data  (sbi posterior)\n",
    "# - sim_obs  (indexable by sim_id)\n",
    "#\n",
    "# Needs (for MARGINAL α*):\n",
    "# - posterior_samples_thetas_for_held_out_sims: {sim_id: torch.Tensor [n_draws, D]}\n",
    "#   If missing, we will attempt to resample them (requires posterior_net_held_out_data, sim_obs).\n",
    "\n",
    "def to_np(x): # Helper to avoid pytorch error when using numpy variables\n",
    "    \"\"\"Convert torch.Tensor or array-like to a NumPy array without using tensor.numpy().\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return np.asarray(x.detach().cpu().tolist(), dtype=float)\n",
    "    else:\n",
    "        return np.asarray(x, dtype=float)\n",
    "\n",
    "def _to1d_cpu(x: torch.Tensor) -> torch.Tensor:\n",
    "    x = x.detach()\n",
    "    if x.ndim > 1:\n",
    "        x = x.squeeze()\n",
    "    return x.cpu()\n",
    "\n",
    "def _to2d_cpu(x: torch.Tensor) -> torch.Tensor:\n",
    "    x = x.detach()\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(0)\n",
    "    return x.cpu()\n",
    "\n",
    "def _alpha_star_joint_from_logps(logp_samples_1d: torch.Tensor, logp_true_scalar: float) -> float:\n",
    "    \"\"\"alpha* (joint) = fraction of posterior mass with density >= density at theta_true.\"\"\"\n",
    "    s = to_np(_to1d_cpu(logp_samples_1d))          # <-- no .numpy()\n",
    "    return float(np.mean(s >= logp_true_scalar))\n",
    "\n",
    "def _alpha_star_marginal_central(samples_1d: torch.Tensor, theta_true_scalar: float) -> float:\n",
    "    \"\"\"\n",
    "    Marginal alpha* using CENTRAL intervals (nested, cheap).\n",
    "    Let u = empirical CDF at theta_true (via rank). The smallest central CI containing theta_true has\n",
    "      alpha* = 1 - 2*min(u, 1-u) = 2*|u - 0.5|.\n",
    "    \"\"\"\n",
    "    x = to_np(_to1d_cpu(samples_1d))              # <-- no .numpy()\n",
    "    N = x.shape[0]\n",
    "    # rank and normalized CDF u \\in (0,1) using the standard SBC tie-handling\n",
    "    less = np.sum(x < theta_true_scalar)\n",
    "    equal = np.sum(x == theta_true_scalar)\n",
    "    # break ties uniformly\n",
    "    rank = less + (np.random.randint(0, equal + 1) if equal > 0 else 0)\n",
    "    u = (rank + 0.5) / (N + 1.0)\n",
    "    return float(2.0 * abs(u - 0.5))\n",
    "\n",
    "# ---- Collect held-out ids & basic info\n",
    "heldout_ids = list(posterior_log_probs_for_held_out_sims.keys())\n",
    "assert len(heldout_ids) > 0, \"posterior_log_probs_for_held_out_sims is empty.\"\n",
    "\n",
    "n_draws = int(_to1d_cpu(posterior_log_probs_for_held_out_sims[heldout_ids[0]]).shape[0])\n",
    "\n",
    "# Param names (optional, for later plotting titles)\n",
    "has_param_names = 'input_sim_parameters_to_infer' in globals()\n",
    "if has_param_names:\n",
    "    param_names = input_sim_parameters_to_infer\n",
    "else:\n",
    "    # Try to infer D from a ground-truth vector\n",
    "    any_gt = ground_truth_thetas_for_held_out_sims[heldout_ids[0]]\n",
    "    D = int(_to2d_cpu(any_gt).shape[1])\n",
    "    param_names = [f\"theta[{j}]\" for j in range(D)]\n",
    "\n",
    "# Determine dimensionality D\n",
    "first_gt = _to2d_cpu(ground_truth_thetas_for_held_out_sims[heldout_ids[0]])\n",
    "D = int(first_gt.shape[1])\n",
    "\n",
    "# Optional caches present?\n",
    "has_true_lp_cache = 'posterior_true_log_prob_for_held_out_sims' in globals()\n",
    "has_sample_cache  = 'posterior_samples_thetas_for_held_out_sims' in globals()\n",
    "\n",
    "# Prepare outputs\n",
    "alpha_stars_joint = []                     # shape [n_sims]\n",
    "alpha_stars_marginal = np.zeros((D, len(heldout_ids)), dtype=float)  # [D, n_sims]\n",
    "\n",
    "# Loop\n",
    "for col, sim_id in enumerate(heldout_ids):\n",
    "    if col % 50 == 0:\n",
    "        print(f\"Processing held-out sim_id={sim_id} ({col+1}/{len(heldout_ids)})...\")\n",
    "    # --- JOINT alpha* from log_prob thresholds\n",
    "    logp_samps = _to1d_cpu(posterior_log_probs_for_held_out_sims[sim_id])\n",
    "\n",
    "    if has_true_lp_cache and sim_id in posterior_true_log_prob_for_held_out_sims:\n",
    "        lp_true = float(_to1d_cpu(posterior_true_log_prob_for_held_out_sims[sim_id]).item())\n",
    "    else:\n",
    "        assert 'posterior_net_held_out_data' in globals(), \"Need posterior_net_held_out_data to compute log_prob(theta_true|x).\"\n",
    "        assert 'sim_obs' in globals(), \"Need sim_obs[sim_id] to compute log_prob(theta_true|x).\"\n",
    "        th_true_vec = ground_truth_thetas_for_held_out_sims[sim_id]\n",
    "        th_true_vec = th_true_vec if th_true_vec.ndim == 2 else th_true_vec.unsqueeze(0)  # [1, D]\n",
    "        x_obs = sim_obs[sim_id].unsqueeze(0)\n",
    "        lp_true = float(posterior_net_held_out_data.log_prob(th_true_vec, x=x_obs).detach().cpu().item())\n",
    "\n",
    "    alpha_stars_joint.append(_alpha_star_joint_from_logps(logp_samps, lp_true))\n",
    "\n",
    "    # --- MARGINAL alpha* (per parameter) using posterior samples (central intervals)\n",
    "    if has_sample_cache and sim_id in posterior_samples_thetas_for_held_out_sims:\n",
    "        samps = _to2d_cpu(posterior_samples_thetas_for_held_out_sims[sim_id])  # [N, D]\n",
    "    else:\n",
    "        # resample to match n_draws\n",
    "        assert 'posterior_net_held_out_data' in globals() and 'sim_obs' in globals(), \\\n",
    "            \"Need posterior samples or the ability to sample them (posterior_net_held_out_data + sim_obs).\"\n",
    "        x_obs = sim_obs[sim_id].unsqueeze(0)\n",
    "        samps = posterior_net_held_out_data.sample((n_draws,), x_obs, show_progress_bars=False).detach().cpu()  # [N, D]\n",
    "\n",
    "    th_true = to_np(\n",
    "        _to2d_cpu(ground_truth_thetas_for_held_out_sims[sim_id]).squeeze(0))  # [D] as NumPy array\n",
    "\n",
    "    for j in range(D):\n",
    "        alpha_stars_marginal[j, col] = _alpha_star_marginal_central(samps[:, j], th_true[j])\n",
    "\n",
    "alpha_stars_joint = np.asarray(alpha_stars_joint)  # [n_sims]\n",
    "# At this point you have:\n",
    "# - alpha_stars_joint: shape [n_sims]\n",
    "# - alpha_stars_marginal: shape [D, n_sims]\n",
    "# - param_names: list of D labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb42b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE CHECK # 4\n",
    "# Posterior density estimator calibration\n",
    "# 4.2 = displaying calibration curves\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "# ----- JOINT plot (same style as before) -----\n",
    "alpha_grid = np.linspace(0.0, 1.0, 1001)\n",
    "steps_joint = (alpha_grid[None, :] >= alpha_stars_joint[:, None]).astype(float)  # [n_sims, 1001]\n",
    "mean_joint = steps_joint.mean(axis=0)\n",
    "std_joint  = steps_joint.std(axis=0, ddof=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.0, 5.2))\n",
    "ax.plot(alpha_grid, mean_joint, lw=2.0, color=\"C0\", label=\"Mean step curve\")\n",
    "ax.plot(alpha_grid, alpha_grid, ls='--', lw=1.2, color=\"black\", label=\"Ideal\")\n",
    "\n",
    "ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "ax.set_xlabel(\"Nominal coverage (α)\")\n",
    "ax.set_ylabel(\"Proportion covered (mean of steps)\")\n",
    "ax.set_title(f\"JOINT calibration via α* step functions | sims={len(alpha_stars_joint)}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "figname_to_save = f\"{path_to_save}\\\\CALIBRATION_test_full_posterior\"\n",
    "plt.savefig(f\"{figname_to_save}.png\", dpi=300)\n",
    "plt.savefig(f\"{figname_to_save}.svg\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: ECE and histogram of alpha* (joint)\n",
    "ece_joint = float(np.mean(np.abs(mean_joint - alpha_grid)))\n",
    "print(f\"[JOINT] Expected Calibration Error (ECE): {ece_joint:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6.0, 3.5))\n",
    "plt.hist(alpha_stars_joint, bins=30, density=True, edgecolor='black', linewidth=0.5)\n",
    "plt.axhline(1.0, ls='--', lw=1.2)  # Uniform(0,1) density\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel(\"α (joint)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Distribution of α* (joint) across held-out sims\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- MARGINAL plots (one small panel per parameter) -----\n",
    "D = alpha_stars_marginal.shape[0]\n",
    "cols = min(4, D)\n",
    "rows = ceil(D / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4.5*cols, 3.6*rows), squeeze=False)\n",
    "\n",
    "for j in range(D):\n",
    "    r = j // cols\n",
    "    c = j % cols\n",
    "    ax = axes[r, c]\n",
    "\n",
    "    steps_j = (alpha_grid[None, :] >= alpha_stars_marginal[j, :, None]).astype(float)  # [n_sims, 1001]\n",
    "    mean_j  = steps_j.mean(axis=0)\n",
    "    std_j   = steps_j.std(axis=0, ddof=0)\n",
    "\n",
    "    # stack of step functions (transparent)\n",
    "    # for i in range(steps_j.shape[0]):\n",
    "    #     ax.plot(alpha_grid, steps_j[i], lw=0.6, color=\"C1\", alpha=0.05)\n",
    "\n",
    "    lo = np.clip(mean_j - std_j, 0, 1)\n",
    "    hi = np.clip(mean_j + std_j, 0, 1)\n",
    "    # ax.fill_between(alpha_grid, lo, hi, alpha=0.18, color=\"C1\", label=\"Mean ± 1 SD\")\n",
    "    ax.plot(alpha_grid, mean_j, lw=1.8, color=\"C1\", label=\"Mean step curve\")\n",
    "    ax.plot(alpha_grid, alpha_grid, ls='--', lw=1.0, color=\"black\", label=\"Ideal\")\n",
    "\n",
    "    ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "    ax.set_xlabel(\"α\")\n",
    "    ax.set_ylabel(\"Proportion covered\")\n",
    "    ax.set_title(f\"{param_names[j]} — marginal α* (central)\")\n",
    "\n",
    "    # if j == 0:\n",
    "    #     ax.legend(loc=\"lower right\", fontsize=8)\n",
    "\n",
    "# Hide any unused axes\n",
    "for j in range(D, rows*cols):\n",
    "    r = j // cols; c = j % cols\n",
    "    axes[r, c].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "figname_to_save = f\"{path_to_save}\\\\CALIBRATION_test_per_param\"\n",
    "plt.savefig(f\"{figname_to_save}.png\", dpi=300)\n",
    "plt.savefig(f\"{figname_to_save}.svg\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: per-parameter summaries\n",
    "for j in range(D):\n",
    "    mean_curve_j = (alpha_grid[None, :] >= alpha_stars_marginal[j, :, None]).mean(axis=0)\n",
    "    ece_j = float(np.mean(np.abs(mean_curve_j - alpha_grid)))\n",
    "    print(f\"[MARGINAL] {param_names[j]} — ECE: {ece_j:.4f}\")\n",
    "\n",
    "# Optional: histograms of marginal alpha*\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4.5*cols, 3.2*rows), squeeze=False)\n",
    "for j in range(D):\n",
    "    r = j // cols; c = j % cols\n",
    "    ax = axes[r, c]\n",
    "    ax.hist(alpha_stars_marginal[j], bins=30, density=True, edgecolor='black', linewidth=0.5)\n",
    "    ax.axhline(1.0, ls='--', lw=1.0)  # Uniform(0,1) density reference\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_title(f\"{param_names[j]} — dist. of α* (marginal)\")\n",
    "    ax.set_xlabel(\"α\"); ax.set_ylabel(\"Density\")\n",
    "# Hide unused axes\n",
    "for j in range(D, rows*cols):\n",
    "    r = j // cols; c = j % cols\n",
    "    axes[r, c].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f24de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE CHECK # 5\n",
    "# Posterior density estimator \"resolution\" = what is the average difference in a parameter (ground-truth) so that the estimated posteriors are different with probability > 0.9?\n",
    "# 5.1 = Resolution calculations\n",
    "\n",
    "# Inputs expected:\n",
    "# - posterior_samples_thetas_for_held_out_sims: {sim_id: torch.Tensor [n_draws, D]}\n",
    "# - ground_truth_thetas_for_held_out_sims:     {sim_id: torch.Tensor [1,D] or [D]}\n",
    "# - input_sim_parameters_to_infer:             list[str]\n",
    "\n",
    "# Config\n",
    "NUM_BINS   = 256     # histogram bins for PDF/CDF\n",
    "BIN_MARGIN = 0.01    # extend min/max by this fraction to avoid edge clipping\n",
    "\n",
    "# Stable ordering of sims\n",
    "heldout_ids = sorted(posterior_samples_thetas_for_held_out_sims.keys())\n",
    "N = len(heldout_ids)\n",
    "assert N > 1, \"Need at least two held-out sims.\"\n",
    "\n",
    "# Infer D and param names\n",
    "first = posterior_samples_thetas_for_held_out_sims[heldout_ids[0]]\n",
    "D = int(first.shape[1])\n",
    "param_names = input_sim_parameters_to_infer if 'input_sim_parameters_to_infer' in globals() else [f\"theta[{j}]\" for j in range(D)]\n",
    "\n",
    "def to_np(x): # Helper to avoid numpy and pytorch compatibility errors\n",
    "    \"\"\"Convert torch.Tensor or array-like to a NumPy array without using tensor.numpy().\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return np.asarray(x.detach().cpu().tolist(), dtype=float)\n",
    "    else:\n",
    "        return np.asarray(x, dtype=float)\n",
    "\n",
    "# Stack ground-truth into [N, D]\n",
    "gt = np.zeros((N, D), dtype=np.float64)\n",
    "for i, sid in enumerate(heldout_ids):\n",
    "    th = ground_truth_thetas_for_held_out_sims[sid]   # torch tensor [1,D] or [D]\n",
    "    th_np = to_np(th)                                 # -> NumPy array\n",
    "    if th_np.ndim == 2:\n",
    "        th_np = th_np[0]\n",
    "    gt[i, :] = th_np\n",
    "\n",
    "flattened_by_param = []  # for plotting cell; list of dicts {name, delta, pdelta}\n",
    "\n",
    "for j in tqdm(range(D), desc=\"Parameters\"):\n",
    "    pname = param_names[j]\n",
    "\n",
    "    # ---- 1) Global range for this parameter across all sims' posterior samples\n",
    "    global_min = +np.inf\n",
    "    global_max = -np.inf\n",
    "    for sid in heldout_ids:\n",
    "        x = posterior_samples_thetas_for_held_out_sims[sid][:, j]\n",
    "        x = to_np(x)   # -> NumPy array\n",
    "        if x.size == 0: continue\n",
    "        xmin, xmax = x.min(), x.max()\n",
    "        if xmin < global_min: global_min = xmin\n",
    "        if xmax > global_max: global_max = xmax\n",
    "    xrng = global_max - global_min\n",
    "    if not np.isfinite(xrng) or xrng <= 0:\n",
    "        # degenerate safety\n",
    "        global_min -= 0.5; global_max += 0.5; xrng = global_max - global_min\n",
    "    edges = np.linspace(global_min - BIN_MARGIN*xrng, global_max + BIN_MARGIN*xrng, NUM_BINS + 1)\n",
    "\n",
    "    # ---- 2) Build PDFs and CDFs on the common grid (with a progress bar)\n",
    "    pdf = np.zeros((N, NUM_BINS), dtype=np.float32)\n",
    "    for i, sid in enumerate(tqdm(heldout_ids, desc=f\"[{pname}] histograms\", leave=False)):\n",
    "        x = posterior_samples_thetas_for_held_out_sims[sid][:, j]\n",
    "        x = to_np(x)   # -> NumPy array\n",
    "        counts, _ = np.histogram(x, bins=edges)\n",
    "        pdf[i, :] = counts.astype(np.float32) / max(1, x.shape[0])\n",
    "    cdf = np.cumsum(pdf, axis=1, dtype=np.float32)  # [N, M]\n",
    "\n",
    "    # ---- 3) All-pairs p_delta via matrix multiply: p = CDF_A @ PDF_B^T\n",
    "    pdelta = (cdf @ pdf.T).astype(np.float32)  # [N, N]\n",
    "\n",
    "    # ---- 4) Ground-truth Δθ (B − A)\n",
    "    gt_j = gt[:, j]\n",
    "    dtheta = gt_j[None, :] - gt_j[:, None]      # [N, N]\n",
    "\n",
    "    # ---- 5) Extract upper-triangle unique pairs and keep only Δθ > 0 (no pΔ filter)\n",
    "    iu, ju = np.triu_indices(N, k=1)\n",
    "    x = dtheta[iu, ju].astype(np.float64)\n",
    "    y = pdelta[iu, ju].astype(np.float64)\n",
    "\n",
    "    keep = x > 0.0\n",
    "    x = x[keep]\n",
    "    y = y[keep]\n",
    "\n",
    "    flattened_by_param.append({'name': pname, 'delta': x, 'pdelta': y})\n",
    "    print(f\"[{pname}] kept {x.size:,} pairs out of {N*(N-1)//2:,} unique pairs; filter: Δθ>0 only.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a857eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE CHECK # 5\n",
    "# Posterior density estimator \"resolution\" = what is the average difference in a parameter (ground-truth) so that the estimated posteriors are different with probability > 0.9?\n",
    "# 5.2 = Resolution figures display\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "# Config\n",
    "p_threshold = 0.90   # your decision threshold for pΔ crossing\n",
    "moving_average_fraction = 0.03\n",
    "\n",
    "def moving_average_by_width(x, y, width_fraction=0.10, step_fraction=0.02, min_pts=50):\n",
    "    \"\"\"\n",
    "    Sliding-window mean over x with window = width_fraction * (x_max - x_min).\n",
    "    Returns centers and means (NaN where insufficient points).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    xmin, xmax = np.min(x), np.max(x)\n",
    "    x_rng = xmax - xmin\n",
    "    if x_rng <= 0:\n",
    "        return np.array([xmin]), np.array([np.nanmean(y)])\n",
    "    width = width_fraction * x_rng\n",
    "    step  = max(width * step_fraction, 1e-12)\n",
    "    centers = np.arange(xmin + 0.5*width, xmax - 0.5*width + step, step)\n",
    "    means = np.full(centers.size, np.nan, dtype=float)\n",
    "    for i, c in enumerate(centers):\n",
    "        lo, hi = c - 0.5*width, c + 0.5*width\n",
    "        m = (x >= lo) & (x <= hi)\n",
    "        if m.sum() >= min_pts:\n",
    "            means[i] = y[m].mean()\n",
    "    return centers, means\n",
    "\n",
    "def first_crossing_x(cx, my, y0):\n",
    "    \"\"\"\n",
    "    Return the leftmost x where the curve my crosses y0 (>=), using linear interpolation.\n",
    "    Returns None if no crossing.\n",
    "    \"\"\"\n",
    "    if cx.size < 2 or not np.any(np.isfinite(my)):\n",
    "        return None\n",
    "    mask = np.isfinite(my)\n",
    "    cx = cx[mask]; my = my[mask]\n",
    "    above = my >= y0\n",
    "    if not np.any(above):\n",
    "        return None\n",
    "    idx = np.argmax(above)  # first True\n",
    "    if idx == 0:\n",
    "        return float(cx[0])\n",
    "    # linear interp between (idx-1, idx)\n",
    "    x0, yA = cx[idx-1], my[idx-1]\n",
    "    x1, yB = cx[idx],   my[idx]\n",
    "    if yB == yA:\n",
    "        return float(cx[idx])\n",
    "    t = (y0 - yA) / (yB - yA)\n",
    "    t = np.clip(t, 0.0, 1.0)\n",
    "    return float(x0 + t*(x1 - x0))\n",
    "\n",
    "# Plot settings\n",
    "max_points_to_scatter = 20_000\n",
    "alpha_scatter = 0.05\n",
    "s_scatter = 5.0\n",
    "\n",
    "D = len(flattened_by_param)\n",
    "cols = min(3, D)\n",
    "rows = ceil(D / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(5.6*cols, 4.8*rows), squeeze=False)\n",
    "\n",
    "for k, item in enumerate(flattened_by_param):\n",
    "    r = k // cols\n",
    "    c = k % cols\n",
    "    ax = axes[r, c]\n",
    "    x = item['delta']\n",
    "    y = item['pdelta']\n",
    "\n",
    "    # Optional downsample for plot responsiveness\n",
    "    if x.size > max_points_to_scatter:\n",
    "        rng = np.random.default_rng(0)\n",
    "        idx = rng.choice(x.size, size=max_points_to_scatter, replace=False)\n",
    "        xs, ys = x[idx], y[idx]\n",
    "    else:\n",
    "        xs, ys = x, y\n",
    "\n",
    "    # Scatter\n",
    "    ax.scatter(xs, ys, s=s_scatter, color='C0', alpha=alpha_scatter)\n",
    "\n",
    "    # Moving average (10% window)\n",
    "    cx, my = moving_average_by_width(x, y, width_fraction=moving_average_fraction, step_fraction=moving_average_fraction/3, min_pts=50)\n",
    "    ax.plot(cx, my, lw=2.0, color='blue', label=f'Moving avg ({moving_average_fraction*100}% width)')\n",
    "\n",
    "    # Reference lines\n",
    "    ax.axhline(0.5, ls='--', lw=2.0, color='black', alpha=0.6)\n",
    "    ax.axhline(p_threshold, ls='--', lw=2.0, color='red', alpha=0.8)\n",
    "\n",
    "    # Crossing Δ where moving-average hits p_threshold\n",
    "    x_cross = first_crossing_x(cx, my, p_threshold)\n",
    "    if x_cross is not None:\n",
    "        ax.axvline(x_cross, lw=2.0, color='red', alpha=0.3)\n",
    "        ax.text(x_cross+0.05, p_threshold-0.05 if p_threshold < 0.95 else (p_threshold-0.04),\n",
    "                f\"Δ≈{x_cross:.3g}\", va='top', ha='left', fontsize=10, weight='bold', color='black') # rotation=90, va='top', ha='right', fontsize=9)\n",
    "\n",
    "    ax.set_xlabel(r\"$\\Delta \\theta$ (ground truth)\")\n",
    "    ax.set_ylabel(r\"$p_\\Delta = \\Pr(\\theta_B > \\theta_A)$\")\n",
    "    ax.set_title(item['name'])\n",
    "    ax.set_ylim(-0.03, 1.03)\n",
    "    ax.legend(loc='lower right', fontsize=8)\n",
    "\n",
    "# Hide any empty axes\n",
    "for k in range(D, rows*cols):\n",
    "    r = k // cols; c = k % cols\n",
    "    axes[r, c].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "figname_to_save = f\"{path_to_save}\\\\SENSITIVITY_test\"\n",
    "plt.savefig(f\"{figname_to_save}.png\", dpi=300)\n",
    "plt.savefig(f\"{figname_to_save}.svg\")\n",
    "plt.show()\n",
    "\n",
    "# Also print a clean numeric summary of the crossing per parameter\n",
    "for item in flattened_by_param:\n",
    "    cx, my = moving_average_by_width(item['delta'], item['pdelta'], width_fraction=0.10, step_fraction=0.01, min_pts=50)\n",
    "    xc = first_crossing_x(cx, my, p_threshold)\n",
    "    if xc is None:\n",
    "        print(f\"{item['name']}: moving average never reaches pΔ ≥ {p_threshold:.2f}.\")\n",
    "    else:\n",
    "        print(f\"{item['name']}: detectable Δθ at pΔ ≥ {p_threshold:.2f} ≈ {xc:.4g}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb8f2e9",
   "metadata": {},
   "source": [
    "# SBI ON EXPERIMENTAL DATA\n",
    "### If you re-run this, even with the same training data (downloaded from repository), the posteriors may be different from reported in the paper (there is variability between estimators based on initial seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-train a network, this time on 100% of the training data\n",
    "\n",
    "# ── Build Torch tensors & prior for SBI ──\n",
    "# Collect the “feature‐summary” column names in exactly the same order:\n",
    "summary_colnames = []\n",
    "for feat in features_for_inference:\n",
    "    if feat in input_sim_parameters_as_features:\n",
    "        summary_colnames += [f\"{feat}\"]\n",
    "    else:\n",
    "        for summary_stat in summary_funcs.keys():\n",
    "            summary_colnames += [f\"{feat}_{summary_stat}\"]\n",
    "\n",
    "theta_colnames = input_sim_parameters_to_infer\n",
    "\n",
    "# Python lists → robust to NumPy bridge issues\n",
    "x_sim = torch.tensor(\n",
    "    df_simulation_summary[summary_colnames].to_numpy(dtype=float).tolist(),\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "theta_raw = torch.tensor(\n",
    "    df_simulation_summary[theta_colnames].to_numpy(dtype=float).tolist(),\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "theta_unit = theta_to_unit(theta_raw)\n",
    "x_o = torch.randn((1, len(summary_colnames)))  # default observation\n",
    "\n",
    "low_original = []\n",
    "high_original = []\n",
    "low_unit = []\n",
    "high_unit = []\n",
    "\n",
    "# ── Build low/high tensors from priors_per_parameters dict ──\n",
    "low_original  = torch.tensor(\n",
    "    [priors_per_parameters_to_infer[name][0] for name in theta_colnames],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "high_original = torch.tensor(\n",
    "    [priors_per_parameters_to_infer[name][1] for name in theta_colnames],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# ── Define a uniform prior on [0,1]^d ──\n",
    "low_unit  = theta_to_unit(low_original)   # should be ~0\n",
    "high_unit = theta_to_unit(high_original)  # should be ~1\n",
    "\n",
    "# ── Normalize your training θ’s ──\n",
    "theta_unit = theta_to_unit(theta_raw)\n",
    "\n",
    "prior = sbi_utils.BoxUniform(\n",
    "    low=torch.zeros_like(low_unit),\n",
    "    high=torch.ones_like(high_unit),\n",
    ")\n",
    "\n",
    "# At this point:\n",
    "#   • `x_sim`   : (n_sims, n_features)\n",
    "#   • `theta_unit`: (n_sims, n_theta)\n",
    "#   • `prior`   : BoxUniform over [0,1]^d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36acd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network on the entire simulated data summary data frame ###############\n",
    "if rerun_network_training_and_sampling:\n",
    "    N = df_simulation_summary.shape[0]\n",
    "\n",
    "    # Train SNPE on (N−nb_samples_to_check) points\n",
    "    inference = sbi_inference.SNPE(prior=prior, density_estimator=sbi_density_estimator)\n",
    "    inference.append_simulations(theta_unit, x_sim)\n",
    "    inference.train(\n",
    "            num_atoms                  = network_training_hyperparameters['num_atoms'],       # default is 10\n",
    "            force_first_round_loss     = True,    # start fresh\n",
    "            training_batch_size        = network_training_hyperparameters['training_batch_size'],     # default is 200\n",
    "            learning_rate              = network_training_hyperparameters['learning_rate'],  # default is 0.0005\n",
    "            validation_fraction        = network_training_hyperparameters['validation_fraction'],      # default is 10%\n",
    "            max_num_epochs             = network_training_hyperparameters['max_num_epochs'],    # train up to 1000 epochs\n",
    "            stop_after_epochs          = network_training_hyperparameters['stop_after_epochs'],      # but at least train 20\n",
    "            show_train_summary         = True\n",
    "        )\n",
    "    signature = inspect.signature(inference.train)\n",
    "    print(f\"Inference method signature = {signature}\")\n",
    "\n",
    "    # Plot the training results\n",
    "    plt.plot(figsize=(8,6))\n",
    "    plt.plot(inference.summary['training_loss'],\n",
    "                label=\"Training loss\", linewidth=3, color='blue', alpha=0.5)\n",
    "    plt.plot(inference.summary['validation_loss'],\n",
    "                label=\"Validation loss\", linewidth=3, color='red', alpha=0.5)\n",
    "    plt.xlabel(\"epochs_trained\")\n",
    "    plt.ylabel(f\"Loss - negative log-probability\\n(neg-log-prob of observation,\\ngiven inferred generative parameters)\")\n",
    "    plt.title(f\"Training results on SBI neural net\\nwith 100% of the simulated observations\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{path_to_save}\\\\network_training_loss_no_held_out_sims_for_training.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4094de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE OR LOAD POSTERIOR\n",
    "if rerun_network_training_and_sampling:\n",
    "    posterior_network_pickle_path = f\"{path_to_save}\\\\posterior_infering_network.pkl\"\n",
    "    # ── build & save the posterior object ──────────────────────────────────────\n",
    "    posterior = inference.build_posterior()\n",
    "    # a sampling method may be necessary if sampling_algorithm == 'mcmc', 'vi' or 'si'\n",
    "    # vi_method = \"rKL\"  # or fKL\n",
    "    # posterior = inference.build_posterior(sample_with=sampling_algorithm,\n",
    "    #                                      vi_method=vi_method)\n",
    "    # Save posterior\n",
    "    with open(posterior_network_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(posterior, f)\n",
    "    print(f\"✅ Posterior-infering network saved to '{posterior_network_pickle_path}'\")\n",
    "else:\n",
    "    # Load the existing network\n",
    "    posterior_network_pickle_path = f\"{path_to_load}\\\\posterior_infering_network.pkl\"\n",
    "    with open(posterior_network_pickle_path, 'rb') as f:\n",
    "        posterior = pickle.load(f)\n",
    "    print(f\"✅ Posterior-infering network loaded from '{posterior_network_pickle_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0629350b",
   "metadata": {},
   "source": [
    "## POSTERIOR ESTIMATE PER PARTICIPANT\n",
    "### (not reported in paper)\n",
    "### One posterior distribution per [Muscle_pair × Intensity × Participant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dcdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using single-muscle posteriors as features, (if len(input_sim_parameters_as_features) >= 1, right now they should all be filled with zeros as placeholder values)\n",
    "# augment df_experiment_summary by duplicating each row N times and assigning N samples of posterior-estimated parameters used for inference to those rows\n",
    "# Thus, the neural network will infer N posterior per condition\n",
    "# N = previously_estimated_posterior_samples_for_experimental_data_SBI (already defined)\n",
    "match_by = ['subject', 'muscle', 'intensity']\n",
    "# map_strings_of_posterior_estimated_parameters_to_param_used_as_features = {\n",
    "#     \"disynpatic_inhib_connections_desired_MN_MN\": \"disynpatic_inhib_connections_desired_MN_MN_self\",\n",
    "#     \"common_input_high_freq_middle_of_range\": \"common_input_high_freq_middle_of_range_self\",\n",
    "#     \"common_input_high_freq_half_width_range\": \"common_input_high_freq_half_width_range_self\",\n",
    "#     \"common_input_std\": \"common_input_std_self\"\n",
    "# }\n",
    "df_experiment_summary_augmented = df_experiment_summary.copy()\n",
    "if len(input_sim_parameters_as_features) >= 1:\n",
    "    df_experiment_summary_augmented['muscle'] = df_experiment_summary['muscle_pair'].str.split(\"<->\").str[0]\n",
    "    # Load csv with reviously estimated posterior\n",
    "    df_previous_posterior_each_subject = pd.read_csv(f\"{previously_estimated_posterior_results_path}{previously_estimated_posterior_each_subject_csv}\")\n",
    "    df_previous_posterior_each_subject['muscle'] = df_previous_posterior_each_subject['muscle_pair'].str.split(\"<->\").str[0]\n",
    "    df_previous_posterior_each_subject.rename(columns=map_strings_of_posterior_estimated_parameters_to_param_used_as_features, inplace=True)\n",
    "    # Apply standardization (determined earlier, ctrl+F \"apply_standardization()\") to the posterior samples of features to be used as features\n",
    "    apply_standardization(df_previous_posterior_each_subject, norm_stats, input_sim_parameters_as_features)\n",
    "\n",
    "    # Build a lookup dict of posterior pools per (subject, muscle, intensity)\n",
    "    posterior_pool = {}\n",
    "    for gkey, gdf in df_previous_posterior_each_subject.groupby(match_by, dropna=False):\n",
    "        posterior_pool[gkey] = gdf.reset_index(drop=True)\n",
    "\n",
    "    # Duplicate each experimental row N times and fill with matched posterior draws\n",
    "    aug_rows = []\n",
    "    rng = np.random.default_rng()  # set a seed here if you want reproducibility, e.g., np.random.default_rng(123)\n",
    "\n",
    "    for idx, row in df_experiment_summary_augmented.iterrows():\n",
    "        gkey = tuple(row[k] for k in match_by)\n",
    "        pool = posterior_pool.get(gkey, None)\n",
    "\n",
    "        # Make N copies of the experimental row\n",
    "        block = pd.DataFrame([row.values] * previously_estimated_posterior_samples_for_experimental_data_SBI,\n",
    "                             columns=df_experiment_summary_augmented.columns)\n",
    "        block['posterior_draw'] = np.arange(previously_estimated_posterior_samples_for_experimental_data_SBI,\n",
    "                                            dtype=int)  # useful to track Monte Carlo runs\n",
    "\n",
    "        if (pool is None) or pool.empty:\n",
    "            # No matching posterior samples: keep placeholders (zeros/NaNs) and warn\n",
    "            for c in input_sim_parameters_as_features:\n",
    "                # keep existing placeholders in df_experiment_summary_augmented or explicitly set NaN\n",
    "                if c not in block.columns:\n",
    "                    block[c] = np.nan\n",
    "            # (optional) print/log a warning\n",
    "            # print(f\"[WARN] No posterior samples for key={gkey}; leaving {input_sim_parameters_as_features} as-is.\")\n",
    "        else:\n",
    "            # Sample N posterior rows with replacement for these features\n",
    "            sampled = pool.sample(n=previously_estimated_posterior_samples_for_experimental_data_SBI,\n",
    "                                  replace=True, random_state=rng.integers(0, 1_000_000))[input_sim_parameters_as_features].reset_index(drop=True)\n",
    "\n",
    "            # Ensure destination columns exist, then assign\n",
    "            for c in input_sim_parameters_as_features:\n",
    "                if c not in block.columns:\n",
    "                    block[c] = np.nan\n",
    "            block.loc[:, input_sim_parameters_as_features] = sampled.values\n",
    "\n",
    "        aug_rows.append(block)\n",
    "\n",
    "    df_experiment_summary_augmented = pd.concat(aug_rows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bbf2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_experiment_summary_augmented # Check result of previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1584f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose which dataframe to use ---\n",
    "df_for_inference = (\n",
    "    df_experiment_summary_augmented\n",
    "    if len(input_sim_parameters_as_features) >= 1\n",
    "    else df_experiment_summary\n",
    ")\n",
    "\n",
    "# --- Config ---\n",
    "keys_cols    = ['subject', 'muscle_pair', 'intensity']   # condition identity\n",
    "feature_cols = summary_colnames                          # observed + posterior-as-features cols\n",
    "num_samples  = (num_posterior_samples['experiment_with_posterior_estimates_as_features']\n",
    "                if len(input_sim_parameters_as_features) >= 1\n",
    "                else num_posterior_samples['experiment'])\n",
    "best_method  = best_posterior_estimate_method            # \"logp\" or \"knn\"\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def to_np(x): # Helper to avoid incompatibility between numpy and pytorch\n",
    "    \"\"\"Convert torch.Tensor or array-like to a NumPy array without using tensor.numpy().\"\"\"\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return np.asarray(x.detach().cpu().tolist(), dtype=float)\n",
    "    else:\n",
    "        return np.asarray(x, dtype=float)\n",
    "\n",
    "def _densest_by_knn(samples_np, k=5):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(samples_np)\n",
    "    dist, _ = nbrs.kneighbors(samples_np)\n",
    "    kth = dist[:, k]   # distance to k-th non-self neighbor\n",
    "    return int(np.argmin(kth))\n",
    "\n",
    "def infer_posteriors_by_condition_looped(\n",
    "    df_obs,\n",
    "    posterior,\n",
    "    feature_cols,\n",
    "    keys_cols,\n",
    "    num_samples=1000,\n",
    "    best_method=\"logp\",          # \"logp\" or \"knn\"\n",
    "    show_progress=True,\n",
    "    transform_unit_to_theta=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each (subject, muscle_pair, intensity), loop over each duplicated row\n",
    "    and sample from p(theta | x_row). Concatenate samples to create a mixture\n",
    "    across the duplicates for that condition.\n",
    "    \"\"\"\n",
    "    posterior_samples_dict = {}\n",
    "    posterior_logp_dict    = {}\n",
    "    best_samples_dict      = {}\n",
    "\n",
    "    device  = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    grouped = df_obs.groupby(keys_cols, dropna=False)\n",
    "\n",
    "    outer_iter = grouped\n",
    "    total_groups = grouped.ngroups if hasattr(grouped, \"ngroups\") else None\n",
    "    if show_progress:\n",
    "        outer_iter = tqdm(grouped, total=total_groups, desc=\"Conditions\", leave=True)\n",
    "\n",
    "    for gkey, gdf in outer_iter:\n",
    "        # Print condition + number of duplicate rows\n",
    "        if show_progress:\n",
    "            outer_iter.write(f\"Condition {gkey}: {len(gdf)} rows, sampling {num_samples} / row\")\n",
    "\n",
    "        all_samps = []\n",
    "        all_logp  = []   # only used if best_method == \"logp\"\n",
    "\n",
    "        inner_iter = gdf.iterrows()\n",
    "        if show_progress:\n",
    "            inner_iter = tqdm(inner_iter, total=len(gdf), desc=f\"{gkey}\", leave=False)\n",
    "\n",
    "        for _, row in inner_iter:\n",
    "            # x row as Python list → torch tensor (no from_numpy)\n",
    "            x_vec = row[feature_cols].to_numpy(dtype=float).tolist()  # length Dx\n",
    "            x_t   = torch.tensor([x_vec], dtype=torch.float32, device=device)  # shape (1, Dx)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # sample for this single row\n",
    "                s = posterior.sample((num_samples,), x=x_t, show_progress_bars=False)  # (N, Dθ)\n",
    "                if best_method == \"logp\":\n",
    "                    lp = posterior.log_prob(s, x=x_t)  # (N,)\n",
    "\n",
    "            if transform_unit_to_theta:\n",
    "                s = unit_to_theta(s)  # your helper\n",
    "\n",
    "            # Store as NumPy via safe helper\n",
    "            all_samps.append(to_np(s))   # (N, Dθ)\n",
    "            if best_method == \"logp\":\n",
    "                all_logp.append(to_np(lp))   # (N,)\n",
    "\n",
    "        samples_flat = np.vstack(all_samps)      # (N * #rows, Dθ)\n",
    "\n",
    "        # Decide best index and logp storage depending on method\n",
    "        if best_method == \"logp\":\n",
    "            logp_flat = np.concatenate(all_logp)  # (N * #rows,)\n",
    "            posterior_logp_dict[gkey] = logp_flat\n",
    "            best_idx = int(np.argmax(logp_flat))\n",
    "        elif best_method == \"knn\":\n",
    "            posterior_logp_dict[gkey] = np.array([])\n",
    "            best_idx = _densest_by_knn(samples_flat, k=5)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown best_method='{best_method}'\")\n",
    "\n",
    "        posterior_samples_dict[gkey] = samples_flat\n",
    "        best_samples_dict[gkey]      = samples_flat[best_idx]\n",
    "\n",
    "    return posterior_samples_dict, posterior_logp_dict, best_samples_dict\n",
    "\n",
    "\n",
    "# --- Run inference ---\n",
    "posterior_samples_dict, posterior_logp_dict, best_samples_dict = infer_posteriors_by_condition_looped(\n",
    "    df_obs=df_for_inference,\n",
    "    posterior=posterior,\n",
    "    feature_cols=feature_cols,\n",
    "    keys_cols=keys_cols,\n",
    "    num_samples=100, # 100 # num_samples # 100 to make things faster\n",
    "    best_method=\"knn\", # \"knn\" # best_method # \"knn\" to make things faster\n",
    "    show_progress=True,\n",
    "    transform_unit_to_theta=True,   # set False if you prefer to transform later\n",
    ")\n",
    "# ^ This can be extremely slow if some experimental observations are outside the coverage of the density estimator\n",
    "# Replaced by num_samples=100 and best_method=\"knn\"\n",
    "\n",
    "# (Optional) If you prefer to delay unit->theta transform, you can run it here:\n",
    "# for k, arr in posterior_samples_dict.items():\n",
    "#     arr_torch = torch.tensor(arr.tolist(), dtype=torch.float32)\n",
    "#     posterior_samples_dict[k] = to_np(unit_to_theta(arr_torch))\n",
    "#\n",
    "# for k, arr in best_samples_dict.items():\n",
    "#     arr_torch = torch.tensor(arr.tolist(), dtype=torch.float32)\n",
    "#     best_samples_dict[k] = to_np(unit_to_theta(arr_torch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce90db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the posteriors (per-participant)\n",
    "\n",
    "if rerun_network_training_and_sampling:\n",
    "    posterior_estimates_pickle_path = f\"{path_to_save}\\\\posterior_estimates_each_subject.pkl\"\n",
    "    # Save the samples, the log proabilities, and the best samples (highest logp)\n",
    "    posterior_estimates = {\n",
    "        \"posterior_samples\": posterior_samples_dict,\n",
    "        \"posterior_logp\": posterior_logp_dict,\n",
    "        \"best_samples\": best_samples_dict,\n",
    "    }\n",
    "    with open(posterior_estimates_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(posterior_estimates, f)\n",
    "    print(f\"✅ Posterior estimate samples saved to '{posterior_estimates_pickle_path}'\")\n",
    "else:\n",
    "    posterior_estimates_pickle_path = f\"{path_to_load}\\\\posterior_estimates_each_subject.pkl\"\n",
    "    with open(posterior_estimates_pickle_path, \"rb\") as f:\n",
    "        posterior_estimates = pickle.load(f)\n",
    "    print(f\"✅ Posterior estimate samples loaded from '{posterior_estimates_pickle_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e1211e",
   "metadata": {},
   "source": [
    "## POSTERIOR ESTIMATE, PARTICIPANTS POOLED\n",
    "### (results reported in paper = experimental observations of all participants' motor units pooled together)\n",
    "### One posterior distribution per [Muscle_pair × Intensity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d8276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAMPLING - same as before, but with subjects grouped together now\n",
    "# DOES NOT SUPPORT 'input_sim_parameters_as_features' YET - CHECK IF IT STILL WORKS FOR MUSCLE PAIRS INFERENCE\n",
    "\n",
    "# --- Choose which dataframe to use ---\n",
    "# df_for_inference = (\n",
    "#     df_experiment_summary_grouped_subjects\n",
    "#     if len(input_sim_parameters_as_features) >= 1\n",
    "#     else df_experiment_summary\n",
    "# )\n",
    "df_for_inference = df_experiment_summary_grouped_subjects\n",
    "\n",
    "# --- Config ---\n",
    "keys_cols    = ['muscle_pair', 'intensity']   # condition identity\n",
    "feature_cols = summary_colnames               # observed + posterior-as-features cols\n",
    "# num_samples  = (num_posterior_samples['experiment_with_posterior_estimates_as_features']\n",
    "#                 if len(input_sim_parameters_as_features) >= 1\n",
    "#                 else num_posterior_samples['experiment'])\n",
    "num_samples = num_posterior_samples['experiment']\n",
    "best_method  = best_posterior_estimate_method            # \"logp\" or \"knn\"\n",
    "device       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- Run inference ---\n",
    "posterior_samples_dict_subjects_grouped, posterior_logp_dict_subjects_grouped, best_samples_dict_subjects_grouped = infer_posteriors_by_condition_looped(\n",
    "    df_obs=df_for_inference,\n",
    "    posterior=posterior,\n",
    "    feature_cols=feature_cols,\n",
    "    keys_cols=keys_cols,\n",
    "    num_samples=num_samples,\n",
    "    best_method=best_method,\n",
    "    show_progress=True,\n",
    "    transform_unit_to_theta=True,   # set False if you prefer to transform later\n",
    ")\n",
    "\n",
    "# (Optional) If you prefer to delay unit->theta transform, you can run it here:\n",
    "# for k, arr in posterior_samples_dict.items():\n",
    "#     posterior_samples_dict[k] = unit_to_theta(torch.from_numpy(arr)).numpy()\n",
    "# for k, arr in best_samples_dict.items():\n",
    "#     best_samples_dict[k] = unit_to_theta(torch.from_numpy(arr)).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed504d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rerun_network_training_and_sampling:\n",
    "    posterior_estimates_pickle_path = f\"{path_to_save}\\\\posterior_estimates_subjects_grouped.pkl\"\n",
    "    # Save the samples, the log proabilities, and the best samples (highest logp)\n",
    "    posterior_estimates_subjects_grouped = {\n",
    "        \"posterior_samples\": posterior_samples_dict_subjects_grouped,\n",
    "        \"posterior_logp\": posterior_logp_dict_subjects_grouped,\n",
    "        \"best_samples\": best_samples_dict_subjects_grouped,\n",
    "    }\n",
    "    with open(posterior_estimates_pickle_path, \"wb\") as f:\n",
    "        pickle.dump(posterior_estimates_subjects_grouped, f)\n",
    "    print(f\"✅ Posterior estimate samples (for grouped subjects) saved to '{posterior_estimates_pickle_path}'\")\n",
    "else:\n",
    "    posterior_estimates_pickle_path = f\"{path_to_load}\\\\posterior_estimates_subjects_grouped.pkl\"\n",
    "    with open(posterior_estimates_pickle_path, \"rb\") as f:\n",
    "        posterior_estimates_subjects_grouped = pickle.load(f)\n",
    "    print(f\"✅ Posterior estimate samples (for grouped subjects) loaded from '{posterior_estimates_pickle_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dc9e96",
   "metadata": {},
   "source": [
    "### Posterior estimates - Generate plots\n",
    "#### Not inline; saves the plots in path_to_save folder\n",
    "#### Those are just plots to see what's going on; the final plots are generated in another scripts (based on the saved .csv files containing the posterior samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5ac86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCHING sbi plt_hist_1d function used in pairplot() to fix numpy VS Pytorch compatibility issue\n",
    "import copy\n",
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "import sbi.analysis.plot as sbi_plot  # where pairplot & plt_hist_1d live\n",
    "\n",
    "def safe_plt_hist_1d(ax, samples, limits, diag_kwargs):\n",
    "    \"\"\"\n",
    "    Replacement for sbi.analysis.plot.plt_hist_1d that:\n",
    "      - uses Freedman–Diaconis per dimension (if requested),\n",
    "      - converts torch limits -> Python floats,\n",
    "      - never triggers torch.Tensor.__array__ (so works with NumPy>=2 + PyTorch>=2.5).\n",
    "    \"\"\"\n",
    "    # Original: hist_kwargs = copy.deepcopy(diag_kwargs[\"mpl_kwargs\"])\n",
    "    hist_kwargs = copy.deepcopy(diag_kwargs[\"mpl_kwargs\"])\n",
    "\n",
    "    # Convert limits (torch tensor of shape (2,)) -> floats\n",
    "    lo = float(limits[0])\n",
    "    hi = float(limits[1])\n",
    "\n",
    "    # Current bins setting (may or may not be present)\n",
    "    bins = hist_kwargs.get(\"bins\", None)\n",
    "\n",
    "    # If no bins specified, apply a FD-style heuristic or fallback\n",
    "    if bins is None:\n",
    "        heuristic = diag_kwargs.get(\"bin_heuristic\", \"Freedman-Diaconis\")\n",
    "\n",
    "        if heuristic == \"Freedman-Diaconis\":\n",
    "            # FD rule on this 1D samples array\n",
    "            bw = 2.0 * iqr(samples) * (len(samples) ** (-1.0 / 3.0))\n",
    "\n",
    "            if not np.isfinite(bw) or bw <= 0.0:\n",
    "                # Fallback: fixed bin count\n",
    "                n_bins = 50\n",
    "            else:\n",
    "                span = hi - lo\n",
    "                if span <= 0:\n",
    "                    span = 1.0\n",
    "                n_bins = int(np.clip(np.ceil(span / bw), 5, 200))\n",
    "        else:\n",
    "            # Other heuristics could go here; for now, just fixed\n",
    "            n_bins = 50\n",
    "\n",
    "        # Turn bin count into edges\n",
    "        bins = np.linspace(lo, hi, n_bins + 1)\n",
    "\n",
    "    # If user gave an integer, turn it into edges (again: with float limits)\n",
    "    if isinstance(bins, int):\n",
    "        bins = np.linspace(lo, hi, bins + 1)\n",
    "\n",
    "    hist_kwargs[\"bins\"] = bins\n",
    "    ax.hist(samples, **hist_kwargs)\n",
    "\n",
    "\n",
    "# Monkey-patch sbi\n",
    "sbi_plot.plt_hist_1d = safe_plt_hist_1d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44f636",
   "metadata": {},
   "source": [
    "### Posterior estimates - per participant\n",
    "(not showing them inline in the jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save_obs_pairplots = f\"{path_to_save}\\\\obs_pairplots_each_subject\"\n",
    "os.makedirs(path_to_save_obs_pairplots, exist_ok=True)\n",
    "\n",
    "n_dim = len(input_sim_parameters_to_infer)\n",
    "\n",
    "for obs_key in posterior_estimates['posterior_samples'].keys():\n",
    "    muscle_pair_i = obs_key[1]\n",
    "    diagonal_density_color = muscle_colors_dict[muscle_pair_i]\n",
    "    off_diagonal_2d_density_colormap = muscle_colormaps_dict[muscle_pair_i]\n",
    "\n",
    "    # Ensure posterior_samples is NumPy (good practice, though sbi can handle both)\n",
    "    posterior_samples = posterior_estimates['posterior_samples'][obs_key]\n",
    "    posterior_samples = to_np(posterior_samples)  # (N, D)\n",
    "    best_theta = posterior_estimates['best_samples'][obs_key]\n",
    "\n",
    "    fig, axes = pairplot(\n",
    "        posterior_samples,\n",
    "        limits=[[low_original[i].item(), high_original[i].item()] for i in range(n_dim)],\n",
    "        diag_kwargs={\n",
    "            \"mpl_kwargs\": {\n",
    "                \"color\": diagonal_density_color,\n",
    "                # no 'bins' here; our patched plt_hist_1d will handle it\n",
    "            },\n",
    "            # You can keep using the default FD heuristic:\n",
    "            \"bin_heuristic\": \"Freedman-Diaconis\",\n",
    "        },\n",
    "        upper_kwargs={\"mpl_kwargs\": {\"cmap\": off_diagonal_2d_density_colormap}},\n",
    "        labels=input_sim_parameters_to_infer,\n",
    "        figsize=(7, 7),\n",
    "    )\n",
    "\n",
    "    # 2) overlay best estimate (best_theta) on every subplot:\n",
    "    best_linestyle = dict(color=\"#000000\", linestyle=\"-\", linewidth=1.5, alpha=1)\n",
    "    best_marker    = dict(marker=\"X\", s=50, edgecolor=\"#000000\",\n",
    "                          facecolor=\"white\", linewidth=1.0)\n",
    "\n",
    "    added_best_legend = False\n",
    "\n",
    "    for i in range(n_dim):\n",
    "        # diagonal: vertical line at best_theta[i]\n",
    "        ax = axes[i, i]\n",
    "        if not added_best_legend:\n",
    "            ax.axvline(best_theta[i], **best_linestyle,\n",
    "                       label=\"θ̂ (posterior with highest prob)\")\n",
    "            added_best_legend = True\n",
    "        else:\n",
    "            ax.axvline(best_theta[i], **best_linestyle)\n",
    "\n",
    "        # off-diagonal (upper triangle)\n",
    "        for j in range(n_dim):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            ax_off = axes[i, j]\n",
    "            if not added_best_legend:\n",
    "                ax_off.scatter(best_theta[j], best_theta[i], **best_marker,\n",
    "                               label=\"θ̂ (posterior with highest prob)\")\n",
    "                added_best_legend = True\n",
    "            else:\n",
    "                ax_off.scatter(best_theta[j], best_theta[i], **best_marker)\n",
    "\n",
    "    axes[0, 0].legend(loc=\"upper right\", fontsize=\"small\")\n",
    "\n",
    "    fig.suptitle(f\"Posterior estimate for {obs_key}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    sanitized_filename = re.sub(r'[^0-9A-Za-z._-]+', '', str(obs_key))\n",
    "    sanitized_filename = sanitized_filename.replace('np.float64', '')\n",
    "    out_path = os.path.join(path_to_save_obs_pairplots, f\"{sanitized_filename}.png\")\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d82287",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dfs_temp = []\n",
    "\n",
    "for (subject, muscle_pair, intensity), arr in posterior_estimates['posterior_samples'].items():\n",
    "    # Make sure 'arr' is a plain NumPy array, not a torch.Tensor\n",
    "    arr_np = to_np(arr)   # or np.asarray(arr.detach().cpu().tolist(), dtype=float) if you prefer inline\n",
    "\n",
    "    # Turn into DataFrame with the right column names\n",
    "    df_i = pd.DataFrame(arr_np, columns=input_sim_parameters_to_infer)\n",
    "\n",
    "    df_i['subject']     = subject\n",
    "    df_i['muscle_pair'] = muscle_pair\n",
    "    df_i['intensity']   = intensity\n",
    "    \n",
    "    list_dfs_temp.append(df_i)\n",
    "\n",
    "# concatenate them all into one big long DataFrame\n",
    "df_kde_posterior_samples = pd.concat(list_dfs_temp, ignore_index=True)\n",
    "\n",
    "# Save the data frame\n",
    "df_kde_posterior_samples_path = f\"{path_to_save}\\\\posterior_samples_each_subject_df.csv\"\n",
    "df_kde_posterior_samples.to_csv(df_kde_posterior_samples_path, index=False)\n",
    "\n",
    "print(f\"✅ Saved posterior samples to: {df_kde_posterior_samples_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796ac6dc",
   "metadata": {},
   "source": [
    "### Posterior estimates - participants pooled\n",
    "(not showing them inline in the jupyter notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892dda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOTTING THE POSTERIOR ESTIMATES FOR EACH EXPERIMENTAL OBSERVATION, WITH SUBJECTS GROUPED\n",
    "\n",
    "# Create folder to save experimental observations pairplots\n",
    "path_to_save_obs_pairplots = f\"{path_to_save}\\\\obs_pairplots_subjects_grouped\"\n",
    "os.makedirs(path_to_save_obs_pairplots, exist_ok=True)\n",
    "\n",
    "n_dim = len(input_sim_parameters_to_infer)\n",
    "\n",
    "for obs_key in posterior_estimates_subjects_grouped['posterior_samples'].keys():\n",
    "    muscle_pair_i = obs_key[0]\n",
    "    diagonal_density_color = muscle_colors_dict[muscle_pair_i]\n",
    "    off_diagonal_2d_density_colormap = muscle_colormaps_dict[muscle_pair_i]\n",
    "\n",
    "    # Ensure NumPy arrays (avoid torch→numpy inside sbi.pairplot)\n",
    "    posterior_samples = posterior_estimates_subjects_grouped['posterior_samples'][obs_key]\n",
    "    posterior_samples_np = to_np(posterior_samples)   # <— key change\n",
    "\n",
    "    # Best-θ as NumPy as well\n",
    "    best_theta = posterior_estimates_subjects_grouped['best_samples'][obs_key]\n",
    "    best_theta = to_np(best_theta).reshape(-1)        # <— key change\n",
    "\n",
    "    # 1) draw the base pairplot (pass NumPy, not torch)\n",
    "    fig, axes = pairplot(\n",
    "        posterior_samples_np,\n",
    "        limits=[[low_original[i].item(), high_original[i].item()] for i in range(n_dim)],\n",
    "        diag_kwargs={\n",
    "            \"mpl_kwargs\": {\n",
    "                \"color\": diagonal_density_color,\n",
    "            },\n",
    "            \"bin_heuristic\": \"Freedman-Diaconis\",\n",
    "        },\n",
    "        upper_kwargs={\"mpl_kwargs\": {\"cmap\": off_diagonal_2d_density_colormap}},\n",
    "        labels=input_sim_parameters_to_infer,\n",
    "        figsize=(7, 7),\n",
    "    )\n",
    "\n",
    "    # 2) overlay best estimate (best_theta) on every subplot:\n",
    "    best_linestyle = dict(color=\"#000000\",  linestyle=\"-\", linewidth=1.5, alpha=1)\n",
    "    best_marker    = dict(marker=\"X\", s=50, edgecolor=\"#000000\", facecolor=\"white\", linewidth=1)\n",
    "\n",
    "    added_best_legend = False\n",
    "\n",
    "    for i in range(n_dim):\n",
    "        ax = axes[i, i]\n",
    "        if not added_best_legend:\n",
    "            ax.axvline(best_theta[i], **best_linestyle, label=\"θ̂ (posterior with highest prob)\")\n",
    "            added_best_legend = True\n",
    "        else:\n",
    "            ax.axvline(best_theta[i], **best_linestyle)\n",
    "\n",
    "        for j in range(n_dim):\n",
    "            if i >= j:  # only upper triangle\n",
    "                continue\n",
    "            ax_off = axes[i, j]\n",
    "            if not added_best_legend:\n",
    "                ax_off.scatter(best_theta[j], best_theta[i], **best_marker,\n",
    "                               label=\"θ̂ (posterior with highest prob)\")\n",
    "                added_best_legend = True\n",
    "            else:\n",
    "                ax_off.scatter(best_theta[j], best_theta[i], **best_marker)\n",
    "\n",
    "    axes[0, 0].legend(loc=\"upper right\", fontsize=\"small\")\n",
    "\n",
    "    fig.suptitle(f\"Posterior estimate for {obs_key}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    sanitized_filename = re.sub(r'[^0-9A-Za-z._-]+', '', str(obs_key))\n",
    "    sanitized_filename = sanitized_filename.replace('np.float64', '')\n",
    "    plt.savefig(f\"{path_to_save_obs_pairplots}\\\\{sanitized_filename}.png\", dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ed819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame for kernel density estimates and plotting for the latent parameters of interest (at the level of subjects grouped together)\n",
    "# Full marginals (= considering the full distribution over the other latent parameters)\n",
    "list_dfs_temp = []\n",
    "for (muscle_pair, intensity), arr in posterior_estimates_subjects_grouped['posterior_samples'].items():\n",
    "    # Ensure NumPy array (avoid torch→numpy issue inside pandas)\n",
    "    arr_np = to_np(arr)\n",
    "    # turn the array into a DataFrame with the right column names\n",
    "    df_i = pd.DataFrame(arr_np, columns=input_sim_parameters_to_infer)\n",
    "\n",
    "    df_i['muscle_pair'] = muscle_pair\n",
    "    df_i['intensity']   = intensity\n",
    "    \n",
    "    list_dfs_temp.append(df_i)\n",
    "\n",
    "# concatenate them all into one big long DataFrame\n",
    "df_kde_posterior_samples_subjects_grouped = pd.concat(list_dfs_temp, ignore_index=True)\n",
    "\n",
    "# Save the data frame\n",
    "df_kde_posterior_samples_subjects_grouped_path = f\"{path_to_save}\\\\posterior_samples_subjects_grouped_df.csv\"\n",
    "df_kde_posterior_samples_subjects_grouped.to_csv(df_kde_posterior_samples_subjects_grouped_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame for the best (highest probability density = posterior mode) sample, per subject, per muscle pair, per intensity\n",
    "list_dfs_temp = []\n",
    "for key in posterior_estimates['best_samples'].keys():\n",
    "    arr = posterior_estimates['best_samples'][key]\n",
    "    df_i = pd.DataFrame({\n",
    "        \"subject\": [key[0]],\n",
    "        \"muscle_pair\": [key[1]],\n",
    "        \"intensity\": [key[2]]})\n",
    "    for i, param_name in enumerate(input_sim_parameters_to_infer):\n",
    "        df_i[param_name] = arr[i]\n",
    "    list_dfs_temp.append(df_i)\n",
    "df_best_sample = pd.concat(list_dfs_temp, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005db3f",
   "metadata": {},
   "source": [
    "##### POSTERIOR FIGURES ARE GENERATED IN A SEPRATAE SCRIPT FROM THE SAVED .csv FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c84e21e",
   "metadata": {},
   "source": [
    "# Posterior predictive checks\n",
    "### CHECK IF ESTIMATED POSTERIOR PARAMETERS, WHEN USED AS SIMULATION PARAMETERS, REPRODUCE THE EXPERIMENTAL DATA\n",
    "GET A FEW SAMPLES FROM THE POSTERIOR AND SIMULATE FROM IT = check if this reproduces the experimental data it is supposed to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e26e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and helper functions\n",
    "# Import libraries and modules\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# Add the parent directory of this notebook to sys.path\n",
    "parent_dir = Path().resolve().parent\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "from simulator import SimulationParameters, run_simulation\n",
    "\n",
    "from brian2 import *\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from dataclasses import fields, dataclass, asdict\n",
    "# Import for parallelization\n",
    "import getpass\n",
    "import psutil\n",
    "from joblib import Parallel, delayed\n",
    "from brian2 import prefs, device\n",
    "import logging\n",
    "from threading import Thread, Event\n",
    "import time\n",
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd06daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function needed when posterior-based simulation of 2 pools (muscle pairs) and not 1\n",
    "def expand_params(params: list[str]) -> list[str]:\n",
    "    out = []\n",
    "    for s in params:\n",
    "        if s.startswith('disynpatic_inhib_'):  # note: output fixes the typo -> disynaptic\n",
    "            if s.endswith('_self'):\n",
    "                out += [\n",
    "                    'disynaptic_inhib_self_connectivity_pool0',\n",
    "                    'disynaptic_inhib_self_connectivity_pool1',\n",
    "                ]\n",
    "            elif s.endswith('_other_pool'):\n",
    "                out += [\n",
    "                    'disynaptic_inhib_connectivity_pool0_to_pool1',\n",
    "                    'disynaptic_inhib_connectivity_pool1_to_pool0',\n",
    "                ]\n",
    "            else:\n",
    "                out.append(s)\n",
    "        elif s.endswith('_self'):\n",
    "            base = s[:-5]  # strip '_self'\n",
    "            out += [f'{base}_pool0', f'{base}_pool1']\n",
    "        else:\n",
    "            out.append(s)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb27b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_parallel_cpus = 16\n",
    "n_sims_per_condition = 1 # 100 # Determine the number of samples (parameters) to draw from the posterior. All those parameter will be used for posterior-predictive simulations.\n",
    "muscle_pairs_posterior_predictive_checks = False # Set to True for the BETWEEN MUSCLES CONFIG\n",
    "# SINGLE MUSCLE CONFIG\n",
    "if not muscle_pairs_posterior_predictive_checks:\n",
    "    priors_filename = f\"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\Mapping_Recurrent_Inhibition\\\\Simulation_parameters\\\\FIG_4_Simulation_based_inference_training_dataset\\\\single muscles training data\\\\Simulation_single_muscle_priors_batch0.pkl\" # used to extract the fixed parameters used for the simulations\n",
    "# MUSCLE PAIRS CONFIG\n",
    "else:\n",
    "    folder_sims_param_muscle_pairs = f\"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\Mapping_Recurrent_Inhibition\\\\Simulation_parameters\\\\FIG_4_Simulation_based_inference_training_dataset\\\\muscle pairs training data\"\n",
    "    muscle_pairs_param_files = [f for f in listdir(folder_sims_param_muscle_pairs) if isfile(join(folder_sims_param_muscle_pairs, f))]\n",
    "    priors_filepath_each = {}\n",
    "    for f in muscle_pairs_param_files:\n",
    "        if 'inference' in f:\n",
    "            continue\n",
    "        priors_filepath_each[f] = f\"{folder_sims_param_muscle_pairs}//{f}\" # Can be any from the paired-muscles simulations, since the muscle-pair specific priors are replaced with input_sim_parameters_as_features\n",
    "\n",
    "# Set the mapping between the data frame column names (= actual parameter field names from SimulationParameters()) and the parameter names used later in the script to create the list of parameters to be simulated in this batch\n",
    "simulate_per_subject_or_subjects_grouped = \"subjects_grouped\" # \"per_subject\" or \"subjects_grouped\"\n",
    "\n",
    "variable_parameters = input_sim_parameters_to_infer + input_sim_parameters_as_features\n",
    "if muscle_pairs_posterior_predictive_checks: # In the case of simulating 2 pools, the variable parameters have to be expanded\n",
    "    variable_parameters = expand_params(variable_parameters)\n",
    "\n",
    "# Create folder to save the new simulations coming from the most likely posteriors\n",
    "path_to_save_posterior_predictive_check_sims = f\"{path_to_save}\\\\posterior_predictive_checks_{simulate_per_subject_or_subjects_grouped}\"\n",
    "# path_to_save_posterior_predictive_check_sims = f\"{simulated_data_path_for_priors_of_posterior_predictive_checks}\\\\posterior_predictive_checks_{simulate_per_subject_or_subjects_grouped}\"\n",
    "os.makedirs(f\"{path_to_save_posterior_predictive_check_sims}\", exist_ok=True)\n",
    "\n",
    "# Get fixed parmeters for the simulation = find the simulation parameters in the first simulation output\n",
    "# SINGLE MUSCLE CASE\n",
    "if not muscle_pairs_posterior_predictive_checks:\n",
    "    # Get fixed parmeters for the simulation = find the simulation parameters in the first simulation output\n",
    "    with open(priors_filename, \"rb\") as f:\n",
    "        batch_sim_priors = pickle.load(f)\n",
    "    # variable_parameters = [str(k) for k, v in batch_sim_priors['free_parameter_bounds'].items()]\n",
    "    fixed_parameters = batch_sim_priors['params_prior_list'][0] # The full list or the first instance [0] of SimulationParameters objects that were simulated in the batch used for network training for SBI\n",
    "    # fixed_parameters = batch_sim_priors['fixed_parameters']\n",
    "# BETWEEN MUSCLE PAIRS CASE\n",
    "else:\n",
    "    batch_sim_priors_each = {}\n",
    "    for key, filename_path in priors_filepath_each.items():\n",
    "        # Expand batch_sim_priors_each to have both directions (VM-VL_intensityXX and VL_VM_intensityXX for instance) by looping into filename_path twice, and just reversing all pool0 and pool1 values from the priors_from_posterior dataframe\n",
    "        with open(filename_path, \"rb\") as f:\n",
    "            batch_sim_priors_each[key] = pickle.load(f)\n",
    "            fixed_parameters = batch_sim_priors_each[key]['params_prior_list'][0] # The baseline fixed parameters are the same for all simulations, so could come from any loaded batch of priors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MANY other helper functions used in the case of simulating pairs of muscles\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "ALIAS_COLUMNS = {\n",
    "    \"std_of_second_common_input_pool0\": \"common_input_std_pool0\",\n",
    "    \"std_of_second_common_input_pool1\": \"common_input_std_pool1\",\n",
    "}\n",
    "POOL_SUFFIXES = (\"_pool0\", \"_pool1\")\n",
    "\n",
    "def split_dir_pair(pair_str: str) -> Tuple[str, str]:\n",
    "    if \"<->\" in pair_str:\n",
    "        a, b = pair_str.split(\"<->\")\n",
    "    elif \"-\" in pair_str:\n",
    "        a, b = pair_str.split(\"-\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized pair string: {pair_str}\")\n",
    "    return a.strip(), b.strip()\n",
    "\n",
    "def undirected_key(a: str, b: str) -> str:\n",
    "    return f\"{a}-{b}\"\n",
    "\n",
    "def batch_key_for_priors(a: str, b: str, intensity: int) -> str:\n",
    "    return f\"{a}-{b}_intensity{int(intensity)}\"\n",
    "\n",
    "def find_priors_entry(batch_sim_priors_each: Dict, a: str, b: str, intensity: int):\n",
    "    key_ab = batch_key_for_priors(a, b, intensity)\n",
    "    key_ba = batch_key_for_priors(b, a, intensity)\n",
    "    if key_ab in batch_sim_priors_each:\n",
    "        return batch_sim_priors_each[key_ab], \"ab\"\n",
    "    elif key_ba in batch_sim_priors_each:\n",
    "        return batch_sim_priors_each[key_ba], \"ba\"\n",
    "    else:\n",
    "        raise KeyError(f\"No priors found for {a}-{b} (or {b}-{a}) @ intensity {intensity}\")\n",
    "\n",
    "def swap_pool_cols_inplace(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    cols = set(df.columns)\n",
    "    pairs, seen, uniq = [], set(), []\n",
    "    for c in list(cols):\n",
    "        if c.endswith(\"_pool0\"):\n",
    "            other = c[:-6] + \"_pool1\"\n",
    "            if other in cols:\n",
    "                pairs.append((c, other))\n",
    "        elif c.endswith(\"_pool1\"):\n",
    "            other = c[:-6] + \"_pool0\"\n",
    "            if other in cols:\n",
    "                pairs.append((other, c))\n",
    "    for a, b in pairs:\n",
    "        if (a, b) not in seen and (b, a) not in seen:\n",
    "            uniq.append((a, b)); seen.add((a, b))\n",
    "    for a, b in uniq:\n",
    "        df[a], df[b] = df[b].copy(), df[a].copy()\n",
    "    return df\n",
    "\n",
    "def filter_priors_df(df: pd.DataFrame, intensity: int, subject: Optional[str]) -> pd.DataFrame:\n",
    "    g = df.copy()\n",
    "    if \"intensity\" in g.columns:\n",
    "        g = g[g[\"intensity\"].astype(int) == int(intensity)]\n",
    "    if subject is not None and \"subject\" in g.columns:\n",
    "        g = g[g[\"subject\"] == subject]\n",
    "    if g.empty:\n",
    "        raise ValueError(f\"priors_from_posterior_df empty after filtering (intensity={intensity}, subject={subject})\")\n",
    "    return g\n",
    "\n",
    "def build_expanded_between_pool_posteriors(\n",
    "    posterior_estimates: Dict[str, Dict],\n",
    "    batch_sim_priors_each: Dict,\n",
    "    variable_parameters: list[str],\n",
    "    sample_priors_with_replacement: bool = True,\n",
    "    seed: Optional[int] = None,\n",
    "    subject_by_condition: Optional[Dict[Tuple[str, int], str]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Merge directions (A<->B, B<->A) into undirected (A-B), expand to variable_parameters,\n",
    "    and fill remaining pool-specific params from priors. If `sample_priors_with_replacement`\n",
    "    is True, draw priors rows with replacement per sample (N) to introduce variability.\n",
    "\n",
    "    Returns:\n",
    "      tensors = {\n",
    "        'posterior_samples': { (pair, intensity): torch.FloatTensor [N, P] },\n",
    "        'best_samples':      { (pair, intensity): torch.FloatTensor [P]     },\n",
    "      }\n",
    "      df_all : pandas.DataFrame over all conditions\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Collect directional arrays by undirected condition\n",
    "    dir_samples, dir_best = {}, {}\n",
    "    for (pair_str, intens), arr in posterior_estimates[\"posterior_samples\"].items():\n",
    "        a, b = split_dir_pair(pair_str)\n",
    "        key = (undirected_key(a, b), int(float(intens)))\n",
    "        dir_samples.setdefault(key, {})[\"ab\"] = arr\n",
    "    for (pair_str, intens), arr in posterior_estimates[\"best_samples\"].items():\n",
    "        a, b = split_dir_pair(pair_str)\n",
    "        key = (undirected_key(a, b), int(float(intens)))\n",
    "        dir_best.setdefault(key, {})[\"ab\"] = arr\n",
    "    # Try to attach reverse if present\n",
    "    for (pair_str, intens), arr in posterior_estimates[\"posterior_samples\"].items():\n",
    "        b, a = split_dir_pair(pair_str)   # flip parsing to find reverse\n",
    "        key = (undirected_key(a, b), int(float(intens)))\n",
    "        if key in dir_samples and \"ba\" not in dir_samples[key]:\n",
    "            dir_samples[key][\"ba\"] = arr\n",
    "    for (pair_str, intens), arr in posterior_estimates[\"best_samples\"].items():\n",
    "        b, a = split_dir_pair(pair_str)\n",
    "        key = (undirected_key(a, b), int(float(intens)))\n",
    "        if key in dir_best and \"ba\" not in dir_best[key]:\n",
    "            dir_best[key][\"ba\"] = arr\n",
    "\n",
    "    tensors = {\"posterior_samples\": {}, \"best_samples\": {}}\n",
    "    df_rows = []\n",
    "    P = len(variable_parameters)\n",
    "    col_idx = {v: i for i, v in enumerate(variable_parameters)}\n",
    "\n",
    "    # Posterior-derived names (the rest come from priors)\n",
    "    POST_NAMES = {\n",
    "        \"excitatory_input_baseline_pool0\",\n",
    "        \"excitatory_input_baseline_pool1\",\n",
    "        \"disynaptic_inhib_connectivity_pool0_to_pool1\",\n",
    "        \"disynaptic_inhib_connectivity_pool1_to_pool0\",\n",
    "        \"between_pool_excitatory_input_correlation\",\n",
    "    }\n",
    "\n",
    "    for (pair, intensity), sides in dir_samples.items():\n",
    "        a, b = pair.split(\"-\")\n",
    "        subj = None if subject_by_condition is None else subject_by_condition.get((pair, intensity), None)\n",
    "\n",
    "        pri_entry, used_order = find_priors_entry(batch_sim_priors_each, a, b, intensity)\n",
    "        pri_df = pri_entry[\"priors_from_posterior_df\"]\n",
    "        pri_df = pri_df.rename(columns=ALIAS_COLUMNS)\n",
    "        pri_df = pri_df if used_order == \"ab\" else swap_pool_cols_inplace(pri_df)\n",
    "        pri_df = filter_priors_df(pri_df, intensity=intensity, subject=subj)\n",
    "\n",
    "        arr_ab = sides.get(\"ab\")\n",
    "        arr_ba = sides.get(\"ba\")\n",
    "        if arr_ab is None and arr_ba is None:\n",
    "            continue\n",
    "        if arr_ab is None: arr_ab = arr_ba\n",
    "        if arr_ba is None: arr_ba = arr_ab\n",
    "\n",
    "        N = min(arr_ab.shape[0], arr_ba.shape[0])\n",
    "        arr_ab = arr_ab[:N, :]\n",
    "        arr_ba = arr_ba[:N, :]\n",
    "\n",
    "        X = np.full((N, P), np.nan, dtype=np.float32)\n",
    "\n",
    "        # Posterior-derived fills\n",
    "        if \"excitatory_input_baseline_pool0\" in col_idx:\n",
    "            X[:, col_idx[\"excitatory_input_baseline_pool0\"]] = arr_ab[:, 0]\n",
    "        if \"excitatory_input_baseline_pool1\" in col_idx:\n",
    "            X[:, col_idx[\"excitatory_input_baseline_pool1\"]] = arr_ba[:, 0]\n",
    "        if \"disynaptic_inhib_connectivity_pool0_to_pool1\" in col_idx:\n",
    "            X[:, col_idx[\"disynaptic_inhib_connectivity_pool0_to_pool1\"]] = arr_ab[:, 1]\n",
    "        if \"disynaptic_inhib_connectivity_pool1_to_pool0\" in col_idx:\n",
    "            X[:, col_idx[\"disynaptic_inhib_connectivity_pool1_to_pool0\"]] = arr_ba[:, 1]\n",
    "        if \"between_pool_excitatory_input_correlation\" in col_idx:\n",
    "            X[:, col_idx[\"between_pool_excitatory_input_correlation\"]] = 0.5 * (arr_ab[:, 2] + arr_ba[:, 2])\n",
    "\n",
    "        # Priors-derived fills: sample with replacement per row\n",
    "        pri_cols_needed = [v for v in variable_parameters if v not in POST_NAMES]\n",
    "        missing_pri = [v for v in pri_cols_needed if v not in pri_df.columns]\n",
    "        if missing_pri:\n",
    "            raise ValueError(\n",
    "                f\"Priors DF missing needed columns for {pair}@{intensity}: {missing_pri}\\n\"\n",
    "                f\"Available: {sorted(pri_df.columns)}\"\n",
    "            )\n",
    "\n",
    "        if sample_priors_with_replacement:\n",
    "            idx = rng.integers(0, len(pri_df), size=N)\n",
    "            sampled = pri_df.iloc[idx]\n",
    "        else:\n",
    "            # deterministic: cycle through without replacement if enough rows, else wrap\n",
    "            if len(pri_df) == 0:\n",
    "                raise ValueError(\"No rows in priors DF after filtering.\")\n",
    "            rep = int(np.ceil(N / len(pri_df)))\n",
    "            sampled = pd.concat([pri_df]*rep, ignore_index=True).iloc[:N]\n",
    "\n",
    "        for v in pri_cols_needed:\n",
    "            X[:, col_idx[v]] = sampled[v].to_numpy(dtype=float)\n",
    "\n",
    "        # Best sample vector (posterior best + one sampled priors row)\n",
    "        best_sides = dir_best.get((pair, intensity), {})\n",
    "        best_ab = best_sides.get(\"ab\")\n",
    "        best_ba = best_sides.get(\"ba\")\n",
    "        if best_ab is None and best_ba is None:\n",
    "            best_vec = X[0].copy()\n",
    "        else:\n",
    "            if best_ab is None: best_ab = best_ba\n",
    "            if best_ba is None: best_ba = best_ab\n",
    "            best_vec = np.full((P,), np.nan, dtype=np.float32)\n",
    "            if \"excitatory_input_baseline_pool0\" in col_idx:\n",
    "                best_vec[col_idx[\"excitatory_input_baseline_pool0\"]] = best_ab[0]\n",
    "            if \"excitatory_input_baseline_pool1\" in col_idx:\n",
    "                best_vec[col_idx[\"excitatory_input_baseline_pool1\"]] = best_ba[0]\n",
    "            if \"disynaptic_inhib_connectivity_pool0_to_pool1\" in col_idx:\n",
    "                best_vec[col_idx[\"disynaptic_inhib_connectivity_pool0_to_pool1\"]] = best_ab[1]\n",
    "            if \"disynaptic_inhib_connectivity_pool1_to_pool0\" in col_idx:\n",
    "                best_vec[col_idx[\"disynaptic_inhib_connectivity_pool1_to_pool0\"]] = best_ba[1]\n",
    "            if \"between_pool_excitatory_input_correlation\" in col_idx:\n",
    "                best_vec[col_idx[\"between_pool_excitatory_input_correlation\"]] = 0.5 * (best_ab[2] + best_ba[2])\n",
    "\n",
    "            # sample one priors row for the remaining fields\n",
    "            row = pri_df.iloc[rng.integers(0, len(pri_df))]\n",
    "            for v in pri_cols_needed:\n",
    "                best_vec[col_idx[v]] = float(row[v])\n",
    "\n",
    "        # Guard\n",
    "        if np.isnan(X).any():\n",
    "            miss = [variable_parameters[j] for j in np.where(np.isnan(X).any(axis=0))[0]]\n",
    "            raise ValueError(f\"Incomplete matrix for {pair}@{intensity}: {miss}\")\n",
    "        if np.isnan(best_vec).any():\n",
    "            miss = [variable_parameters[j] for j in np.where(np.isnan(best_vec))[0]]\n",
    "            raise ValueError(f\"Incomplete best_vec for {pair}@{intensity}: {miss}\")\n",
    "\n",
    "        tensors[\"posterior_samples\"][(pair, intensity)] = torch.tensor(X, dtype=torch.float32)\n",
    "        tensors[\"best_samples\"][(pair, intensity)] = torch.tensor(best_vec, dtype=torch.float32)\n",
    "\n",
    "        df_block = pd.DataFrame(X, columns=variable_parameters)\n",
    "        df_block.insert(0, \"pair\", pair)\n",
    "        df_block.insert(1, \"intensity\", intensity)\n",
    "        if \"subject\" in pri_df.columns:\n",
    "            # carry over sampled subjects (optional, useful for debugging)\n",
    "            df_block[\"subject_from_priors\"] = sampled[\"subject\"].to_numpy() if sample_priors_with_replacement else np.nan\n",
    "        df_rows.append(df_block)\n",
    "\n",
    "    df_all = pd.concat(df_rows, ignore_index=True) if df_rows else pd.DataFrame(columns=[\"pair\",\"intensity\"]+variable_parameters)\n",
    "    return tensors, df_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fdc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find set of priors (torch tensors and data frame) (samples from previously estimated posterior) that will be used later to create \"params_prior_list\" = list of SimulationParameters variables (dataclasses) that will be used for the simulations\n",
    "\n",
    "if simulate_per_subject_or_subjects_grouped == \"subjects_grouped\":\n",
    "    if not muscle_pairs_posterior_predictive_checks: # single muscle case\n",
    "        posterior_samples_to_draw_posterior_predictive_checks_sim_param_from = posterior_estimates_subjects_grouped\n",
    "        posterior_samples_to_draw_posterior_predictive_checks_sim_param_from_df = df_kde_posterior_samples_subjects_grouped\n",
    "    else: # between muscles case\n",
    "        posterior_samples_to_draw_posterior_predictive_checks_sim_param_from, posterior_samples_to_draw_posterior_predictive_checks_sim_param_from_df = build_expanded_between_pool_posteriors(\n",
    "            posterior_estimates=posterior_estimates_subjects_grouped,\n",
    "            batch_sim_priors_each=batch_sim_priors_each,\n",
    "            variable_parameters=variable_parameters,\n",
    "            sample_priors_with_replacement=True,   # <- enables per-row sampling\n",
    "            seed=42,                               # <- reproducibility\n",
    "            )\n",
    "elif simulate_per_subject_or_subjects_grouped == \"per_subject\":\n",
    "    if not muscle_pairs_posterior_predictive_checks:\n",
    "        posterior_samples_to_draw_posterior_predictive_checks_sim_param_from = posterior_estimates\n",
    "        posterior_samples_to_draw_posterior_predictive_checks_sim_param_from_df = df_kde_posterior_samples\n",
    "    else: # between muscles case\n",
    "        posterior_samples_to_draw_posterior_predictive_checks_sim_param_from, posterior_samples_to_draw_posterior_predictive_checks_sim_param_from_df = build_expanded_between_pool_posteriors(\n",
    "            posterior_estimates=posterior_estimates,\n",
    "            batch_sim_priors_each=batch_sim_priors_each,\n",
    "            variable_parameters=variable_parameters,\n",
    "            sample_priors_with_replacement=True,   # <- enables per-row sampling\n",
    "            seed=42,                               # <- reproducibility\n",
    "            )\n",
    "else:\n",
    "    print(f'Please select [\"subjects_grouped\" or \"per_subject\"] for simulate_per_subject_or_subjects_grouped')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792053f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples_to_draw_posterior_predictive_checks_sim_param_from_df # Checking results from previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca52872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure you already have:\n",
    "# # - variable_parameters = expand_params(input_sim_parameters_to_infer + input_sim_parameters_as_features)\n",
    "# # - batch_sim_priors_each loaded (your loop with pickle.load)\n",
    "# # - posterior_estimates_subjects_grouped with keys 'posterior_samples' and 'best_samples'\n",
    "\n",
    "# tensors_grouped, df_grouped = build_expanded_between_pool_posteriors(\n",
    "#     posterior_estimates=posterior_estimates_subjects_grouped,\n",
    "#     batch_sim_priors_each=batch_sim_priors_each,\n",
    "#     variable_parameters=variable_parameters,\n",
    "# )\n",
    "\n",
    "# # Access:\n",
    "# # tensors_grouped['posterior_samples'][( 'SOL-GM', 10 )]  -> torch.FloatTensor [N, len(variable_parameters)]\n",
    "# # tensors_grouped['best_samples'][( 'SOL-GM', 10 )]      -> torch.FloatTensor [len(variable_parameters)]\n",
    "# # df_grouped                                              -> one big table across all pairs & intensities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1044d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brian2.units.fundamentalunits import Quantity, have_same_dimensions\n",
    "from brian2.units import ms, nA, msiemens\n",
    "\n",
    "def ensure_unit(v, unit, scale=1.0):\n",
    "    \"\"\"\n",
    "    If v already has units, return it (rescaled if you like).\n",
    "    If v is unitless (float/ndarray), attach `scale * unit`.\n",
    "    \"\"\"\n",
    "    if isinstance(v, Quantity):\n",
    "        # Optional: rescale to the target unit (not strictly necessary)\n",
    "        # return v.in_units(unit)\n",
    "        # Brian2 accepts any same-dimension quantity, so returning v is fine.\n",
    "        return v\n",
    "    else:\n",
    "        return (v * scale) * unit\n",
    "\n",
    "# 1) A mapping of input-names → functions that take (value, init_kwargs, default)\n",
    "#    and mutate init_kwargs appropriately.\n",
    "_special_setters = {\n",
    "    # If your posterior values are in the same magnitude as you used before,\n",
    "    # keep scale=1e3 as you had (your default is 3.0*1e3*nA).\n",
    "    'Renshaw_to_MN_IPSP': lambda v, kw, default: kw.update({\n",
    "        'Renshaw_to_MN_IPSP': ensure_unit(v, nA, scale=1e3)\n",
    "    }),\n",
    "    'AHP_conductance_delta_after_spiking': lambda v, kw, default: kw.update({\n",
    "        'AHP_conductance_delta_after_spiking': ensure_unit(v, msiemens)\n",
    "    }),\n",
    "    'tau_Renshaw': lambda v, kw, default: kw.update({\n",
    "        'tau_Renshaw': ensure_unit(v, ms)\n",
    "    }),\n",
    "    'synaptic_IPSP_decay_time_constant': lambda v, kw, default: kw.update({\n",
    "        'synaptic_IPSP_decay_time_constant': ensure_unit(v, ms)\n",
    "    }),\n",
    "    'MN_RC_synpatic_delay': lambda v, kw, default: kw.update({\n",
    "        'MN_RC_synpatic_delay': ensure_unit(v, ms)\n",
    "    }),\n",
    "}\n",
    "\n",
    "\n",
    "def build_posterior_predictive_check_simulation_parameters(fixed_parameters, free_priors, batch_name):\n",
    "    \"\"\"\n",
    "    Build a SimulationParameters with:\n",
    "      - values taken from 'fixed_parameters' (dataclass instance), EXCEPT fields with init=False,\n",
    "      - overrides from 'free_priors' (SBI posterior draws),\n",
    "      - special handling for multi-field params (e.g., common_input_std, frequency ranges, disynaptic matrix),\n",
    "      - and a custom output folder name.\n",
    "    \"\"\"\n",
    "    default = SimulationParameters()\n",
    "    init_kwargs = {}\n",
    "\n",
    "    # Partition dataclass fields into init vs non-init\n",
    "    init_field_names     = {f.name for f in fields(SimulationParameters) if f.init}\n",
    "    non_init_field_names = {f.name for f in fields(SimulationParameters) if not f.init}\n",
    "\n",
    "    # --- Start from dict copies of inputs ---\n",
    "    fixed_params = asdict(fixed_parameters).copy()  # includes non-init fields!\n",
    "    free_params  = free_priors.copy()\n",
    "\n",
    "    # Drop any non-init fields from the fixed side\n",
    "    for k in list(fixed_params.keys()):\n",
    "        if k in non_init_field_names:\n",
    "            fixed_params.pop(k)\n",
    "\n",
    "    # --- Resolve naming collisions / special cases ---\n",
    "\n",
    "    # Split common_input_std: keep input0 from fixed side, map posterior to input1\n",
    "    if 'common_input_std' in fixed_params:\n",
    "        # keep only pool 0, input 0 (shape is [pool, input])\n",
    "        first_val = np.asarray(fixed_params.pop('common_input_std'))[0][0]\n",
    "        fixed_params['common_input_std_input0'] = float(first_val)\n",
    "\n",
    "    if 'common_input_std' in free_params:\n",
    "        free_params['common_input_std_input1'] = float(free_params.pop('common_input_std'))\n",
    "\n",
    "    # If a legacy within-pool RI is present on the free side, map it to pool0 self-connectivity\n",
    "    if 'disynpatic_inhib_connections_desired_MN_MN' in fixed_params:\n",
    "        fixed_params.pop('disynpatic_inhib_connections_desired_MN_MN', None)\n",
    "    if 'disynpatic_inhib_connections_desired_MN_MN' in free_params:\n",
    "        free_params['disynaptic_inhib_self_connectivity_pool0'] = float(\n",
    "            free_params.pop('disynpatic_inhib_connections_desired_MN_MN')\n",
    "        )\n",
    "\n",
    "    # Merge into a single “prior overrides” dict\n",
    "    all_priors = {**fixed_params, **free_params}\n",
    "\n",
    "    # Staging buffers for multi-field assembly\n",
    "    common_input_std_to_assign = np.array([[0.0, 0.0],\n",
    "                                           [0.0, 0.0]], dtype=float)  # [pool, input]\n",
    "    disyn = deepcopy(default.disynpatic_inhib_connections_desired_MN_MN)\n",
    "    disyn_touched = False\n",
    "\n",
    "    # Iterate once over all keys\n",
    "    for name, val in all_priors.items():\n",
    "\n",
    "        # 1) Special setters (if you have a registry elsewhere)\n",
    "        if name in _special_setters:\n",
    "            _special_setters[name](val, init_kwargs, default)\n",
    "            continue\n",
    "\n",
    "        # 2) Single-field remappings / list-wrapping\n",
    "        if name == 'excitatory_input_baseline':\n",
    "            # ensure it's a list with at least one pool\n",
    "            init_kwargs['excitatory_input_baseline'] = [float(val)]\n",
    "            continue\n",
    "\n",
    "        # 3) Frequency band of common input (middle & half-width → [low, high])\n",
    "        if name in (\"common_input_high_freq_middle_of_range\",\n",
    "                    \"common_input_high_freq_half_width_range\"):\n",
    "            # Use whatever is already staged, else default’s array\n",
    "            freq = deepcopy(init_kwargs.get(\"frequency_range_of_common_input\",\n",
    "                                            default.frequency_range_of_common_input))\n",
    "            pool_idx  = 0  # we only use pool 0 for the posterior-predictive check here\n",
    "            input_idx = 1  # use the second input as “high-frequency” slot\n",
    "            mid  = float(all_priors.get('common_input_high_freq_middle_of_range',\n",
    "                                        default.common_input_characteristics[\"Frequency_middle_of_range\"][\"pool_0\"][\"input_1\"]))\n",
    "            half = float(all_priors.get('common_input_high_freq_half_width_range',\n",
    "                                        default.common_input_characteristics[\"Frequency_half_width_of_range\"][\"pool_0\"][\"input_1\"]))\n",
    "            freq[pool_idx, input_idx, 0] = mid - half\n",
    "            freq[pool_idx, input_idx, 1] = mid + half\n",
    "            init_kwargs[\"frequency_range_of_common_input\"] = freq\n",
    "            continue\n",
    "\n",
    "        # 4) Common‐input STD per input (we assemble a [pool,input] array)\n",
    "        if name.startswith(\"common_input_std_input\"):\n",
    "            pool_idx  = 0\n",
    "            input_idx = int(name[-1])  # expects ...input0 or ...input1\n",
    "            common_input_std_to_assign[pool_idx, input_idx] = float(val)\n",
    "            init_kwargs[\"common_input_std\"] = common_input_std_to_assign\n",
    "            continue\n",
    "\n",
    "        # 5) Disynaptic connectivity entries (assemble 2×2)\n",
    "        if name == 'disynaptic_inhib_self_connectivity_pool0':\n",
    "            disyn[0, 0] = float(val); disyn_touched = True; continue\n",
    "        if name == 'disynaptic_inhib_self_connectivity_pool1':\n",
    "            disyn[1, 1] = float(val); disyn_touched = True; continue\n",
    "        if name == 'disynaptic_inhib_connectivity_pool0_to_pool1':\n",
    "            disyn[0, 1] = float(val); disyn_touched = True; continue\n",
    "        if name == 'disynaptic_inhib_connectivity_pool1_to_pool0':\n",
    "            disyn[1, 0] = float(val); disyn_touched = True; continue\n",
    "\n",
    "        # 6) Plain assignment only if it's an init=True field\n",
    "        if name in init_field_names:\n",
    "            init_kwargs[name] = val\n",
    "            continue\n",
    "\n",
    "        # Otherwise ignore quietly or raise (your choice)\n",
    "        # raise KeyError(f\"Unrecognized or non-init parameter '{name}'\")\n",
    "\n",
    "    # Write back the disynaptic matrix if touched\n",
    "    if disyn_touched:\n",
    "        init_kwargs['disynpatic_inhib_connections_desired_MN_MN'] = disyn\n",
    "\n",
    "    # Always override the output folder\n",
    "    init_kwargs['output_folder_name'] = batch_name\n",
    "\n",
    "    # FINAL GUARD: drop any accidental non-init fields before constructing\n",
    "    init_kwargs = {k: v for k, v in init_kwargs.items() if k in init_field_names}\n",
    "\n",
    "    # Build the dataclass (runs __post_init__)\n",
    "    return SimulationParameters(**init_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62917517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_values_filled(d):\n",
    "    for v in d.values():\n",
    "        if isinstance(v, dict):\n",
    "            if not all_values_filled(v):\n",
    "                return False\n",
    "        else:\n",
    "            if v is None:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def build_posterior_predictive_check_simulation_parameters_two_pools(\n",
    "    fixed_parameters,\n",
    "    free_priors: dict,\n",
    "    batch_name: str,\n",
    "    baseline_jitter: float = 0.0,   # like your baseline_excit_input_to_add_to_posterior_sample\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a SimulationParameters for 2 pools using expanded variable names.\n",
    "\n",
    "    Accepts keys like:\n",
    "      - excitatory_input_baseline_pool0 / _pool1\n",
    "      - common_input_high_freq_middle_of_range_pool0/_pool1\n",
    "      - common_input_high_freq_half_width_range_pool0/_pool1\n",
    "      - common_input_std_pool0/_pool1        (assumed to map to input #1 == 'second' input)\n",
    "      - disynaptic_inhib_*                    (pool0_to_pool1, pool1_to_pool0, self_connectivity_pool*)\n",
    "      - between_pool_excitatory_input_correlation (direct field if present)\n",
    "\n",
    "    Also supports legacy aliases if they sneak in:\n",
    "      - std_of_second_common_input_pool0/_pool1  -> common_input_std_pool*\n",
    "    \"\"\"\n",
    "    default = SimulationParameters()\n",
    "    init_field_names = {f.name for f in fields(SimulationParameters) if f.init}\n",
    "    non_init_field_names = {f.name for f in fields(SimulationParameters) if not f.init}\n",
    "\n",
    "    fixed_params = asdict(fixed_parameters).copy()\n",
    "    for k in list(fixed_params.keys()):\n",
    "        if k in non_init_field_names:\n",
    "            fixed_params.pop(k)\n",
    "\n",
    "    # normalize aliases on free side (be liberal in what we accept)\n",
    "    free_params = free_priors.copy()\n",
    "    alias_map = {\n",
    "        \"std_of_second_common_input_pool0\": \"common_input_std_pool0\",\n",
    "        \"std_of_second_common_input_pool1\": \"common_input_std_pool1\",\n",
    "    }\n",
    "    for old, new in alias_map.items():\n",
    "        if old in free_params and new not in free_params:\n",
    "            free_params[new] = free_params.pop(old)\n",
    "\n",
    "    # merge (free overrides fixed)\n",
    "    all_priors = {**fixed_params, **free_params}\n",
    "\n",
    "    init_kwargs = {}\n",
    "\n",
    "    # stage multi-field buffers\n",
    "    # common_input_std is [pool, input] -> we'll write input #1 (\"second\") here\n",
    "    cis = deepcopy(default.common_input_std)\n",
    "    cis_touched = False\n",
    "\n",
    "    # 2x2 disynaptic matrix\n",
    "    disyn = deepcopy(default.disynpatic_inhib_connections_desired_MN_MN)\n",
    "    disyn_touched = False\n",
    "\n",
    "    # frequency ranges of common input: we’ll set input#1 band per pool when both middle & half_width are provided\n",
    "    hf = {\n",
    "        0: {\"middle_of_range\": None, \"half_width\": None},\n",
    "        1: {\"middle_of_range\": None, \"half_width\": None},\n",
    "    }\n",
    "    freq_range = deepcopy(default.frequency_range_of_common_input)  # shape [pool, input, 2]\n",
    "\n",
    "    # iterate once\n",
    "    for name, val in all_priors.items():\n",
    "        # 0) unit-aware special setters\n",
    "        if name in _special_setters:\n",
    "            _special_setters[name](val, init_kwargs, default)\n",
    "            continue\n",
    "\n",
    "        # 1) excitatory baseline per pool\n",
    "        if name.startswith(\"excitatory_input_baseline_pool\"):\n",
    "            pool = 0 if name.endswith(\"pool0\") else 1\n",
    "            arr = deepcopy(init_kwargs.get(\"excitatory_input_baseline\", default.excitatory_input_baseline))\n",
    "            # add optional small jitter like your training script\n",
    "            jitter = np.random.uniform(0, baseline_jitter) if baseline_jitter > 0 else 0.0\n",
    "            arr[pool] = float(val) + jitter\n",
    "            init_kwargs[\"excitatory_input_baseline\"] = arr\n",
    "            continue\n",
    "\n",
    "        # 2) high-frequency common input band per pool (input index = 1)\n",
    "        if name.startswith(\"common_input_high_freq_middle_of_range_pool\"):\n",
    "            pool = 0 if name.endswith(\"pool0\") else 1\n",
    "            hf[pool][\"middle_of_range\"] = float(val)\n",
    "            continue\n",
    "        if name.startswith(\"common_input_high_freq_half_width_range_pool\"):\n",
    "            pool = 0 if name.endswith(\"pool0\") else 1\n",
    "            hf[pool][\"half_width\"] = float(val)\n",
    "            continue\n",
    "\n",
    "        # 3) common-input std (second input, per pool)\n",
    "        if name in (\"common_input_std_pool0\", \"common_input_std_pool1\"):\n",
    "            pool = 0 if name.endswith(\"pool0\") else 1\n",
    "            cis[pool, 1] = float(val)\n",
    "            cis_touched = True\n",
    "            continue\n",
    "\n",
    "        # 4) disynaptic connectivity entries\n",
    "        if name == \"disynaptic_inhib_self_connectivity_pool0\":\n",
    "            disyn[0, 0] = float(val); disyn_touched = True; continue\n",
    "        if name == \"disynaptic_inhib_self_connectivity_pool1\":\n",
    "            disyn[1, 1] = float(val); disyn_touched = True; continue\n",
    "        if name == \"disynaptic_inhib_connectivity_pool0_to_pool1\":\n",
    "            disyn[0, 1] = float(val); disyn_touched = True; continue\n",
    "        if name == \"disynaptic_inhib_connectivity_pool1_to_pool0\":\n",
    "            disyn[1, 0] = float(val); disyn_touched = True; continue\n",
    "\n",
    "        # 5) plain assignment if it’s a real dataclass field (e.g., between_pool_excitatory_input_correlation)\n",
    "        if name in init_field_names:\n",
    "            init_kwargs[name] = val\n",
    "            continue\n",
    "\n",
    "        # otherwise ignore silently (keeps it robust to stray keys)\n",
    "        # raise KeyError(f\"Unrecognized parameter '{name}'\")\n",
    "\n",
    "    # write frequency bands if both parts present\n",
    "    for pool in (0, 1):\n",
    "        if all_values_filled(hf[pool]):\n",
    "            mid  = hf[pool][\"middle_of_range\"]\n",
    "            half = hf[pool][\"half_width\"]\n",
    "            # input index 1\n",
    "            freq_range[pool, 1, 0] = mid - half\n",
    "            freq_range[pool, 1, 1] = mid + half\n",
    "    init_kwargs[\"frequency_range_of_common_input\"] = freq_range\n",
    "\n",
    "    if cis_touched:\n",
    "        init_kwargs[\"common_input_std\"] = cis\n",
    "    if disyn_touched:\n",
    "        init_kwargs[\"disynpatic_inhib_connections_desired_MN_MN\"] = disyn\n",
    "\n",
    "    # output folder\n",
    "    init_kwargs[\"output_folder_name\"] = batch_name\n",
    "\n",
    "    # final guard: keep only init=True fields\n",
    "    init_kwargs = {k: v for k, v in init_kwargs.items() if k in init_field_names}\n",
    "\n",
    "    return SimulationParameters(**init_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Utility/Helper functions to create the new folders for the posterior predictive checks simulations\n",
    "import re\n",
    "import numbers\n",
    "\n",
    "def sanitize_str(s: str) -> str:\n",
    "    # Replace any Windows‑illegal filename chars with '-'\n",
    "    return re.sub(r'[<>:\"/\\\\|?*\\s]+', '', s).strip('-.')\n",
    "\n",
    "def tuple_to_folder_name(tup):\n",
    "    parts = []\n",
    "    for x in tup:\n",
    "        if isinstance(x, numbers.Number):\n",
    "            # if it’s an integer-valued float, drop the .0\n",
    "            if float(x).is_integer():\n",
    "                parts.append(str(int(x)))\n",
    "            else:\n",
    "                parts.append(str(x))\n",
    "        else:\n",
    "            parts.append(sanitize_str(str(x)))\n",
    "    # join with underscores\n",
    "    return \"_\".join(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper used in the cell below if sanity_check_plots == True and muscle_pairs_posterior_predictive_checks == True\n",
    "def data_limits(X, pad_frac: float = 0.05):\n",
    "    \"\"\"\n",
    "    X: (N, D) torch.Tensor or np.ndarray\n",
    "    returns: list of [low, high] for each dim, with padding and degenerate-interval guard\n",
    "    \"\"\"\n",
    "    X = X.detach().cpu().numpy() if hasattr(X, \"detach\") else np.asarray(X)\n",
    "    mins = np.nanmin(X, axis=0)\n",
    "    maxs = np.nanmax(X, axis=0)\n",
    "\n",
    "    lims = []\n",
    "    for lo, hi in zip(mins, maxs):\n",
    "        if not np.isfinite(lo) or not np.isfinite(hi):\n",
    "            lo, hi = 0.0, 1.0\n",
    "        if hi == lo:  # degenerate -> widen a hair\n",
    "            width = 1.0 if lo == 0 else abs(lo) * 1e-3\n",
    "            lo -= width; hi += width\n",
    "        pad = pad_frac * (hi - lo)\n",
    "        lims.append([lo - pad, hi + pad])\n",
    "    return lims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29585f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_posterior_predictive_sims_param = {}\n",
    "dict_of_posterior_predictive_sims_params_with_highest_likelihood = {}\n",
    "sanity_check_plots = True  # True\n",
    "folder_path_highest_likelihood = f\"{path_to_save_posterior_predictive_check_sims}\\\\_highest_likelihood_sims\"\n",
    "os.makedirs(folder_path_highest_likelihood, exist_ok=True)\n",
    "\n",
    "for current_condition, posterior_samples_current in posterior_samples_to_draw_posterior_predictive_checks_sim_param_from['posterior_samples'].items():\n",
    "    current_path = f\"{path_to_save_posterior_predictive_check_sims}\\\\{tuple_to_folder_name(current_condition)}\"\n",
    "    os.makedirs(current_path, exist_ok=True)\n",
    "    current_path_highest_likelihood = f\"{folder_path_highest_likelihood}\\\\{tuple_to_folder_name(current_condition)}\"\n",
    "    os.makedirs(current_path_highest_likelihood, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading posterior predictive checks parameters (samples on the inferred posterior) for simulation {current_condition}\")\n",
    "\n",
    "    # Keep the original tensor for indexing / .item()\n",
    "    posterior_samples_tensor = posterior_samples_current\n",
    "\n",
    "    # Safe NumPy version for sbi.pairplot / data_limits (avoids .numpy())\n",
    "    posterior_samples_np = to_np(posterior_samples_tensor)   # <--- key line\n",
    "\n",
    "    N = posterior_samples_tensor.shape[0]\n",
    "    g = torch.Generator().manual_seed(42)  # reproducible; remove for random each run\n",
    "\n",
    "    # choose indices (without replacement if possible; with replacement otherwise)\n",
    "    if n_sims_per_condition <= N:\n",
    "        choice_idx = torch.randperm(N, generator=g)[:n_sims_per_condition]\n",
    "    else:\n",
    "        choice_idx = torch.randint(N, (n_sims_per_condition,), generator=g)\n",
    "\n",
    "    print(\n",
    "        f\"    Total posterior samples: {posterior_samples_tensor.shape} ([samples, n_parameters]).\\n\"\n",
    "        f\"    {n_sims_per_condition} randomly selected samples will be simulated.\\n \"\n",
    "    )\n",
    "\n",
    "    if sanity_check_plots:\n",
    "        muscle_pair_i = current_condition[0]\n",
    "\n",
    "        if not muscle_pairs_posterior_predictive_checks:\n",
    "            fig, axes = pairplot(\n",
    "                posterior_samples_np,  # <--- use NumPy\n",
    "                limits=[[low_original[i].item(), high_original[i].item()] for i in range(n_dim)],\n",
    "                diag_kwargs={\"mpl_kwargs\": {\"color\": muscle_colors_dict[muscle_pair_i], \"linewidth\": 2}},\n",
    "                upper_kwargs={\"mpl_kwargs\": {\"cmap\": muscle_colormaps_dict[muscle_pair_i]}},\n",
    "                labels=variable_parameters,\n",
    "                figsize=(3 * posterior_samples_np.shape[1], 3 * posterior_samples_np.shape[1]),\n",
    "            )\n",
    "        else:\n",
    "            # Also compute limits from NumPy, not torch\n",
    "            limits = data_limits(posterior_samples_np, pad_frac=0.05)\n",
    "            fig, axes = pairplot(\n",
    "                posterior_samples_np,  # <--- use NumPy\n",
    "                limits=limits,\n",
    "                diag_kwargs={\"mpl_kwargs\": {\"color\": muscle_colors_dict[muscle_pair_i], \"linewidth\": 2}},\n",
    "                upper_kwargs={\"mpl_kwargs\": {\"cmap\": muscle_colormaps_dict[muscle_pair_i]}},\n",
    "                labels=variable_parameters,\n",
    "                figsize=(3 * posterior_samples_np.shape[1], 3 * posterior_samples_np.shape[1]),\n",
    "            )\n",
    "\n",
    "        plt.suptitle(f\"{current_condition}\\nDraws from posterior for posterior predictive checks simulations\")\n",
    "\n",
    "    # Initialize dict entry\n",
    "    dict_of_posterior_predictive_sims_param[current_condition] = []\n",
    "\n",
    "    # Best-parameter combination (highest likelihood)\n",
    "    free_params_with_highest_likelihood_for_current_condition = {}\n",
    "    for param_idx, param_val in enumerate(\n",
    "        posterior_samples_to_draw_posterior_predictive_checks_sim_param_from['best_samples'][current_condition]\n",
    "    ):\n",
    "        free_params_with_highest_likelihood_for_current_condition[variable_parameters[param_idx]] = param_val.item()\n",
    "\n",
    "    if not muscle_pairs_posterior_predictive_checks:  # single pair case\n",
    "        dict_of_posterior_predictive_sims_params_with_highest_likelihood[current_condition] = (\n",
    "            build_posterior_predictive_check_simulation_parameters(\n",
    "                fixed_parameters,\n",
    "                free_params_with_highest_likelihood_for_current_condition,\n",
    "                current_path_highest_likelihood,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        dict_of_posterior_predictive_sims_params_with_highest_likelihood[current_condition] = (\n",
    "            build_posterior_predictive_check_simulation_parameters_two_pools(\n",
    "                fixed_parameters,\n",
    "                free_params_with_highest_likelihood_for_current_condition,\n",
    "                current_path_highest_likelihood,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    dict_of_posterior_predictive_sims_params_with_highest_likelihood[current_condition].output_plots = True\n",
    "\n",
    "    # Draw individual posterior samples for simulations\n",
    "    for posterior_sample_draw_idx in choice_idx.tolist():\n",
    "        draw_i = posterior_samples_tensor[posterior_sample_draw_idx]  # still a torch row\n",
    "\n",
    "        free_parameters = {}\n",
    "        for param_i_idx, param_i_val in enumerate(draw_i):\n",
    "            param_i_val = param_i_val.item()  # scalar float\n",
    "            free_parameters[variable_parameters[param_i_idx]] = param_i_val\n",
    "\n",
    "            if sanity_check_plots:\n",
    "                # diagonal histograms: vertical line for each draw\n",
    "                axes[param_i_idx, param_i_idx].axvline(\n",
    "                    param_i_val, linestyle=\"-\", linewidth=2, color=\"k\", alpha=0.1, zorder=-1\n",
    "                )\n",
    "\n",
    "                # specifically highlight the \"best\" draw once\n",
    "                if posterior_sample_draw_idx == 0:\n",
    "                    param_i_highest_logp_val = (\n",
    "                        posterior_samples_to_draw_posterior_predictive_checks_sim_param_from['best_samples']\n",
    "                        [current_condition][param_i_idx]\n",
    "                    )\n",
    "                    axes[param_i_idx, param_i_idx].axvline(\n",
    "                        param_i_highest_logp_val, linestyle=\"-\", linewidth=1.5, color=\"r\", alpha=1, zorder=10\n",
    "                    )\n",
    "\n",
    "                # off-diagonal scatter\n",
    "                for param_j_idx, param_j_val in enumerate(draw_i):\n",
    "                    if param_j_idx <= param_i_idx:\n",
    "                        continue\n",
    "                    param_j_val = param_j_val.item()\n",
    "                    axes[param_i_idx, param_j_idx].scatter(\n",
    "                        x=param_j_val,\n",
    "                        y=param_i_val,\n",
    "                        s=30,\n",
    "                        color=\"#FFFFFF\",\n",
    "                        edgecolor=\"k\",\n",
    "                        alpha=0.7,\n",
    "                    )\n",
    "                    if posterior_sample_draw_idx == 0:\n",
    "                        param_j_highest_logp_val = (\n",
    "                            posterior_samples_to_draw_posterior_predictive_checks_sim_param_from['best_samples']\n",
    "                            [current_condition][param_j_idx]\n",
    "                        )\n",
    "                        param_i_highest_logp_val = (\n",
    "                            posterior_samples_to_draw_posterior_predictive_checks_sim_param_from['best_samples']\n",
    "                            [current_condition][param_i_idx]\n",
    "                        )\n",
    "                        axes[param_i_idx, param_j_idx].scatter(\n",
    "                            x=param_j_highest_logp_val,\n",
    "                            y=param_i_highest_logp_val,\n",
    "                            s=100,\n",
    "                            marker=\"X\",\n",
    "                            facecolor=\"#FFFFFF\",\n",
    "                            edgecolor=\"r\",\n",
    "                            linewidth=1,\n",
    "                            alpha=1,\n",
    "                            zorder=10,\n",
    "                        )\n",
    "\n",
    "        # Fill dict entry iteratively\n",
    "        if not muscle_pairs_posterior_predictive_checks:  # single pair case\n",
    "            dict_of_posterior_predictive_sims_param[current_condition].append(\n",
    "                build_posterior_predictive_check_simulation_parameters(\n",
    "                    fixed_parameters, free_parameters, current_path\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            dict_of_posterior_predictive_sims_param[current_condition].append(\n",
    "                build_posterior_predictive_check_simulation_parameters_two_pools(\n",
    "                    fixed_parameters, free_parameters, current_path\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Set plotting to true\n",
    "        dict_of_posterior_predictive_sims_param[current_condition][-1].output_plots = True\n",
    "\n",
    "    if sanity_check_plots:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f\"{path_to_save_posterior_predictive_check_sims}\\\\{tuple_to_folder_name(current_condition)}_pairplot_of_posterior_samples_simulated.png\",\n",
    "            dpi=300,\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "flattened_list_of_posterior_predictive_sims_param = [\n",
    "    item for sublist in dict_of_posterior_predictive_sims_param.values() for item in sublist\n",
    "]\n",
    "flattened_list_of_highest_likelihood_predictive_sims_param = list(\n",
    "    dict_of_posterior_predictive_sims_params_with_highest_likelihood.values()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARALLELIZATION\n",
    "\n",
    "# # # Function to make sure to terminate any Python process that runs in the background (this can happen when the kernel crashes during the parallelized computations)\n",
    "def kill_other_python_processes():\n",
    "    me = os.getpid()\n",
    "    user = getpass.getuser()\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'username']):\n",
    "        try:\n",
    "            # only consider Python executables run by this user\n",
    "            if proc.info['username'] != user:\n",
    "                continue\n",
    "            name = proc.info['name'].lower()\n",
    "            # match python, pythonw, python3, etc\n",
    "            if name.startswith('python'):\n",
    "                pid = proc.info['pid']\n",
    "                if pid != me:\n",
    "                    proc.kill()   # or proc.terminate()\n",
    "                    print(f\"{name} process terminated\")\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "            pass\n",
    "if __name__ == '__main__':\n",
    "    kill_other_python_processes()\n",
    "    # now safe to start joblib Parallel(...)\n",
    "\n",
    "# --- Helper to wrap a single simulation and then reset Brian2 ---\n",
    "def _run_and_reset(params):\n",
    "    # params must be a SimulationParameters instance\n",
    "    out = run_simulation(params)\n",
    "    # after each run, reset Brian2’s magic network so the next worker starts fresh\n",
    "    device.reinit()     # clears all Brian2 objects\n",
    "    device.activate()   # re–activate the default runtime device\n",
    "    return out\n",
    "\n",
    "# # # PARALLEL SIMULATIONS\n",
    "def parallel_simulate(params_prior_list, n_jobs=8):\n",
    "    \"\"\"\n",
    "    params_prior_list : list of SimulationParameters\n",
    "    n_jobs      :       number of parallel workers\n",
    "    \"\"\"\n",
    "    # Note: `prefer=\"processes\"` is the default for `n_jobs>1`\n",
    "    sim_outputs = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(_run_and_reset)(p) for p in params_prior_list)\n",
    "    return sim_outputs\n",
    "\n",
    "_tail_thread = None\n",
    "_tail_stop = threading.Event()\n",
    "\n",
    "def start_tail(logfile=\"simulations_progress_log.log\", poll_interval=1.0):\n",
    "    \"\"\"\n",
    "    Spawn a thread that prints only the log‐lines whose timestamp\n",
    "    is ≥ the moment you called start_tail(), and strips off everything\n",
    "    before the log‐level (INFO:, WARNING:, ERROR:, etc.).\n",
    "    \"\"\"\n",
    "    global _tail_thread, _tail_stop\n",
    "\n",
    "    # make sure the file exists (touch it)\n",
    "    open(logfile, \"a\").close()\n",
    "\n",
    "    # remember \"now\" and clear any previous stop flag\n",
    "    start_dt = datetime.now()\n",
    "    _tail_stop.clear()\n",
    "\n",
    "    def _tail_loop():\n",
    "        level_re = re.compile(r'\\b(?:DEBUG|INFO|WARNING|ERROR|CRITICAL)\\b:\\s*')\n",
    "        with open(logfile, \"r\", encoding=\"utf-8\") as f:\n",
    "            # seek to end: we only want new lines\n",
    "            f.seek(0, 2)\n",
    "            while not _tail_stop.is_set():\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    time.sleep(poll_interval)\n",
    "                    continue\n",
    "\n",
    "                # try to parse timestamp at the very start\n",
    "                try:\n",
    "                    ts_str = \" \".join(line.split(\" \")[:2]).rstrip(\",\")\n",
    "                    ts = datetime.strptime(ts_str, \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "                except Exception:\n",
    "                    ts = start_dt  # force print for non‐timestamped lines\n",
    "\n",
    "                if ts >= start_dt:\n",
    "                    # strip off everything before the level marker\n",
    "                    m = level_re.search(line)\n",
    "                    if m:\n",
    "                        print(line[m.start():], end=\"\")\n",
    "                    else:\n",
    "                        print(line, end=\"\")\n",
    "\n",
    "    # fire up the thread (only one at a time)\n",
    "    if _tail_thread is None or not _tail_thread.is_alive():\n",
    "        _tail_thread = threading.Thread(target=_tail_loop, daemon=True)\n",
    "        _tail_thread.start()\n",
    "    else:\n",
    "        print(\"Tail already running; call `end_tail()` first if you want to restart.\")\n",
    "\n",
    "def stop_tail():\n",
    "    \"\"\"Stop the background tail thread.\"\"\"\n",
    "    _tail_stop.set()\n",
    "    if _tail_thread:\n",
    "        _tail_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cda52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parallel simulations - N iterations for each posterior predictive check sample drawn\n",
    "print(f\"Simulating {len(flattened_list_of_posterior_predictive_sims_param)} posterior predictive checks - simulations with parameters drawn from inferred posteriors\\n\")\n",
    "stop_tail() # just in case one is already running\n",
    "start_tail()\n",
    "simulation_output_files = parallel_simulate(flattened_list_of_posterior_predictive_sims_param, n_jobs=sim_parallel_cpus)\n",
    "time.sleep(1)  # give it a moment to print the last lines\n",
    "stop_tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb88e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parallel simulations - Best posterior estimate of sim parameters (highest likelihood) for each condition\n",
    "print(f\"Simulating {len(flattened_list_of_highest_likelihood_predictive_sims_param)} sims - one for each condition. Sims are the parameter combinations with the highest likelihood from the posterior\\n\")\n",
    "stop_tail() # just in case one is already running\n",
    "start_tail()\n",
    "simulation_output_files = parallel_simulate(flattened_list_of_highest_likelihood_predictive_sims_param, n_jobs=sim_parallel_cpus)\n",
    "time.sleep(1)  # give it a moment to print the last lines\n",
    "stop_tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529972e",
   "metadata": {},
   "source": [
    "################################################################################################################################################\n",
    "\n",
    "From there, the analyzes of the posterior-generated simulations can be performed directly with the ipynb notebook \"analyze_batch.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapping_RI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
