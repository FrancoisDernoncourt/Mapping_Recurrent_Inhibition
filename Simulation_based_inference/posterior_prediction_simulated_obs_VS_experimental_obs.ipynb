{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafdd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "import cmasher as cmr\n",
    "import warnings\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import gc\n",
    "import re\n",
    "import math\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_experimental_obs = f\"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\SBI_motor_neuron_behavior\\\\_experimental_data\"\n",
    "csv_experimental_obs = f\"experimental_data_dataframe_reorganized_and_filtered_dir_inhibited_persp_other_MUs_as_ref.csv\"\n",
    "\n",
    "# To be changed to the folders where you created (or downloaded from the repository) the posterior-predicted simulations\n",
    "path_posterior_sampled_obs_single_muscle = f\"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\SBI_motor_neuron_behavior\\\\$$$_Simulation_batch_single_muscle\\\\Z_inference\\\\inference_model_2\\\\posterior_predictive_checks_subjects_grouped\"\n",
    "path_subfolder_highest_likelihood_posterior_sample_obs_single_muscle = f\"_highest_likelihood_sims\"\n",
    "\n",
    "path_posterior_sampled_obs_paired_muscles = f\"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\SBI_motor_neuron_behavior\\\\$$$_Simulation_batches_muscle_pairs\\\\Z_inference\\\\inference_model_0\\\\posterior_predictive_checks_subjects_grouped\"\n",
    "path_subfolder_highest_likelihood_posterior_sample_obs_paired_muscles = f\"_highest_likelihood_sims\"\n",
    "\n",
    "# To be changed to the folder in which the general analyses (.csv) have been saved for the training data (prior-sampled simulations)\n",
    "path_prior_samples_obs_single_muscle = f\"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\SBI_motor_neuron_behavior\\\\$$$_Simulation_batch_single_muscle\" # those simulations serve as the \"null\" predictions of the neuron model (random sampling of the parameter space, without inference)\n",
    "path_prior_samples_obs_paired_muscles = f\"C:\\\\Users\\\\franc\\\\Documents\\\\GitHub\\\\SBI_motor_neuron_behavior\\\\$$$_Simulation_batches_muscle_pairs\" # those simulations serve as the \"null\" predictions of the neuron model (random sampling of the parameter space, without inference)\n",
    "csv_prior_samples_obs = f\"___general_analysis_of_simulations.csv\" # those simulations serve as the \"null\" predictions of the neuron model (random sampling of the parameter space, without inference)\n",
    "\n",
    "path_to_save_into = f\"validation_posterior_predictive_checks\"\n",
    "os.makedirs(path_to_save_into, exist_ok=True)\n",
    "\n",
    "observations = [\n",
    "    \"trough_area\",\n",
    "    \"peak_height\",\n",
    "    \"firing_rate\",\n",
    "    \"IPSP_delay\"\n",
    "]\n",
    "\n",
    "perspective = 'other_MUs_as_ref' # choose from {'other_MUs_as_ref','MU_as_ref','combined'}\n",
    "direction   = 'inhibited' # choose from {'inhibited','inhibiting'}\n",
    "\n",
    "pool_pair_name = {\n",
    "    \"within_muscle\": [\"pool_0<->pool_0\"],\n",
    "    \"between_muscles\": [\"pool_0<->pool_1\"] # The loaded data frames have already made the duplications to get both AA->BB and BB->AA, so just need to select a the baseline direction (from AA to BB) here\n",
    "} \n",
    "\n",
    "summarize_over = [\n",
    "    # \"subject\", # comment out if SBI on pooled subjects\n",
    "    \"muscle_pair\",\n",
    "    \"intensity\",\n",
    "    \"sim_idx\", # used only for the simulations\n",
    "]\n",
    "\n",
    "# Filtering\n",
    "min_r2_for_baseline_curve_fit = {\"simulation\":0.1,\n",
    "                                 \"experiment\": 0.1}\n",
    "min_r2_for_overall_curve_fit = {\"simulation\":0.75,\n",
    "                                 \"experiment\": 0.75}\n",
    "min_nb_spikes = {\"simulation\":10_000,\n",
    "                \"experiment\": 5_000}\n",
    "\n",
    "colors_dict = {\n",
    "  \"VL<->VL\": \"#D62728\",\n",
    "  \"VL<->VM\": \"#FF9201\",\n",
    "  \"VM<->VL\": \"#FF9201\",\n",
    "  \"VM<->VM\": \"#FFC400\",\n",
    "  \"TA<->TA\": \"#00C71B\",\n",
    "  \"FDI<->FDI\": \"#14BFA8\",\n",
    "  \"GM<->GM\": \"#2489DC\",\n",
    "  \"GM<->SOL\": \"#7D74EC\",\n",
    "  \"SOL<->GM\": \"#7D74EC\",\n",
    "  \"SOL<->SOL\": \"#BB86ED\",\n",
    "\n",
    "  \"VL\": \"#D62728\",\n",
    "  \"VM\": \"#FFC400\",\n",
    "  \"TA\": \"#00C71B\",\n",
    "  \"FDI\": \"#14BFA8\",\n",
    "  \"GM\": \"#2489DC\",\n",
    "  \"SOL\": \"#BB86ED\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a651d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_analyses(\n",
    "    base_folder,\n",
    "    filename=\"analysis_output.pkl\",\n",
    "    params_file=\"sim_parameters.json\",\n",
    "    best_subfolder_name=\"_highest_likelihood_sims\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Walk each condition subfolder of `base_folder`, loading every\n",
    "    `filename` it finds in its child subfolders into two dicts:\n",
    "      - all_sims: keyed by \"<condition>_sim<N>\"\n",
    "      - best_sims: keyed by \"<condition>_simHighestLikelihood\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_sims : dict[str, dict]\n",
    "       mapping each \"<condition>_sim<N>\" → loaded pickle dict (with 'sim_parameters' added)\n",
    "    best_sims : dict[str, dict]\n",
    "       mapping each \"<condition>_simHighestLikelihood\" → loaded pickle dict\n",
    "    num_conditions : int\n",
    "       number of immediate subdirectories (conditions) found in `base_folder`\n",
    "    \"\"\"\n",
    "    base = Path(base_folder)\n",
    "\n",
    "    # find all immediate subdirectories\n",
    "    condition_dirs = [d for d in base.iterdir() if d.is_dir()]\n",
    "    num_conditions = len(condition_dirs)\n",
    "\n",
    "    all_sims = {}\n",
    "    best_sims = {}\n",
    "\n",
    "    for cond_dir in condition_dirs:\n",
    "        condition = cond_dir.name\n",
    "        if cond_dir.name == best_subfolder_name:\n",
    "            continue\n",
    "\n",
    "        # 1) load all the regular sims\n",
    "        sim_dirs = [d for d in cond_dir.iterdir() if (d / filename).is_file()]\n",
    "        for idx, sim_dir in enumerate(sorted(sim_dirs), start=1):\n",
    "            key = f\"{condition}_sim{idx}\"\n",
    "            pkl_path = sim_dir / filename\n",
    "\n",
    "            # load pickle\n",
    "            with open(pkl_path, \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "            # load params JSON if present\n",
    "            params_path = sim_dir / params_file\n",
    "            if params_path.exists():\n",
    "                with open(params_path, \"r\", encoding=\"utf-8\") as jf:\n",
    "                    data[\"sim_parameters\"] = json.load(jf)\n",
    "            else:\n",
    "                data[\"sim_parameters\"] = None\n",
    "            data['condition'] = condition\n",
    "            data['sim_type'] = 'posterior_sample'\n",
    "\n",
    "            all_sims[key] = data\n",
    "\n",
    "    # 2) load the best‐likelihood sim(s) from the special subfolder\n",
    "    best_dir = Path(os.path.join(base_folder, best_subfolder_name))\n",
    "    if best_dir.is_dir():\n",
    "        num_conditions -= 1\n",
    "        best_dirs_current_cond = [d for d in best_dir.iterdir() if d.is_dir()]\n",
    "        for best_dir_current_cond in best_dirs_current_cond:\n",
    "            condition = best_dir_current_cond.name\n",
    "            sim_dirs = [d for d in best_dir_current_cond.iterdir() if (d / filename).is_file()]\n",
    "            for idx, sim_dir in enumerate(sorted(sim_dirs), start=1):\n",
    "                # print(sim_dir)\n",
    "                for pkl_path in sim_dir.glob(f\"*{filename}\"):\n",
    "                    key = f\"{condition}_simHighestLikelihood\"\n",
    "                    with open(pkl_path, \"rb\") as f:\n",
    "                        data = pickle.load(f)\n",
    "                    params_path = pkl_path.parent / params_file\n",
    "                    if params_path.exists():\n",
    "                        with open(params_path, \"r\", encoding=\"utf-8\") as jf:\n",
    "                            data[\"sim_parameters\"] = json.load(jf)\n",
    "                    else:\n",
    "                        data[\"sim_parameters\"] = None\n",
    "                    data['condition'] = condition\n",
    "                    data['sim_type'] = 'posterior_highest_likelihood_sample'\n",
    "                    best_sims[key] = data\n",
    "\n",
    "    return all_sims, best_sims, num_conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD SIMULATED OBSERVATIONS \n",
    "all_sims_single_muscle, best_sims_single_muscles, n_conditions = load_analyses(path_posterior_sampled_obs_single_muscle,\n",
    "                                    best_subfolder_name=path_subfolder_highest_likelihood_posterior_sample_obs_single_muscle)\n",
    "print(\"SINGLE MUSCLE SIMULATIONS:\")\n",
    "print(f\"    Simulations from posterior samples: {len(all_sims_single_muscle)} entries, from {n_conditions} conditions\")\n",
    "print(f\"    Simulations from single posterior sample wih highest likelihood: {len(best_sims_single_muscles)} entries (one for each of the {n_conditions} conditions)\")\n",
    "\n",
    "all_sims_muscle_pair, best_sims_muscle_pair, n_conditions = load_analyses(path_posterior_sampled_obs_paired_muscles,\n",
    "                                    best_subfolder_name=path_subfolder_highest_likelihood_posterior_sample_obs_paired_muscles)\n",
    "print(\"MUSCLE PAIRS SIMULATIONS:\")\n",
    "print(f\"    Simulations from posterior samples: {len(all_sims_muscle_pair)} entries, from {n_conditions} conditions\")\n",
    "print(f\"    Simulations from single posterior sample wih highest likelihood: {len(best_sims_muscle_pair)} entries (one for each of the {n_conditions} conditions)\")\n",
    "\n",
    "# Merge into one big dict with all sims\n",
    "all_sims = all_sims_single_muscle.copy()\n",
    "all_sims.update(all_sims_muscle_pair)\n",
    "best_sims = best_sims_single_muscles.copy()\n",
    "best_sims.update(best_sims_muscle_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"POSTERIOR-PREDICTIVE CHECKS - CONDITIONS LOADED (TOTAL):\")\n",
    "print(best_sims.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TURN THE SIMULATED OBSERVATIONS INTO A DATAFRAME (1 row = 1 motor unit)\n",
    "\n",
    "# pre-compile\n",
    "_subject_re     = re.compile(r'^([A-Za-z]{4})')\n",
    "_muscle_pair_re = re.compile(r'([A-Za-z]+-[A-Za-z]+)')\n",
    "_intensity_re   = re.compile(r'_(\\d+)$')\n",
    "_sim_idx_re     = re.compile(r\"_sim(\\d+)$\")\n",
    "\n",
    "def sims_dict_to_dataframe(\n",
    "    sims_dict,\n",
    "    perspective,\n",
    "    direction,\n",
    "    observations,\n",
    "    pool_pair_name={  # mapping of case -> allowed pool pairs\n",
    "        \"within_muscle\": [\"pool_0<->pool_0\"],\n",
    "        \"between_muscles\": [\"pool_0<->pool_1\"],\n",
    "    },\n",
    "):\n",
    "    # which cross-histogram branch to read\n",
    "    key_to_load = \"inhibited\" if perspective == \"other_MUs_as_ref\" else \"inhibiting\"\n",
    "    subkey_to_load = (\n",
    "        \"forward\" if (direction == \"inhibited\") ^ (perspective == \"other_MUs_as_ref\") else \"backward\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for sim_key, sim_data in sims_dict.items():\n",
    "        # sim index\n",
    "        m = _sim_idx_re.search(sim_key)\n",
    "        sim_idx = int(m.group(1)) if m else None\n",
    "\n",
    "        condition = sim_data[\"condition\"]\n",
    "\n",
    "        # subject (fallback if missing)\n",
    "        m_sub = _subject_re.match(condition)\n",
    "        subject = m_sub.group(1) if m_sub else \"pooled_subjects\"\n",
    "\n",
    "        # muscle pair like 'AA-BB' -> 'AA<->BB'\n",
    "        m_pair = _muscle_pair_re.search(condition)\n",
    "        muscle_pair = m_pair.group(1) if m_pair else None  # 'AA-BB'\n",
    "        muscle_pair_arrow = muscle_pair.replace(\"-\", \"<->\") if muscle_pair else None  # 'AA<->BB'\n",
    "\n",
    "        # intensity\n",
    "        m_int = _intensity_re.search(condition)\n",
    "        intensity = int(m_int.group(1)) if m_int else None\n",
    "\n",
    "        # decide within vs between\n",
    "        if muscle_pair_arrow and \"<->\" in muscle_pair_arrow:\n",
    "            a, b = muscle_pair_arrow.split(\"<->\")\n",
    "            case = \"within_muscle\" if a == b else \"between_muscles\"\n",
    "        else:\n",
    "            # fallback: infer from keys present in Cross_histograms\n",
    "            ch_keys = set(sim_data.get(\"Cross_histograms\", {}).keys())\n",
    "            has_between = (\"pool_0<->pool_1\" in ch_keys) or (\"pool_1<->pool_0\" in ch_keys)\n",
    "            case = \"between_muscles\" if has_between else \"within_muscle\"\n",
    "\n",
    "        allowed_pairs = pool_pair_name[case]\n",
    "\n",
    "        cross_all = sim_data.get(\"Cross_histograms\", {})\n",
    "\n",
    "        # iterate only allowed pool-pairs\n",
    "        for pool_pair in allowed_pairs:\n",
    "            ch_dict = cross_all.get(pool_pair)\n",
    "            if not ch_dict:\n",
    "                continue\n",
    "\n",
    "            for mn_idx, hist_data in ch_dict.items():\n",
    "                row = {\n",
    "                    \"sim_idx\":     sim_idx,\n",
    "                    \"mn_idx\":      mn_idx,\n",
    "                    \"condition\":   condition,\n",
    "                    \"muscle_pair\": muscle_pair_arrow,  # 'AA<->BB'\n",
    "                    \"pool_pair\":   pool_pair,          # 'pool_0<->pool_1' etc.\n",
    "                    \"intensity\":   intensity,\n",
    "                    \"subject\":     subject,\n",
    "                    \"perspective\": perspective,\n",
    "                    \"direction\":   direction,\n",
    "                    \"r2_full\":     hist_data[key_to_load][\"r2_full\"],\n",
    "                    \"r2_baseline\": hist_data[key_to_load][\"r2_base\"],\n",
    "                    \"n_spikes\":    hist_data[key_to_load][\"n_spikes\"],\n",
    "                }\n",
    "\n",
    "                # observations\n",
    "                for obs in observations:\n",
    "                    if obs == \"trough_area\":\n",
    "                        val = hist_data[key_to_load][subkey_to_load][\"raw_area\"] * -100.0  # %\n",
    "                    elif obs == \"peak_height\":\n",
    "                        val = hist_data[key_to_load][\"sync_height\"] * 100.0               # %\n",
    "                    elif obs == \"firing_rate\":\n",
    "                        val = sim_data[\"Firing_rates\"][\"MN\"][\"mean\"][f\"MN_{mn_idx}\"]\n",
    "                    elif obs == \"IPSP_delay\":\n",
    "                        val = hist_data[key_to_load][f\"delay_{subkey_to_load}_IPSP\"] * 1000.0  # ms\n",
    "                    elif obs == \"hist_plateau_duration\":\n",
    "                        val = hist_data[key_to_load][obs] * 1000.0  # ms\n",
    "                    elif obs == \"proportion_of_prob_within_plateau_duration\":\n",
    "                        val = hist_data[key_to_load][obs] * 100.0   # %\n",
    "                    else:\n",
    "                        raise KeyError(f\"Unrecognized observation '{obs}'\")\n",
    "                    row[obs] = val\n",
    "\n",
    "                rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# build both DataFrames\n",
    "df_obs_simulated_from_posterior_samples = sims_dict_to_dataframe(\n",
    "    all_sims,\n",
    "    perspective,\n",
    "    direction,\n",
    "    observations,\n",
    "    pool_pair_name\n",
    ")\n",
    "\n",
    "df_obs_simulated_from_posterior_sample_highest_likelihood = sims_dict_to_dataframe(\n",
    "    best_sims,\n",
    "    perspective,\n",
    "    direction,\n",
    "    observations,\n",
    "    pool_pair_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d987a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD EXPERIMENTAL OBSERVATIONS\n",
    "df_obs_experiment = pd.read_csv(f\"{path_experimental_obs}\\\\{csv_experimental_obs}\")\n",
    "# Define a rename mapping: {old_name: new_name, …}\n",
    "rename_map = {\n",
    "    \"MU_idx\": \"mn_idx\",\n",
    "    \"firing_rates_mean\": \"firing_rate\",\n",
    "    \"raw_area\": \"trough_area\",\n",
    "    \"sync_height\": \"peak_height\",\n",
    "    \"IPSP_timing_of_trough\": \"IPSP_delay\",\n",
    "    \"proportion_of_prob_within_plateau_duration\": \"proportion_of_prob_within_plateau_duration\", # same name\n",
    "    \"hist_plateau_duration\": \"hist_plateau_duration\", # same name\n",
    "    \"r2_base\": \"r2_baseline\"\n",
    "}\n",
    "\n",
    "# Apply the renaming (only the keys in rename_map get renamed)\n",
    "df_obs_experiment = df_obs_experiment.rename(columns=rename_map)\n",
    "\n",
    "# Define exactly which columns should be kept\n",
    "keep_cols = list(df_obs_simulated_from_posterior_samples.columns)\n",
    "# select only those columns (drops everything else)\n",
    "df_obs_experiment = df_obs_experiment.filter(items=keep_cols)\n",
    "\n",
    "df_obs_experiment['trough_area'] *= -100 # turn to positive %\n",
    "df_obs_experiment['peak_height'] *= 100 # turn to %\n",
    "df_obs_experiment['IPSP_delay'] *= 1000 # turn to ms\n",
    "# df_obs_experiment['hist_plateau_duration'] *= 1000 # turn to ms\n",
    "# df_obs_experiment['proportion_of_prob_within_plateau_duration'] *= 100 # turn to %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3eb15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOOP THROUGH THE EXPERIMENTAL DATA .PKL FILES TO LOAD INDIVIDUAL CROSS-HISTOGRAM ANALYSES RESULTS\n",
    "# (Used only to plot the cross-histograms)\n",
    "exp_analysis_results_dict = {} # folder to look into = path_experimental_obs\n",
    "for pkl_path in Path(path_experimental_obs).glob(\"*.pkl\"):\n",
    "    key = pkl_path.stem               # e.g. \"subject1_cross_histogram\"\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        exp_analysis_results_dict[key] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f23d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD SIMULATED PRIOR OBSERVATIONS (RANDOM SAMPLING OF PARAMETER SPACE - USED TO TRAIN THE SBI NETWORK)\n",
    "# First load single muscle case\n",
    "df_prior_single = pd.read_csv(Path(path_prior_samples_obs_single_muscle) / csv_prior_samples_obs)\n",
    "# Union of columns is taken automatically; missing cols become NaN\n",
    "df_prior_pairs  = pd.read_csv(Path(path_prior_samples_obs_paired_muscles) / csv_prior_samples_obs)\n",
    "# something something pd.read_csv(f\"{path_prior_samples_obs_paired_muscles}\\\\{csv_prior_samples_obs}\")\n",
    "df_obs_simulated_from_prior = pd.concat([df_prior_single, df_prior_pairs], ignore_index=True, sort=False)# And append the muscle pairs case\n",
    "\n",
    "# pull out the digits after \"_sim\" at the end of the string\n",
    "df_obs_simulated_from_prior['sim_name'] = (\n",
    "    df_obs_simulated_from_prior['sim_name']\n",
    "      .str\n",
    "      .extract(r'output_(\\d+)$')      # returns a DataFrame with one column\n",
    "      .astype(float)               # or .astype(int) if you know they all match\n",
    "      .iloc[:, 0]                  # turn it into a Series\n",
    ")\n",
    "# Define a rename mapping: {old_name: new_name, …}\n",
    "rename_map = {\n",
    "    \"sim_name\": \"sim_idx\",\n",
    "    \"MN_index\": \"mn_idx\",\n",
    "    \"Firing_rates_mean\": \"firing_rate\",\n",
    "    \"sync_height\": \"peak_height\",\n",
    "    # \"proportion_of_prob_within_plateau_duration\": \"proportion_of_prob_within_plateau_duration\", # same name\n",
    "    # \"hist_plateau_duration\": \"hist_plateau_duration\", # same name\n",
    "    \"r2_base\": \"r2_baseline\"\n",
    "}\n",
    "if direction == 'inhibited':\n",
    "    if perspective == 'other_MUs_as_ref':\n",
    "        rename_map[\"raw_area_fwd\"] = 'trough_area'\n",
    "        rename_map[\"delay_forward_IPSP\"] = 'IPSP_delay'\n",
    "    elif perspective == 'MU_as_ref':\n",
    "        rename_map[\"raw_area_bwd\"] = 'trough_area'\n",
    "        rename_map[\"delay_backward_IPSP\"] = 'IPSP_delay'\n",
    "elif direction == 'inhibiting':\n",
    "    if perspective == 'other_MUs_as_ref':\n",
    "        rename_map[\"raw_area_bwd\"] = 'trough_area'\n",
    "        rename_map[\"delay_backward_IPSP\"] = 'IPSP_delay'\n",
    "    elif perspective == 'MU_as_ref':\n",
    "        rename_map[\"raw_area_fwd\"] = 'trough_area'\n",
    "        rename_map[\"delay_forward_IPSP\"] = 'IPSP_delay'\n",
    "\n",
    "# Apply the renaming (only the keys in rename_map get renamed)\n",
    "df_obs_simulated_from_prior = df_obs_simulated_from_prior.rename(columns=rename_map)\n",
    "\n",
    "# Define exactly which columns should be kept\n",
    "keep_cols = list(df_obs_simulated_from_posterior_samples.columns)\n",
    "# select only those columns (drops everything else)\n",
    "df_obs_simulated_from_prior = df_obs_simulated_from_prior.filter(items=keep_cols)\n",
    "\n",
    "df_obs_simulated_from_prior['trough_area'] *= -100 # turn to positive %\n",
    "df_obs_simulated_from_prior['peak_height'] *= 100 # turn to positive %\n",
    "df_obs_simulated_from_prior['IPSP_delay'] *= 1000 # turn to ms\n",
    "# df_obs_simulated_from_prior['hist_plateau_duration'] *= 1000 # turn to ms\n",
    "# df_obs_simulated_from_prior['proportion_of_prob_within_plateau_duration'] *= 100 # turn to %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter all data frames\n",
    "df_obs_simulated_from_posterior_samples = df_obs_simulated_from_posterior_samples[df_obs_simulated_from_posterior_samples['r2_baseline']>min_r2_for_baseline_curve_fit['simulation']]\n",
    "df_obs_simulated_from_posterior_samples = df_obs_simulated_from_posterior_samples[df_obs_simulated_from_posterior_samples['r2_full']>min_r2_for_overall_curve_fit['simulation']]\n",
    "df_obs_simulated_from_posterior_samples = df_obs_simulated_from_posterior_samples[df_obs_simulated_from_posterior_samples['n_spikes']>min_nb_spikes['simulation']]\n",
    "df_obs_simulated_from_posterior_samples = df_obs_simulated_from_posterior_samples[df_obs_simulated_from_posterior_samples['perspective']==perspective]\n",
    "df_obs_simulated_from_posterior_samples = df_obs_simulated_from_posterior_samples[df_obs_simulated_from_posterior_samples['direction']==direction]\n",
    "df_obs_simulated_from_posterior_sample_highest_likelihood = df_obs_simulated_from_posterior_sample_highest_likelihood[df_obs_simulated_from_posterior_sample_highest_likelihood['r2_baseline']>min_r2_for_baseline_curve_fit['simulation']]\n",
    "df_obs_simulated_from_posterior_sample_highest_likelihood = df_obs_simulated_from_posterior_sample_highest_likelihood[df_obs_simulated_from_posterior_sample_highest_likelihood['r2_full']>min_r2_for_overall_curve_fit['simulation']]\n",
    "df_obs_simulated_from_posterior_sample_highest_likelihood = df_obs_simulated_from_posterior_sample_highest_likelihood[df_obs_simulated_from_posterior_sample_highest_likelihood['n_spikes']>min_nb_spikes['simulation']]\n",
    "df_obs_simulated_from_posterior_sample_highest_likelihood = df_obs_simulated_from_posterior_sample_highest_likelihood[df_obs_simulated_from_posterior_sample_highest_likelihood['perspective']==perspective]\n",
    "df_obs_simulated_from_posterior_sample_highest_likelihood = df_obs_simulated_from_posterior_sample_highest_likelihood[df_obs_simulated_from_posterior_sample_highest_likelihood['direction']==direction]\n",
    "df_obs_experiment = df_obs_experiment[df_obs_experiment['r2_baseline']>min_r2_for_baseline_curve_fit['experiment']]\n",
    "df_obs_experiment = df_obs_experiment[df_obs_experiment['r2_full']>min_r2_for_overall_curve_fit['experiment']]\n",
    "df_obs_experiment = df_obs_experiment[df_obs_experiment['n_spikes']>min_nb_spikes['experiment']]\n",
    "df_obs_experiment = df_obs_experiment[df_obs_experiment['perspective']==perspective]\n",
    "df_obs_experiment = df_obs_experiment[df_obs_experiment['direction']==direction]\n",
    "df_obs_simulated_from_prior = df_obs_simulated_from_prior[df_obs_simulated_from_prior['r2_baseline']>min_r2_for_baseline_curve_fit['simulation']]\n",
    "df_obs_simulated_from_prior = df_obs_simulated_from_prior[df_obs_simulated_from_prior['r2_full']>min_r2_for_overall_curve_fit['simulation']]\n",
    "df_obs_simulated_from_prior = df_obs_simulated_from_prior[df_obs_simulated_from_prior['n_spikes']>min_nb_spikes['simulation']]\n",
    "df_obs_simulated_from_prior = df_obs_simulated_from_prior[df_obs_simulated_from_prior['perspective']==perspective]\n",
    "df_obs_simulated_from_prior = df_obs_simulated_from_prior[df_obs_simulated_from_prior['direction']==direction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea065ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs_simulated_from_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec24f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize all the 'observations' values according to the prior (simulated data from uniform prior) data mean and std\n",
    "# compute means and stds on the *prior* data only\n",
    "exp_stats = {}\n",
    "for obs in observations:\n",
    "    μ = df_obs_simulated_from_prior[obs].mean()\n",
    "    σ = df_obs_simulated_from_prior[obs].std(ddof=0)        # population‐style std, or ddof=1 for sample‐std\n",
    "    exp_stats[obs] = {\"mean\": μ, \"std\": σ}\n",
    "# replace in-place:\n",
    "for obs in observations:\n",
    "    df_obs_experiment[obs] = (df_obs_experiment[obs] - exp_stats[obs][\"mean\"]) / exp_stats[obs][\"std\"]\n",
    "    df_obs_simulated_from_posterior_samples[obs] = (df_obs_simulated_from_posterior_samples[obs] - exp_stats[obs][\"mean\"]) / exp_stats[obs][\"std\"]\n",
    "    df_obs_simulated_from_posterior_sample_highest_likelihood[obs] = (df_obs_simulated_from_posterior_sample_highest_likelihood[obs] - exp_stats[obs][\"mean\"]) / exp_stats[obs][\"std\"]\n",
    "    df_obs_simulated_from_prior[obs] = (df_obs_simulated_from_prior[obs] - exp_stats[obs][\"mean\"]) / exp_stats[obs][\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c22ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build summaries...\n",
    "def iqr(series: pd.Series) -> float:\n",
    "    \"\"\"Interquartile range = Q3 - Q1.\"\"\"\n",
    "    return float(series.quantile(0.75) - series.quantile(0.25))\n",
    "iqr.__name__ = \"iqr\"  # ensures a clean column suffix in groupby.agg\n",
    "\n",
    "def summarize_dataframe(df, summarize_over, observations):\n",
    "    \"\"\"\n",
    "    Group df by columns in summarize_over that exist.\n",
    "    For each obs in `observations`, compute mean, std, median, iqr.\n",
    "    For other columns, take the first value.\n",
    "    Returns a DataFrame with flat columns: obs_mean, obs_std, obs_median, obs_iqr.\n",
    "    \"\"\"\n",
    "    # 1) pick only grouping columns that exist\n",
    "    group_cols = [c for c in summarize_over if c in df.columns]\n",
    "    if not group_cols:\n",
    "        raise ValueError(f\"None of {summarize_over} found in DataFrame columns.\")\n",
    "\n",
    "    # 2) \"other\" columns (neither grouping nor summarized)\n",
    "    other_cols = [c for c in df.columns if c not in group_cols and c not in observations]\n",
    "\n",
    "    # 2a) optional sanity check for constancy within groups\n",
    "    for col in other_cols:\n",
    "        nunique = df.groupby(group_cols)[col].nunique()\n",
    "        if (nunique > 1).any():\n",
    "            print(f\"    Warning: column '{col}' has >1 distinct values in some groups.\")\n",
    "\n",
    "    # 3) aggregation dict\n",
    "    agg_stats = ['mean', 'std', 'median', iqr]\n",
    "    agg_dict = {c: agg_stats for c in observations if c in df.columns}\n",
    "    agg_dict.update({c: 'first' for c in other_cols})\n",
    "\n",
    "    # 4) group & agg\n",
    "    summary = df.groupby(group_cols, as_index=False).agg(agg_dict)\n",
    "\n",
    "    # 5) flatten column MultiIndex -> single level names\n",
    "    new_cols = []\n",
    "    for col, func in summary.columns:\n",
    "        if func == \"\":\n",
    "            # groupers come through with empty func\n",
    "            new_cols.append(col)\n",
    "        elif func == \"first\":\n",
    "            new_cols.append(col)\n",
    "        else:\n",
    "            # 'mean'|'std'|'median'|'iqr'\n",
    "            new_cols.append(f\"{col}_{func}\")\n",
    "    summary.columns = new_cols\n",
    "    return summary\n",
    "\n",
    "\n",
    "# For each simulation (simulated observations - whether from prior or from posterior)\n",
    "print('Summarizing data frame of simulated observations from inferred posterior')\n",
    "df_summary_obs_simulated_from_posterior = summarize_dataframe(df_obs_simulated_from_posterior_samples,\n",
    "                                                                summarize_over,\n",
    "                                                                observations)\n",
    "print('Summarizing data frame of simulated observations from prior')\n",
    "df_summary_obs_simulated_from_prior = summarize_dataframe(df_obs_simulated_from_prior,\n",
    "                                                          summarize_over,\n",
    "                                                          observations)\n",
    "print('Summarizing data frame of simulated observations from inferred posterior - specifically the mode of the posterior (set of parameters with highest likelihood)')\n",
    "df_obs_simulated_from_posterior_sample_highest_likelihood_temp = df_obs_simulated_from_posterior_sample_highest_likelihood.copy()\n",
    "df_obs_simulated_from_posterior_sample_highest_likelihood_temp['sim_idx'] = 0 # to allow grouping\n",
    "df_summary_obs_simulated_highest_likelihood = summarize_dataframe(df_obs_simulated_from_posterior_sample_highest_likelihood_temp,\n",
    "                                                          summarize_over,\n",
    "                                                          observations)\n",
    "# For each condition\n",
    "print('Summarizing data frame of experimental observations')\n",
    "df_summary_obs_experiment = summarize_dataframe(df_obs_experiment,\n",
    "                                                summarize_over,\n",
    "                                                observations)\n",
    "# From there, compute the groups to iterate over\n",
    "subjects = list(np.unique(df_summary_obs_experiment['subject']))\n",
    "muscle_pairs = list(np.unique(df_summary_obs_experiment['muscle_pair']))\n",
    "intensities = list(np.unique(df_summary_obs_experiment['intensity']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d96be67",
   "metadata": {},
   "source": [
    "# Posterior predictive checks - accuracy and correlation between posterior-predicted features and experimentally observed features, across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "observations = [\n",
    "    \"trough_area\",\n",
    "    \"peak_height\",\n",
    "    \"firing_rate\",\n",
    "    \"IPSP_delay\",\n",
    "]\n",
    "summary_stats = ['mean', 'std', 'median', 'iqr']\n",
    "# 4 features × 4 stats = 16 columns\n",
    "feat_cols = [f\"{obs}_{stat}\" for obs in observations for stat in summary_stats]\n",
    "print(\"feat_cols:\", feat_cols)\n",
    "\n",
    "# Make sure the prior summary DF exists and has the 16 feature columns\n",
    "df_prior_for_pca = df_summary_obs_simulated_from_prior.copy()\n",
    "\n",
    "# Drop rows with NaNs in features\n",
    "df_prior_for_pca = df_prior_for_pca.dropna(subset=feat_cols)\n",
    "print(f\"PCA training rows after dropping NaNs: {len(df_prior_for_pca)}\")\n",
    "\n",
    "X_prior = df_prior_for_pca[feat_cols].to_numpy(dtype=float)\n",
    "\n",
    "# Fit PCA on these (no extra whitening – your features are already z-scored to the prior)\n",
    "pca = PCA(n_components=min(len(feat_cols), X_prior.shape[0]))\n",
    "pca.fit(X_prior)\n",
    "\n",
    "print(\"PCA fitted. Explained variance ratio (first few):\", pca.explained_variance_ratio_[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f878f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_rgb\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# ===========================\n",
    "# Config\n",
    "# ===========================\n",
    "EXCLUDE_PAIRS = [\"VL<->VM\", \"GM<->SOL\"]   # same as before\n",
    "GROUP_COLS = [\"muscle_pair\", \"intensity\"]\n",
    "\n",
    "SUMMARY_STAT        = \"mean\"   # which summary stat column to use for the feature rows\n",
    "POST_SUMMARY        = \"mean\"   # \"mode\" (highest-likelihood) or \"mean\" (posterior mean) for curves + metrics\n",
    "STANDARDIZATION_MODE = \"exp\"  # \"prior\" = relative to prior SD; \"exp\" = relative to across-condition SD\n",
    "NBINS_Y             = 30\n",
    "VSCALE_QUANTILE     = 0.999\n",
    "FIG_W               = 10\n",
    "FIG_H_PER_ROW       = 3.0\n",
    "\n",
    "# base colors for each feature & for PC1 row\n",
    "feature_colors = {\n",
    "    \"trough_area\": \"blue\",\n",
    "    \"peak_height\": \"red\",\n",
    "    \"firing_rate\": \"orange\",\n",
    "    \"IPSP_delay\": \"purple\",\n",
    "}\n",
    "PC1_COLOR = \"green\"\n",
    "\n",
    "# Need PCA + full feature list for PC1 row\n",
    "assert \"feat_cols\" in globals(), \"This cell expects a global 'feat_cols' list (all 16 feature columns).\"\n",
    "assert \"pca\" in globals(), \"This cell expects a fitted sklearn PCA instance called 'pca'.\"\n",
    "assert \"exp_stats\" in globals(), \"This cell expects a global 'exp_stats' dict with prior mean/std for each observation.\"\n",
    "\n",
    "\n",
    "def white_to_color_cmap(color_hex, steps=256):\n",
    "    rgb = to_rgb(color_hex)\n",
    "    return LinearSegmentedColormap.from_list(\"white_to_color\", [(1, 1, 1), rgb], N=steps)\n",
    "\n",
    "\n",
    "def rmse(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    return float(np.sqrt(np.mean((a - b)**2)))\n",
    "\n",
    "\n",
    "def r2(pred, true):\n",
    "    pred = np.asarray(pred, dtype=float); true = np.asarray(true, dtype=float)\n",
    "    ss_res = np.sum((pred - true)**2)\n",
    "    ss_tot = np.sum((true - true.mean())**2)\n",
    "    return float(1.0 - ss_res/ss_tot) if ss_tot > 0 else np.nan\n",
    "\n",
    "\n",
    "def filter_and_dropna_single(df, col):\n",
    "    out = df.copy()\n",
    "    if \"muscle_pair\" in out.columns:\n",
    "        out = out[~out[\"muscle_pair\"].isin(EXCLUDE_PAIRS)]\n",
    "    out = out.dropna(subset=[col])\n",
    "    return out\n",
    "\n",
    "\n",
    "def filter_and_dropna_multifeat(df, cols):\n",
    "    out = df.copy()\n",
    "    if \"muscle_pair\" in out.columns:\n",
    "        out = out[~out[\"muscle_pair\"].isin(EXCLUDE_PAIRS)]\n",
    "    out[cols] = out[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    out = out.dropna(subset=cols)\n",
    "    return out\n",
    "\n",
    "\n",
    "def make_transform_from_y_true(y_true_z, mode):\n",
    "    \"\"\"\n",
    "    Build a transform v -> v_std for the chosen standardization mode.\n",
    "    y_true_z are the (already prior-z-scored) experimental values across conditions.\n",
    "    \"\"\"\n",
    "    y_true_z = np.asarray(y_true_z, dtype=float)\n",
    "    if mode == \"exp\":\n",
    "        mu = float(y_true_z.mean())\n",
    "        sd = float(y_true_z.std(ddof=0))\n",
    "        if sd <= 0:\n",
    "            sd = 1.0\n",
    "        def transform(v):\n",
    "            return (np.asarray(v, dtype=float) - mu) / sd\n",
    "    else:  # \"prior\" or anything else: identity\n",
    "        mu = 0.0\n",
    "        sd = 1.0\n",
    "        def transform(v):\n",
    "            return np.asarray(v, dtype=float)\n",
    "    return transform, mu, sd\n",
    "\n",
    "\n",
    "if STANDARDIZATION_MODE == \"prior\":\n",
    "    std_label = \"relative to prior SD\"\n",
    "else:\n",
    "    std_label = \"relative to across-condition SD\"\n",
    "\n",
    "# ---------------------------\n",
    "# Row specification: 4 features + 1 PC1 row\n",
    "# ---------------------------\n",
    "row_specs = [{\"kind\": \"feature\", \"feat\": feat} for feat in observations]\n",
    "row_specs.append({\"kind\": \"pc1\", \"feat\": \"PC1\"})\n",
    "\n",
    "n_rows = len(row_specs)\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows, 1,\n",
    "    figsize=(FIG_W, max(FIG_H_PER_ROW * n_rows, 3.0)),\n",
    "    squeeze=False\n",
    ")\n",
    "\n",
    "y_ranges = [None] * n_rows\n",
    "density_vals_for_vscale = []\n",
    "\n",
    "# ===========================\n",
    "# First pass: compute y-range and gather density values for shared v-scale\n",
    "# ===========================\n",
    "for row_idx, spec in enumerate(row_specs):\n",
    "    kind = spec[\"kind\"]\n",
    "\n",
    "    if kind == \"feature\":\n",
    "        feat = spec[\"feat\"]\n",
    "        col_name = f\"{feat}_{SUMMARY_STAT}\"\n",
    "\n",
    "        df_exp  = filter_and_dropna_single(df_summary_obs_experiment, col_name)\n",
    "        df_post = filter_and_dropna_single(df_summary_obs_simulated_from_posterior, col_name)\n",
    "        df_best = filter_and_dropna_single(df_summary_obs_simulated_from_posterior, col_name)\n",
    "\n",
    "        # build maps: cond -> values\n",
    "        exp_map = {k: float(g.iloc[0][col_name])\n",
    "                   for k, g in df_exp.groupby(GROUP_COLS, dropna=False)}\n",
    "        post_map = {}\n",
    "        for k, g in df_post.groupby(GROUP_COLS, dropna=False):\n",
    "            vals = g[col_name].to_numpy(dtype=float)\n",
    "            if vals.size > 0:\n",
    "                post_map[k] = vals\n",
    "        best_map = {k: float(g.iloc[0][col_name])\n",
    "                    for k, g in df_best.groupby(GROUP_COLS, dropna=False)}\n",
    "\n",
    "        conds = sorted(set(exp_map.keys()) & set(post_map.keys()) & set(best_map.keys()))\n",
    "        if not conds:\n",
    "            spec[\"skip\"] = True\n",
    "            continue\n",
    "\n",
    "        # these are in prior-z space\n",
    "        y_true_z = np.array([exp_map[c] for c in conds], dtype=float)\n",
    "        y_best_z = np.array([best_map[c] for c in conds], dtype=float)\n",
    "        post_vals_all_z = np.concatenate([post_map[c] for c in conds])\n",
    "\n",
    "        # transform for chosen standardization mode\n",
    "        transform, mu_z, sd_z = make_transform_from_y_true(y_true_z, STANDARDIZATION_MODE)\n",
    "        spec[\"mu_z\"] = mu_z\n",
    "        spec[\"sd_z\"] = sd_z\n",
    "\n",
    "        y_true_std = transform(y_true_z)\n",
    "        y_best_std = transform(y_best_z)\n",
    "        post_vals_all_std = transform(post_vals_all_z)\n",
    "\n",
    "        # robust y-range in standardized plotting units\n",
    "        all_vals = np.concatenate([y_true_std, y_best_std, post_vals_all_std])\n",
    "        q_low, q_high = np.quantile(all_vals, [0.01, 0.99])\n",
    "        span = q_high - q_low if q_high > 0 else 1.0\n",
    "        y_min = q_low - 0.1 * span\n",
    "        y_max = q_high + 0.1 * span\n",
    "        y_ranges[row_idx] = (y_min, y_max)\n",
    "\n",
    "        # density image (unsorted) for v-scale, in standardized units\n",
    "        y_edges = np.linspace(y_min, y_max, NBINS_Y + 1)\n",
    "        img_unsorted = np.zeros((NBINS_Y, len(conds)), dtype=float)\n",
    "        for j, cond in enumerate(conds):\n",
    "            vals_std = transform(post_map[cond])\n",
    "            vals_std = np.clip(vals_std, y_min, y_max)\n",
    "            counts, _ = np.histogram(vals_std, bins=y_edges)\n",
    "            col_pdf = counts.astype(float)\n",
    "            if col_pdf.sum() > 0:\n",
    "                col_pdf /= col_pdf.sum()\n",
    "            img_unsorted[:, j] = col_pdf\n",
    "\n",
    "        density_vals_for_vscale.append(img_unsorted.ravel())\n",
    "\n",
    "    elif kind == \"pc1\":\n",
    "        # PC1 over full 16-D feature space\n",
    "        df_exp_pc  = filter_and_dropna_multifeat(df_summary_obs_experiment, feat_cols)\n",
    "        df_post_pc = filter_and_dropna_multifeat(df_summary_obs_simulated_from_posterior, feat_cols)\n",
    "        df_best_pc = filter_and_dropna_multifeat(df_summary_obs_simulated_from_posterior, feat_cols)\n",
    "\n",
    "        exp_map_pc = {k: g.iloc[0][feat_cols].to_numpy(dtype=float)\n",
    "                      for k, g in df_exp_pc.groupby(GROUP_COLS, dropna=False)}\n",
    "        post_map_pc = {}\n",
    "        for k, g in df_post_pc.groupby(GROUP_COLS, dropna=False):\n",
    "            X = g[feat_cols].to_numpy(dtype=float)\n",
    "            if X.shape[0] > 0:\n",
    "                post_map_pc[k] = X\n",
    "        best_map_pc = {k: g.iloc[0][feat_cols].to_numpy(dtype=float)\n",
    "                       for k, g in df_best_pc.groupby(GROUP_COLS, dropna=False)}\n",
    "\n",
    "        conds = sorted(set(exp_map_pc.keys()) & set(post_map_pc.keys()) & set(best_map_pc.keys()))\n",
    "        if not conds:\n",
    "            spec[\"skip\"] = True\n",
    "            continue\n",
    "\n",
    "        pc_true_z_list = []\n",
    "        pc_best_z_list = []\n",
    "        post_pc_vals_all_z = []\n",
    "\n",
    "        for cond in conds:\n",
    "            vec_exp  = exp_map_pc[cond]\n",
    "            vec_best = best_map_pc[cond]\n",
    "            X_post   = post_map_pc[cond]\n",
    "\n",
    "            pc_true_z = pca.transform(vec_exp[None, :])[:, 0][0]\n",
    "            pc_best_z = pca.transform(vec_best[None, :])[:, 0][0]\n",
    "            pcs_post_z = pca.transform(X_post)[:, 0]\n",
    "\n",
    "            pc_true_z_list.append(pc_true_z)\n",
    "            pc_best_z_list.append(pc_best_z)\n",
    "            post_pc_vals_all_z.append(pcs_post_z)\n",
    "\n",
    "        pc_true_z = np.array(pc_true_z_list, dtype=float)\n",
    "        pc_best_z = np.array(pc_best_z_list, dtype=float)\n",
    "        post_pc_vals_all_z = np.concatenate(post_pc_vals_all_z)\n",
    "\n",
    "        # standardize PC1 according to mode\n",
    "        transform, mu_z, sd_z = make_transform_from_y_true(pc_true_z, STANDARDIZATION_MODE)\n",
    "        spec[\"mu_z\"] = mu_z\n",
    "        spec[\"sd_z\"] = sd_z\n",
    "\n",
    "        pc_true_std = transform(pc_true_z)\n",
    "        pc_best_std = transform(pc_best_z)\n",
    "        post_pc_vals_all_std = transform(post_pc_vals_all_z)\n",
    "\n",
    "        all_vals = np.concatenate([pc_true_std, pc_best_std, post_pc_vals_all_std])\n",
    "        q_low, q_high = np.quantile(all_vals, [0.01, 0.99])\n",
    "        span = q_high - q_low if q_high > 0 else 1.0\n",
    "        y_min = q_low - 0.1 * span\n",
    "        y_max = q_high + 0.1 * span\n",
    "        y_ranges[row_idx] = (y_min, y_max)\n",
    "\n",
    "        y_edges = np.linspace(y_min, y_max, NBINS_Y + 1)\n",
    "        img_unsorted = np.zeros((NBINS_Y, len(conds)), dtype=float)\n",
    "        for j, cond in enumerate(conds):\n",
    "            pcs_post_z = pca.transform(post_map_pc[cond])[:, 0]\n",
    "            pcs_post_std = transform(pcs_post_z)\n",
    "            pcs_post_std = np.clip(pcs_post_std, y_min, y_max)\n",
    "            counts, _ = np.histogram(pcs_post_std, bins=y_edges)\n",
    "            col_pdf = counts.astype(float)\n",
    "            if col_pdf.sum() > 0:\n",
    "                col_pdf /= col_pdf.sum()\n",
    "            img_unsorted[:, j] = col_pdf\n",
    "\n",
    "        density_vals_for_vscale.append(img_unsorted.ravel())\n",
    "\n",
    "# Shared v-scale\n",
    "if density_vals_for_vscale:\n",
    "    all_density_vals = np.concatenate(density_vals_for_vscale)\n",
    "else:\n",
    "    all_density_vals = np.array([1.0], dtype=float)\n",
    "\n",
    "vmin = 0.0\n",
    "vmax = float(np.quantile(all_density_vals, VSCALE_QUANTILE))\n",
    "if vmax <= 0:\n",
    "    vmax = 1.0\n",
    "\n",
    "# ===========================\n",
    "# Second pass: plotting with shared v-scale\n",
    "# ===========================\n",
    "last_im = None\n",
    "\n",
    "for row_idx, spec in enumerate(row_specs):\n",
    "    ax = axes[row_idx, 0]\n",
    "    kind = spec[\"kind\"]\n",
    "\n",
    "    if spec.get(\"skip\", False):\n",
    "        ax.axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    y_min, y_max = y_ranges[row_idx]\n",
    "    x_left = -0.5\n",
    "    mu_z = spec.get(\"mu_z\", 0.0)\n",
    "    sd_z = spec.get(\"sd_z\", 1.0)\n",
    "\n",
    "    def transform(v):\n",
    "        return (np.asarray(v, dtype=float) - mu_z) / sd_z\n",
    "\n",
    "    if kind == \"feature\":\n",
    "        feat = spec[\"feat\"]\n",
    "        col_name = f\"{feat}_{SUMMARY_STAT}\"\n",
    "\n",
    "        df_exp  = filter_and_dropna_single(df_summary_obs_experiment, col_name)\n",
    "        df_post = filter_and_dropna_single(df_summary_obs_simulated_from_posterior, col_name)\n",
    "        df_best = filter_and_dropna_single(df_summary_obs_simulated_from_posterior, col_name)\n",
    "\n",
    "        exp_map = {k: float(g.iloc[0][col_name])\n",
    "                   for k, g in df_exp.groupby(GROUP_COLS, dropna=False)}\n",
    "        post_map = {}\n",
    "        for k, g in df_post.groupby(GROUP_COLS, dropna=False):\n",
    "            vals = g[col_name].to_numpy(dtype=float)\n",
    "            if vals.size > 0:\n",
    "                post_map[k] = vals\n",
    "        best_map = {k: float(g.iloc[0][col_name])\n",
    "                    for k, g in df_best.groupby(GROUP_COLS, dropna=False)}\n",
    "\n",
    "        conds = sorted(set(exp_map.keys()) & set(post_map.keys()) & set(best_map.keys()))\n",
    "        if not conds:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        # prior-z values (as stored in the DF)\n",
    "        y_true_z = np.array([exp_map[c] for c in conds], dtype=float)\n",
    "        y_best_z = np.array([best_map[c] for c in conds], dtype=float)\n",
    "        y_post_mean_z = np.array([post_map[c].mean() for c in conds], dtype=float)\n",
    "\n",
    "        # choose which posterior summary to show (still in z-prior)\n",
    "        if POST_SUMMARY.lower() == \"mean\":\n",
    "            y_pred_z = y_post_mean_z\n",
    "            pred_label = \"Posterior mean\"\n",
    "        else:\n",
    "            y_pred_z = y_best_z\n",
    "            pred_label = \"Posterior mode\"\n",
    "\n",
    "        # standardized values for metrics & plotting, according to STANDARDIZATION_MODE\n",
    "        y_true_std = transform(y_true_z)\n",
    "        y_pred_std = transform(y_pred_z)\n",
    "\n",
    "        # --- standardized metrics ---\n",
    "        rmse_pred = rmse(y_pred_std, y_true_std)\n",
    "        r2_pred   = r2(y_pred_std, y_true_std)\n",
    "        if len(y_true_std) >= 2:\n",
    "            pear_r, _    = pearsonr(y_pred_std, y_true_std)\n",
    "            spear_rho, _ = spearmanr(y_pred_std, y_true_std)\n",
    "            r2_cal = pear_r**2  # linear R²\n",
    "        else:\n",
    "            pear_r = spear_rho = np.nan\n",
    "            r2_cal = np.nan\n",
    "\n",
    "        # --- unstandardized differences (absolute) in original units ---\n",
    "        sigma_raw = exp_stats[feat][\"std\"]  # std used for z-scoring this feature\n",
    "        diff_raw  = (y_pred_z - y_true_z) * sigma_raw        # predicted − experimental, in raw units\n",
    "        abs_diff_raw = np.abs(diff_raw)\n",
    "\n",
    "        mean_abs_diff   = float(np.mean(abs_diff_raw))\n",
    "        sd_abs_diff     = float(np.std(abs_diff_raw, ddof=0))\n",
    "        median_abs_diff = float(np.median(abs_diff_raw))\n",
    "        q1_abs, q3_abs  = [float(q) for q in np.quantile(abs_diff_raw, [0.25, 0.75])]\n",
    "        min_abs_diff    = float(np.min(abs_diff_raw))\n",
    "        max_abs_diff    = float(np.max(abs_diff_raw))\n",
    "\n",
    "        # sorting by experimental value in standardized units\n",
    "        order = np.argsort(y_true_std)\n",
    "        conds_sorted = [conds[i] for i in order]\n",
    "        y_true_sorted = y_true_std[order]\n",
    "        y_pred_sorted = y_pred_std[order]\n",
    "\n",
    "        # density image in sorted order (standardized units)\n",
    "        y_edges = np.linspace(y_min, y_max, NBINS_Y + 1)\n",
    "        img_sorted = np.zeros((NBINS_Y, len(conds_sorted)), dtype=float)\n",
    "        for j, cond in enumerate(conds_sorted):\n",
    "            vals_z = post_map[cond]\n",
    "            vals_std = transform(vals_z)\n",
    "            vals_std = np.clip(vals_std, y_min, y_max)\n",
    "            counts, _ = np.histogram(vals_std, bins=y_edges)\n",
    "            col_pdf = counts.astype(float)\n",
    "            if col_pdf.sum() > 0:\n",
    "                col_pdf /= col_pdf.sum()\n",
    "            img_sorted[:, j] = col_pdf\n",
    "\n",
    "        base_color = feature_colors.get(feat, \"#2a6f97\")\n",
    "        cmap = white_to_color_cmap(base_color, steps=256)\n",
    "\n",
    "        x_right = len(conds_sorted) - 0.5\n",
    "        im = ax.imshow(\n",
    "            img_sorted,\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "            extent=[x_left, x_right, y_min, y_max],\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            interpolation=None,\n",
    "        )\n",
    "        last_im = im\n",
    "\n",
    "        x = np.arange(len(conds_sorted))\n",
    "        # experiment (black) and posterior summary (colored)\n",
    "        ax.plot(x, y_true_sorted, color=\"black\", lw=1.4, marker=\"o\", markersize=3,\n",
    "                label=\"Experiment\")\n",
    "        ax.plot(x, y_pred_sorted, color=base_color, lw=1.2, marker=\"o\", markersize=3,\n",
    "                alpha=0.9, label=pred_label)\n",
    "\n",
    "        tick_labels = [f\"{mp}_{inten}\" for (mp, inten) in conds_sorted]\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(tick_labels, rotation=90, ha=\"right\", fontsize=7)\n",
    "\n",
    "        ax.set_xlim(x_left, x_right)\n",
    "        ax.set_ylabel(f\"{feat} ({SUMMARY_STAT})\")\n",
    "\n",
    "        if row_idx == 0:\n",
    "            ax.set_title(\n",
    "                \"Posterior predictive accuracy across conditions\\n\"\n",
    "                f\"(background: posterior density; lines: experiment vs posterior {POST_SUMMARY})\"\n",
    "            )\n",
    "            ax.legend(loc=\"upper right\", fontsize=8, frameon=False)\n",
    "\n",
    "        # standardized metrics (left box)\n",
    "        txt_std = (f\"Posterior {POST_SUMMARY} vs experiment\\n\"\n",
    "                   f\"({std_label}):\\n\"\n",
    "                   f\"  RMSE        = {rmse_pred:.3f}\\n\"\n",
    "                   f\"  R²          = {r2_pred:.3f}\\n\"\n",
    "                   f\"  R² (linear) = {r2_cal:.3f}\\n\"\n",
    "                   f\"  Pearson r   = {pear_r:.3f}\\n\"\n",
    "                   f\"  Spearman ρ  = {spear_rho:.3f}\")\n",
    "        ax.text(0.01, 0.99, txt_std, transform=ax.transAxes,\n",
    "                va=\"top\", ha=\"left\", fontsize=8, color=\"black\",\n",
    "                bbox=dict(facecolor=\"white\", alpha=0.75, edgecolor=\"none\"))\n",
    "\n",
    "        # unstandardized absolute differences (right box)\n",
    "        txt_raw = (f\"|pred − exp| in raw units:\\n\"\n",
    "                   f\"  mean±SD = {mean_abs_diff:.3g} ± {sd_abs_diff:.3g}\\n\"\n",
    "                   f\"  median [Q1–Q3] = {median_abs_diff:.3g} \"\n",
    "                   f\"[{q1_abs:.3g}–{q3_abs:.3g}]\\n\"\n",
    "                   f\"  min–max = {min_abs_diff:.3g}–{max_abs_diff:.3g}\")\n",
    "        ax.text(0.52, 0.99, txt_raw, transform=ax.transAxes,\n",
    "                va=\"top\", ha=\"left\", fontsize=8, color=\"black\",\n",
    "                bbox=dict(facecolor=\"white\", alpha=0.75, edgecolor=\"none\"))\n",
    "\n",
    "    elif kind == \"pc1\":\n",
    "        # PC1 row using full 16-D feature vectors\n",
    "        df_exp_pc  = filter_and_dropna_multifeat(df_summary_obs_experiment, feat_cols)\n",
    "        df_post_pc = filter_and_dropna_multifeat(df_summary_obs_simulated_from_posterior, feat_cols)\n",
    "        df_best_pc = filter_and_dropna_multifeat(df_summary_obs_simulated_from_posterior, feat_cols)\n",
    "\n",
    "        exp_map_pc = {k: g.iloc[0][feat_cols].to_numpy(dtype=float)\n",
    "                      for k, g in df_exp_pc.groupby(GROUP_COLS, dropna=False)}\n",
    "        post_map_pc = {}\n",
    "        for k, g in df_post_pc.groupby(GROUP_COLS, dropna=False):\n",
    "            X = g[feat_cols].to_numpy(dtype=float)\n",
    "            if X.shape[0] > 0:\n",
    "                post_map_pc[k] = X\n",
    "        best_map_pc = {k: g.iloc[0][feat_cols].to_numpy(dtype=float)\n",
    "                       for k, g in df_best_pc.groupby(GROUP_COLS, dropna=False)}\n",
    "\n",
    "        conds = sorted(set(exp_map_pc.keys()) & set(post_map_pc.keys()) & set(best_map_pc.keys()))\n",
    "        if not conds:\n",
    "            ax.axis(\"off\")\n",
    "            continue\n",
    "\n",
    "        pc_true_z = []\n",
    "        pc_best_z = []\n",
    "        pc_mean_post_z = []\n",
    "        post_pc_per_cond_z = {}\n",
    "\n",
    "        for cond in conds:\n",
    "            vec_exp  = exp_map_pc[cond]\n",
    "            vec_best = best_map_pc[cond]\n",
    "            X_post   = post_map_pc[cond]\n",
    "\n",
    "            pc_true_val_z = pca.transform(vec_exp[None, :])[:, 0][0]\n",
    "            pc_best_val_z = pca.transform(vec_best[None, :])[:, 0][0]\n",
    "            pcs_post_z    = pca.transform(X_post)[:, 0]\n",
    "\n",
    "            pc_true_z.append(pc_true_val_z)\n",
    "            pc_best_z.append(pc_best_val_z)\n",
    "            pc_mean_post_z.append(pcs_post_z.mean())\n",
    "            post_pc_per_cond_z[cond] = pcs_post_z\n",
    "\n",
    "        pc_true_z = np.array(pc_true_z, dtype=float)\n",
    "        pc_best_z = np.array(pc_best_z, dtype=float)\n",
    "        pc_mean_post_z = np.array(pc_mean_post_z, dtype=float)\n",
    "\n",
    "        # standardized PC1 according to mode\n",
    "        pc_true_std = transform(pc_true_z)\n",
    "        if POST_SUMMARY.lower() == \"mean\":\n",
    "            pc_pred_z = pc_mean_post_z\n",
    "            pred_label = \"Posterior mean (PC1)\"\n",
    "        else:\n",
    "            pc_pred_z = pc_best_z\n",
    "            pred_label = \"Posterior mode (PC1)\"\n",
    "\n",
    "        pc_pred_std = transform(pc_pred_z)\n",
    "\n",
    "        rmse_pred = rmse(pc_pred_std, pc_true_std)\n",
    "        r2_pred   = r2(pc_pred_std, pc_true_std)\n",
    "        if len(pc_true_std) >= 2:\n",
    "            pear_r, _    = pearsonr(pc_pred_std, pc_true_std)\n",
    "            spear_rho, _ = spearmanr(pc_pred_std, pc_true_std)\n",
    "            r2_cal = pear_r**2\n",
    "        else:\n",
    "            pear_r = spear_rho = np.nan\n",
    "            r2_cal = np.nan\n",
    "\n",
    "        # sort by experimental PC1 (standardized)\n",
    "        order = np.argsort(pc_true_std)\n",
    "        conds_sorted = [conds[i] for i in order]\n",
    "        pc_true_sorted = pc_true_std[order]\n",
    "        pc_pred_sorted = pc_pred_std[order]\n",
    "\n",
    "        # density image in sorted order\n",
    "        y_edges = np.linspace(y_min, y_max, NBINS_Y + 1)\n",
    "        img_sorted = np.zeros((NBINS_Y, len(conds_sorted)), dtype=float)\n",
    "        for j, cond in enumerate(conds_sorted):\n",
    "            pcs_post_z = post_pc_per_cond_z[cond]\n",
    "            pcs_post_std = transform(pcs_post_z)\n",
    "            pcs_post_std = np.clip(pcs_post_std, y_min, y_max)\n",
    "            counts, _ = np.histogram(pcs_post_std, bins=y_edges)\n",
    "            col_pdf = counts.astype(float)\n",
    "            if col_pdf.sum() > 0:\n",
    "                col_pdf /= col_pdf.sum()\n",
    "            img_sorted[:, j] = col_pdf\n",
    "\n",
    "        x_right = len(conds_sorted) - 0.5\n",
    "        cmap = white_to_color_cmap(PC1_COLOR, steps=256)\n",
    "        im = ax.imshow(\n",
    "            img_sorted,\n",
    "            aspect=\"auto\",\n",
    "            origin=\"lower\",\n",
    "            extent=[x_left, x_right, y_min, y_max],\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            interpolation=\"nearest\",\n",
    "        )\n",
    "        last_im = im\n",
    "\n",
    "        x = np.arange(len(conds_sorted))\n",
    "        ax.plot(x, pc_true_sorted, color=\"black\", lw=1.4, marker=\"o\", markersize=3,\n",
    "                label=\"Experiment (PC1)\")\n",
    "        ax.plot(x, pc_pred_sorted, color=PC1_COLOR, lw=1.2, marker=\"o\", markersize=3,\n",
    "                alpha=0.9, label=pred_label)\n",
    "\n",
    "        tick_labels = [f\"{mp}_{inten}\" for (mp, inten) in conds_sorted]\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(tick_labels, rotation=90, ha=\"right\", fontsize=7)\n",
    "\n",
    "        ax.set_xlim(x_left, x_right)\n",
    "        ax.set_ylabel(\"PC1 score\\n(all 4×4 features)\")\n",
    "\n",
    "        txt = (f\"Posterior {POST_SUMMARY} vs experiment (PC1)\\n\"\n",
    "               f\"({std_label}):\\n\"\n",
    "               f\"  RMSE        = {rmse_pred:.3f}\\n\"\n",
    "               f\"  R²          = {r2_pred:.3f}\\n\"\n",
    "               f\"  R² (linear) = {r2_cal:.3f}\\n\"\n",
    "               f\"  Pearson r   = {pear_r:.3f}\\n\"\n",
    "               f\"  Spearman ρ  = {spear_rho:.3f}\")\n",
    "        ax.text(0.01, 0.99, txt, transform=ax.transAxes,\n",
    "                va=\"top\", ha=\"left\", fontsize=8, color=\"black\",\n",
    "                bbox=dict(facecolor=\"white\", alpha=0.75, edgecolor=\"none\"))\n",
    "\n",
    "# shared x-label on last row\n",
    "axes[-1, 0].set_xlabel(\"Conditions (muscle_pair_intensity, sorted per row)\")\n",
    "\n",
    "# leave space on the right for colorbar\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1.0])\n",
    "\n",
    "# colorbar (shared)\n",
    "if last_im is not None:\n",
    "    cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    cbar = fig.colorbar(last_im, cax=cax)\n",
    "    cbar.set_label(\"Posterior column PDF (density)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262558c9",
   "metadata": {},
   "source": [
    "# Visual inspection of the experimental VS simulated synchronization cross-histograms\n",
    "### + distance to experimental observations (in feature space) calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7414c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each observed metric:\n",
    "# Mean +- std of experimental observations as vertical line + shaded area\n",
    "# Histogram of distribution of means from simulated samples from posterior (posterior prediction)\n",
    "# Histogram of distribution of means from simulated samples from prior (random, null predictions)\n",
    "# Mean +- std of simulated observation from highest likelihood sample\n",
    "distance_and_diff_dict = {}\n",
    "\n",
    "os.makedirs(path_to_save_into, exist_ok=True)\n",
    "\n",
    "for subject_i in subjects:\n",
    "    if len(subjects)==1: # if only one subject (= atually not iterating over subjects)\n",
    "        filtering_subjects = False\n",
    "        subject_title = \"pooled\"\n",
    "    else:\n",
    "        filtering_subjects = True\n",
    "        subject_title = subject_i\n",
    "    for muscle_pair_i in muscle_pairs:\n",
    "        for intensity_i in intensities:\n",
    "            dict_storage_current_key = f\"{subject_title}_{muscle_pair_i}_{intensity_i}\"\n",
    "            distance_and_diff_dict[dict_storage_current_key] = {}\n",
    "            # Filter each data frame to get only the corresponding values #################\n",
    "            temp_df_exp_obs = df_summary_obs_experiment.copy()\n",
    "            if filtering_subjects and ('subject' in temp_df_exp_obs.columns):\n",
    "                temp_df_exp_obs = temp_df_exp_obs[temp_df_exp_obs['subject']==subject_i]\n",
    "            if 'muscle_pair' in temp_df_exp_obs.columns:\n",
    "                temp_df_exp_obs = temp_df_exp_obs[temp_df_exp_obs['muscle_pair']==muscle_pair_i]\n",
    "            if 'intensity' in temp_df_exp_obs.columns:\n",
    "                temp_df_exp_obs = temp_df_exp_obs[temp_df_exp_obs['intensity']==intensity_i]\n",
    "            #\n",
    "            temp_df_exp_other_obs = df_obs_experiment.copy()\n",
    "            if filtering_subjects and ('subject' in temp_df_exp_other_obs.columns):\n",
    "                temp_df_exp_other_obs = temp_df_exp_other_obs[temp_df_exp_other_obs['subject']!=subject_i]\n",
    "            if 'muscle_pair' in temp_df_exp_other_obs.columns:\n",
    "                temp_df_exp_other_obs = temp_df_exp_other_obs[temp_df_exp_other_obs['muscle_pair']!=muscle_pair_i]\n",
    "            if 'intensity' in temp_df_exp_other_obs.columns:\n",
    "                temp_df_exp_other_obs = temp_df_exp_other_obs[temp_df_exp_other_obs['intensity']!=intensity_i]\n",
    "            # Get the data frame for each different experimental condition\n",
    "            temp_df_exp_obs_each_separately = {}\n",
    "            for subject_j_iter, subject_j in enumerate(np.unique(df_obs_experiment['subject'].values)):\n",
    "                if (not filtering_subjects):\n",
    "                    subject_j_title = \"pooled\"\n",
    "                    if (subject_j_iter > 0):\n",
    "                        break\n",
    "                else:\n",
    "                    subject_j_title = subject_j\n",
    "                for muscle_pair_j in np.unique(df_obs_experiment['muscle_pair'].values):\n",
    "                    for intensity_j in np.unique(df_obs_experiment['intensity'].values):\n",
    "                        temp_df_other_exp_cond = df_obs_experiment.copy()\n",
    "                        if filtering_subjects:\n",
    "                            temp_df_other_exp_cond = temp_df_other_exp_cond[\n",
    "                                temp_df_other_exp_cond['subject']==subject_j]\n",
    "                        #\n",
    "                        temp_df_other_exp_cond = temp_df_other_exp_cond[\n",
    "                            temp_df_other_exp_cond['muscle_pair']==muscle_pair_j]\n",
    "                        temp_df_other_exp_cond = temp_df_other_exp_cond[\n",
    "                            temp_df_other_exp_cond['intensity']==intensity_j]    \n",
    "                        #                \n",
    "                        temp_df_exp_obs_each_separately[\n",
    "                            f\"{subject_j_title}_{muscle_pair_j}_{intensity_j}\"] = temp_df_other_exp_cond\n",
    "            #\n",
    "            temp_df_sim_obs_posterior_highest_likelihood = df_obs_simulated_from_posterior_sample_highest_likelihood.copy()\n",
    "            if filtering_subjects:\n",
    "                temp_df_sim_obs_posterior_highest_likelihood = temp_df_sim_obs_posterior_highest_likelihood[temp_df_sim_obs_posterior_highest_likelihood['subject']==subject_i]\n",
    "            if 'muscle_pair' in temp_df_sim_obs_posterior_highest_likelihood.columns:\n",
    "                temp_df_sim_obs_posterior_highest_likelihood = temp_df_sim_obs_posterior_highest_likelihood[temp_df_sim_obs_posterior_highest_likelihood['muscle_pair']==muscle_pair_i]\n",
    "            if 'intensity' in temp_df_sim_obs_posterior_highest_likelihood.columns:\n",
    "                temp_df_sim_obs_posterior_highest_likelihood = temp_df_sim_obs_posterior_highest_likelihood[temp_df_sim_obs_posterior_highest_likelihood['intensity']==intensity_i]\n",
    "            #\n",
    "            temp_df_sim_obs_posterior = df_summary_obs_simulated_from_posterior.copy()\n",
    "            if filtering_subjects:\n",
    "                temp_df_sim_obs_posterior = temp_df_sim_obs_posterior[temp_df_sim_obs_posterior['subject']==subject_i]\n",
    "            if 'muscle_pair' in temp_df_sim_obs_posterior.columns:\n",
    "                temp_df_sim_obs_posterior = temp_df_sim_obs_posterior[temp_df_sim_obs_posterior['muscle_pair']==muscle_pair_i]\n",
    "            if 'intensity' in temp_df_sim_obs_posterior.columns:\n",
    "                temp_df_sim_obs_posterior = temp_df_sim_obs_posterior[temp_df_sim_obs_posterior['intensity']==intensity_i]\n",
    "            #\n",
    "            temp_df_sim_obs_prior = df_summary_obs_simulated_from_prior.copy()\n",
    "            if filtering_subjects:\n",
    "                temp_df_sim_obs_prior = temp_df_sim_obs_prior[temp_df_sim_obs_prior['subject']==subject_i]\n",
    "            if 'muscle_pair' in temp_df_sim_obs_prior.columns:\n",
    "                temp_df_sim_obs_prior = temp_df_sim_obs_prior[temp_df_sim_obs_prior['muscle_pair']==muscle_pair_i]\n",
    "            if 'intensity' in temp_df_sim_obs_prior.columns:\n",
    "                temp_df_sim_obs_prior = temp_df_sim_obs_prior[temp_df_sim_obs_prior['intensity']==intensity_i]\n",
    "            \n",
    "            # Compute euclidean distance in observation space ##########################\n",
    "            distance_and_diff_dict[dict_storage_current_key]['distance'] = {}\n",
    "            exp_obs_vector = np.zeros(len(observations))\n",
    "            sim_obs_posterior_highest_likelihood_vector = np.zeros(len(observations))\n",
    "            sim_obs_posterior_mat = np.zeros((temp_df_sim_obs_posterior.shape[0], len(observations)))\n",
    "            sim_obs_prior_mat = np.zeros((temp_df_sim_obs_prior.shape[0], len(observations)))\n",
    "            exp_other_obs_vector = np.zeros((temp_df_exp_other_obs.shape[0], len(observations)))\n",
    "            for i, obs_metric in enumerate(observations):\n",
    "                exp_obs_vector[i] = temp_df_exp_obs[f\"{obs_metric}_mean\"].iloc[0]\n",
    "                exp_other_obs_vector[:,i] = temp_df_exp_other_obs[f\"{obs_metric}\"]\n",
    "                sim_obs_posterior_highest_likelihood_vector[i] = temp_df_sim_obs_posterior_highest_likelihood[f\"{obs_metric}\"].mean()\n",
    "                sim_obs_posterior_mat[:,i] = temp_df_sim_obs_posterior[f\"{obs_metric}_mean\"]\n",
    "                sim_obs_prior_mat[:,i] = temp_df_sim_obs_prior[f\"{obs_metric}_mean\"]\n",
    "            # Euclidean distances, vectorized:\n",
    "            euc_dist_posterior = np.linalg.norm(\n",
    "                sim_obs_posterior_mat - exp_obs_vector[np.newaxis, :],\n",
    "                axis=1)\n",
    "            distance_and_diff_dict[dict_storage_current_key][\n",
    "                'distance']['posterior'] = euc_dist_posterior\n",
    "            euc_dist_posterior_highest_likelihood = np.linalg.norm(\n",
    "                sim_obs_posterior_highest_likelihood_vector[np.newaxis, :] - exp_obs_vector[np.newaxis, :],\n",
    "                axis=1)\n",
    "            distance_and_diff_dict[dict_storage_current_key][\n",
    "                'distance']['posterior_highest_likelihood'] = euc_dist_posterior_highest_likelihood\n",
    "            euc_dist_prior = np.linalg.norm(\n",
    "                sim_obs_prior_mat - exp_obs_vector[np.newaxis, :],\n",
    "                axis=1)\n",
    "            distance_and_diff_dict[dict_storage_current_key][\n",
    "                'distance']['prior'] = euc_dist_prior\n",
    "            euc_dist_other_experimental_conditions = np.linalg.norm(\n",
    "                exp_other_obs_vector - exp_obs_vector[np.newaxis, :],\n",
    "                axis=1)\n",
    "            euc_dist_other_experimental_conditions = euc_dist_other_experimental_conditions[ # ensure no NaN values\n",
    "                np.isfinite(euc_dist_other_experimental_conditions)]\n",
    "            distance_and_diff_dict[dict_storage_current_key][\n",
    "                'distance']['other_exp_pooled'] = euc_dist_other_experimental_conditions\n",
    "            # Same, but for each experimental condition separately\n",
    "            distance_and_diff_dict[dict_storage_current_key][\n",
    "                'distance']['other_exp_separately'] = {}\n",
    "            for exp_cond_i_idx, exp_cond_i_vals in temp_df_exp_obs_each_separately.items():\n",
    "                temp_mat = np.zeros((exp_cond_i_vals.shape[0], len(observations)))\n",
    "                for i, obs_metric in enumerate(observations):\n",
    "                    temp_mat[:,i] = exp_cond_i_vals[f\"{obs_metric}\"]\n",
    "                temp_euc_dist = np.linalg.norm(\n",
    "                    temp_mat - exp_obs_vector[np.newaxis, :],\n",
    "                    axis=1)\n",
    "                distance_and_diff_dict[dict_storage_current_key][\n",
    "                    'distance']['other_exp_separately'][exp_cond_i_idx] = temp_euc_dist\n",
    "\n",
    "            # Start plotting ############################\n",
    "            N = len(observations)  # =4\n",
    "            n_grid_rows = math.floor(math.sqrt(N))  # =2\n",
    "            n_grid_cols = math.ceil (math.sqrt(N))  # =2\n",
    "            total_rows  = 1 + n_grid_rows           # =3\n",
    "\n",
    "            fig = plt.figure(figsize=(16, 20))\n",
    "            gs  = fig.add_gridspec(total_rows, n_grid_cols,\n",
    "                                height_ratios=[1] + [1]*n_grid_rows,\n",
    "                                hspace=0.4, wspace=0.3)\n",
    "            \n",
    "            # 1) main axes spanning the full width - euclidean distance in \"observation/metric space\"\n",
    "            ax_main = fig.add_subplot(gs[0, :])\n",
    "            # Define a common x‐axis grid spanning the full range\n",
    "            all_data = np.concatenate([euc_dist_posterior, euc_dist_prior, euc_dist_other_experimental_conditions])\n",
    "            x_min, x_max = 0, all_data.max() / 2 # all_data.min(), all_data.max() / 2 # dividing the max by two seems a good cutoff\n",
    "            x_grid = np.linspace(x_min, x_max, 200)\n",
    "            # Fit a KDE to each (automatic bandwidth)\n",
    "            if euc_dist_posterior.shape[0] >= 1:\n",
    "                kde_euc_dist_posterior = gaussian_kde(euc_dist_posterior)\n",
    "                density_to_plot_posterior = kde_euc_dist_posterior(x_grid)\n",
    "            else:\n",
    "                density_to_plot_posterior = np.full((len(x_grid)),np.nan)\n",
    "            if euc_dist_prior.shape[0] >= 1:\n",
    "                kde_euc_dist_prior = gaussian_kde(euc_dist_prior)\n",
    "                density_to_plot_prior = kde_euc_dist_prior(x_grid)\n",
    "            else:\n",
    "                density_to_plot_prior = np.full((len(x_grid)),np.nan)\n",
    "            if euc_dist_other_experimental_conditions.shape[0] >= 1:\n",
    "                kde_euc_dist_other_experimental_conditions = gaussian_kde(euc_dist_other_experimental_conditions)\n",
    "                density_to_plot_other_exp = kde_euc_dist_other_experimental_conditions(x_grid)\n",
    "            else:\n",
    "                density_to_plot_other_exp = np.full((len(x_grid)),np.nan)\n",
    "            # Actually plot\n",
    "            ax_main.axvline(euc_dist_posterior_highest_likelihood[0], linewidth=2, color='blue', alpha=0.7, zorder=10,\n",
    "                        label=f\"Sim with highest likelihood\\nDist = {euc_dist_posterior_highest_likelihood[0]:.2f}\\n \")\n",
    "            ax_main.plot(x_grid, density_to_plot_posterior,\n",
    "                        color=\"#00A2FFFF\", alpha=1, zorder=1, linewidth=3,\n",
    "                        label=f\"Distribution of distances to experimental data for\\ndata simulated from posterior (= from inferred params)\\nMean dist = {np.mean(euc_dist_posterior):.2f}±{np.std(euc_dist_posterior):.2f}\\n \")\n",
    "            ax_main.plot(x_grid, density_to_plot_prior,\n",
    "                        color=\"#AA69FF\", alpha=1, zorder=0, linewidth=3,\n",
    "                        label=f\"Distribution of distances to experimental data for\\ndata simulated from prior (= random params, null model)\\nMean dist = {np.mean(euc_dist_prior):.2f}±{np.std(euc_dist_prior):.2f}\\n \")\n",
    "            ax_main.plot(x_grid, density_to_plot_other_exp,\n",
    "                        color=\"#FFC400\", alpha=1, zorder=-1, linewidth=3,\n",
    "                        label=f\"Distribution of distances to experimental data for\\nexperimental data from other conditions (each MN)\\nMean dist = {np.nanmean(euc_dist_other_experimental_conditions):.2f}±{np.nanstd(euc_dist_other_experimental_conditions):.2f}\\n \")\n",
    "            #\n",
    "            ax_main.legend(fontsize=\"small\")\n",
    "            ax_main.set_ylabel(\"Density\")\n",
    "            ax_main.set_xlabel(\"Euclidean distance in the (standardized) observation/metric space\")\n",
    "            ax_main.set_title(f\"Subject: {subject_title}, Muscle pair: {muscle_pair_i}, Intensity: {intensity_i}%\\n \\nEuclidean distance in observation-space\")\n",
    "\n",
    "            # 2) detail axes in the grid below - Difference for each metric/observation\n",
    "            detail_axes = []\n",
    "            distance_and_diff_dict[dict_storage_current_key]['difference'] = {\n",
    "                'posterior': {},\n",
    "                'posterior_highest_likelihood': {},\n",
    "                'prior': {},\n",
    "                'other_exp_pooled': {},\n",
    "                'other_exp_separately': {}\n",
    "            }\n",
    "            for i in range(N):\n",
    "                # Create plot\n",
    "                # row = 1 or 2, col = 0 or 1\n",
    "                row = 1 + i // n_grid_cols\n",
    "                col = i %  n_grid_cols\n",
    "                ax = fig.add_subplot(gs[row, col])\n",
    "                # Data to display\n",
    "                obs_exp_mean = temp_df_exp_obs[f\"{observations[i]}_mean\"].iloc[0]\n",
    "                obs_exp_std = temp_df_exp_obs[f\"{observations[i]}_std\"].iloc[0]\n",
    "                obs_sim_posterior_highest_likelihood_mean = temp_df_sim_obs_posterior_highest_likelihood[f\"{observations[i]}\"].mean()\n",
    "                obs_sim_posterior_highest_likelihood_std = temp_df_sim_obs_posterior_highest_likelihood[f\"{observations[i]}\"].std()\n",
    "                abs_dist_posterior = np.abs(obs_exp_mean - temp_df_sim_obs_posterior[f\"{observations[i]}_mean\"].values)\n",
    "                distance_and_diff_dict[dict_storage_current_key][\n",
    "                    'difference']['posterior'][observations[i]] = abs_dist_posterior\n",
    "                abs_dist_posterior_max_likelihood = np.abs(obs_exp_mean - temp_df_sim_obs_posterior_highest_likelihood[f\"{observations[i]}\"].values)\n",
    "                distance_and_diff_dict[dict_storage_current_key][\n",
    "                    'difference']['posterior_highest_likelihood'][observations[i]] = abs_dist_posterior_max_likelihood\n",
    "                abs_dist_prior = np.abs(obs_exp_mean - temp_df_sim_obs_prior[f\"{observations[i]}_mean\"].values)\n",
    "                distance_and_diff_dict[dict_storage_current_key][\n",
    "                    'difference']['prior'][observations[i]] = abs_dist_prior\n",
    "                abs_dist_other_exp_obs_pooled = np.abs(obs_exp_mean - temp_df_exp_other_obs[f\"{observations[i]}\"].values)\n",
    "                distance_and_diff_dict[dict_storage_current_key][\n",
    "                    'difference']['other_exp_pooled'][observations[i]] = abs_dist_other_exp_obs_pooled\n",
    "                # Fill dict with difference for other experimental observations, separately\n",
    "                for other_cond_i_idx, other_cond_i_df in temp_df_exp_obs_each_separately.items():\n",
    "                    if other_cond_i_idx not in distance_and_diff_dict[dict_storage_current_key]['difference']['other_exp_separately'].keys():\n",
    "                        distance_and_diff_dict[dict_storage_current_key]['difference']['other_exp_separately'][other_cond_i_idx] = {}\n",
    "                    distance_and_diff_dict[dict_storage_current_key]['difference'][\n",
    "                        'other_exp_separately'][other_cond_i_idx][observations[i]] = np.abs(\n",
    "                        obs_exp_mean - other_cond_i_df[f\"{observations[i]}\"].values)\n",
    "                # exp data\n",
    "                ax.axvline(obs_exp_mean, linewidth=2, color='red', alpha=0.7, zorder=10,\n",
    "                           label=f\"Experimental observations (each MN)\\nMean = {obs_exp_mean:.2f}±{obs_exp_std:.2f}\\n \")\n",
    "                ax.axvline(obs_exp_mean-obs_exp_std, linewidth=2, color='red', linestyle='--', alpha=0.3, zorder=10)\n",
    "                ax.axvline(obs_exp_mean+obs_exp_std, linewidth=2, color='red', linestyle='--', alpha=0.3, zorder=10)\n",
    "                ax.axvspan(xmin=obs_exp_mean-obs_exp_std, xmax=obs_exp_mean+obs_exp_std, color=\"red\", alpha=0.1, zorder=-10)\n",
    "                # sim data highest likelihood posterior\n",
    "                ax.axvline(obs_sim_posterior_highest_likelihood_mean, linewidth=2, color='blue', alpha=0.7, zorder=10,\n",
    "                           label=f\"Simulated obs from highest-likelihood\\nparameters from posterior (each MN)\\nMean = {obs_sim_posterior_highest_likelihood_mean:.2f}±{obs_sim_posterior_highest_likelihood_std:.2f}\\n \")\n",
    "                ax.axvline(obs_sim_posterior_highest_likelihood_mean-obs_sim_posterior_highest_likelihood_std, linewidth=2, color='blue', linestyle='--', alpha=0.3, zorder=10)\n",
    "                ax.axvline(obs_sim_posterior_highest_likelihood_mean+obs_sim_posterior_highest_likelihood_std, linewidth=2, color='blue', linestyle='--', alpha=0.3, zorder=10)\n",
    "                ax.axvspan(xmin=obs_sim_posterior_highest_likelihood_mean-obs_sim_posterior_highest_likelihood_std, xmax=obs_sim_posterior_highest_likelihood_mean+obs_sim_posterior_highest_likelihood_std, color=\"blue\", alpha=0.1, zorder=-10)\n",
    "                # Compte the KDEs to dsiplay\n",
    "                # Define a common x‐axis grid spanning the full range\n",
    "                temp_obs_posterior = temp_df_sim_obs_posterior[f\"{observations[i]}_mean\"].values\n",
    "                temp_obs_posterior = temp_obs_posterior[np.isfinite(temp_obs_posterior)]\n",
    "                temp_obs_prior = temp_df_sim_obs_prior[f\"{observations[i]}_mean\"].values\n",
    "                temp_obs_prior = temp_obs_prior[np.isfinite(temp_obs_prior)]\n",
    "                temp_obs_others = temp_df_exp_other_obs[f\"{observations[i]}\"].values\n",
    "                temp_obs_others = temp_obs_others[np.isfinite(temp_obs_others)]\n",
    "                all_data = np.concatenate([temp_obs_posterior,\n",
    "                                           temp_obs_prior,\n",
    "                                           temp_obs_others])\n",
    "                x_min, x_max = all_data.min(), all_data.max()\n",
    "                x_grid = np.linspace(x_min, x_max, 200)\n",
    "                # Fit a KDE to each (automatic bandwidth)\n",
    "                if len(temp_obs_posterior) >= 1:\n",
    "                    kde_sim_posterior = gaussian_kde(temp_obs_posterior)\n",
    "                    density_to_plot_posterior = kde_sim_posterior(x_grid)\n",
    "                else:\n",
    "                    density_to_plot_posterior = np.full((len(x_grid)),np.nan)\n",
    "                if len(temp_obs_prior) >= 1:\n",
    "                    kde_sim_prior = gaussian_kde(temp_obs_prior)\n",
    "                    density_to_plot_prior = kde_sim_prior(x_grid)\n",
    "                else:\n",
    "                    density_to_plot_prior = np.full((len(x_grid)),np.nan)\n",
    "                if len(temp_obs_others) >= 1:\n",
    "                    kde_other_experimental_conditions = gaussian_kde(temp_obs_others)\n",
    "                    density_to_plot_other_exp = kde_other_experimental_conditions(x_grid)\n",
    "                else:\n",
    "                    density_to_plot_other_exp = np.full((len(x_grid)),np.nan)\n",
    "                # sim data posterior\n",
    "                ax.plot(x_grid, density_to_plot_posterior,\n",
    "                        color=\"#00A2FFFF\", alpha=1, zorder=1, linewidth=3,\n",
    "                        label=f\"Distribution of means for\\ndata simulated from posterior\\n(= from inferred params)\\nMean diff = {np.mean(abs_dist_posterior):.2f}±{np.std(abs_dist_posterior):.2f}\\n \")\n",
    "                # sim data prior\n",
    "                ax.plot(x_grid, density_to_plot_prior,\n",
    "                        color=\"#AA69FF\", alpha=1, zorder = 0, linewidth=3,\n",
    "                        label=f\"Distribution of means for\\ndata simulated from prior\\n(= random params, null model)\\nMean diff = {np.mean(abs_dist_prior):.2f}±{np.std(abs_dist_prior):.2f}\\n \")\n",
    "                # distance to experimental data from other conditions\n",
    "                ax.plot(x_grid, density_to_plot_other_exp,\n",
    "                        color=\"#FFC400\", alpha=1, zorder = -1, linewidth=3,\n",
    "                        label=f\"Distribution experimental data\\nfrom other conditions (each MN)\\nMean diff = {np.nanmean(abs_dist_other_exp_obs_pooled):.2f}±{np.nanstd(abs_dist_other_exp_obs_pooled):.2f}\\n \")\n",
    "                #\n",
    "                ax.set_ylabel(\"Density\")\n",
    "                ax.set_xlabel(f\"Standardized {observations[i]}\\n(pooled experimental observations are transformed\\nto have mean = 0, std = 1)\")\n",
    "                ax.legend(fontsize=\"small\")\n",
    "                ax.set_title(f\"{observations[i]}\")\n",
    "                detail_axes.append(ax)\n",
    "\n",
    "            plt.savefig(f\"{path_to_save_into}\\\\{re.sub(r'[^0-9A-Za-z._-]+', '', str(muscle_pair_i))}_{intensity_i}_{subject_title}.png\")\n",
    "            plt.savefig(f\"{path_to_save_into}\\\\{re.sub(r'[^0-9A-Za-z._-]+', '', str(muscle_pair_i))}_{intensity_i}_{subject_title}.svg\")      \n",
    "            plt.close()\n",
    "            # plt.show()\n",
    "            # plt.close()\n",
    "\n",
    "            ### Display the direct histogram comparisons\n",
    "            def sanitize(x): # Function to recreate the appropriate key string from the loop\n",
    "                # 1) If it looks like a number, cast to float, then to int if possible\n",
    "                try:\n",
    "                    val = float(x)\n",
    "                except (TypeError, ValueError):\n",
    "                    s = str(x)\n",
    "                else:\n",
    "                    if val.is_integer():\n",
    "                        return str(int(val))\n",
    "                    return str(val)\n",
    "                # 2) Otherwise it's a string: replace your arrows with hyphens\n",
    "                s = re.sub(r'<->', '-', s)\n",
    "                # 3) (Optional) drop anything except letters, digits, dash, or underscore\n",
    "                s = re.sub(r'[^A-Za-z0-9\\-_]', '', s)\n",
    "                return s\n",
    "            #\n",
    "            if not filtering_subjects:\n",
    "                subject_str = ''\n",
    "            else:\n",
    "                subject_str = f\"{subject_i}_\"\n",
    "            if perspective == 'other_MUs_as_ref':\n",
    "                perspective_name_for_cross_hist = 'inhibited'\n",
    "            elif perspective == 'MU_as_ref':\n",
    "                perspective_name_for_cross_hist = 'inhibiting'\n",
    "\n",
    "            ### START THE FIGURE\n",
    "            plt.figure(figsize=(6,12))\n",
    "            ## SUBPLOT 1) Experimentally observed cross-histograms\n",
    "            plt.subplot(2,1,1)\n",
    "            # Loop through exp_analysis_results_dict and find matching\n",
    "            temp_list_of_exp_MNs_cross_hist = []\n",
    "            for exp_analysis_key, exp_analysis_result in exp_analysis_results_dict.items():\n",
    "                if 'Cross_histograms' not in exp_analysis_result.keys():\n",
    "                    continue\n",
    "                # Check if correct subject - only if exp_analysis_key==True\n",
    "                if filtering_subjects:\n",
    "                    if subject_i not in exp_analysis_key:\n",
    "                        continue\n",
    "                # Check if correct intensity\n",
    "                if str(np.round(intensity_i).astype(int)) not in exp_analysis_key:\n",
    "                    continue\n",
    "                # Check if correct muscle pair\n",
    "                if muscle_pair_i not in exp_analysis_result['Cross_histograms']['cross_histograms']:\n",
    "                    continue\n",
    "                # Now load all relevant MNs in the list\n",
    "                for exp_mn_idx, exp_mn_cross_hist in exp_analysis_result['Cross_histograms']['cross_histograms'][muscle_pair_i].items():\n",
    "                    # Check if the histogram match the filtering criterion\n",
    "                    n_spikes_temp = exp_analysis_result['Cross_histograms'][muscle_pair_i][exp_mn_idx][perspective_name_for_cross_hist]['n_spikes']\n",
    "                    r2_full_temp = exp_analysis_result['Cross_histograms'][muscle_pair_i][exp_mn_idx][perspective_name_for_cross_hist]['r2_full']\n",
    "                    r2_base_temp = exp_analysis_result['Cross_histograms'][muscle_pair_i][exp_mn_idx][perspective_name_for_cross_hist]['r2_base']\n",
    "                    if (n_spikes_temp < min_nb_spikes[\"experiment\"]) or (r2_full_temp < min_r2_for_overall_curve_fit[\"experiment\"]) or (r2_base_temp < min_r2_for_baseline_curve_fit[\"experiment\"]):\n",
    "                        continue\n",
    "                    temp_list_of_exp_MNs_cross_hist.append(exp_mn_cross_hist[perspective_name_for_cross_hist])\n",
    "            if len(temp_list_of_exp_MNs_cross_hist) > 0:\n",
    "                hist_samples_length = len(temp_list_of_exp_MNs_cross_hist[0])\n",
    "                cross_hist_time = np.linspace(start=-200, stop=200, num=hist_samples_length)\n",
    "                for mn_i in range(len(temp_list_of_exp_MNs_cross_hist)):\n",
    "                        plt.plot(cross_hist_time, temp_list_of_exp_MNs_cross_hist[mn_i],\n",
    "                            color=colors_dict[muscle_pair_i], alpha=0.05, linewidth=1)\n",
    "                            # alpha=4/len(temp_list_of_exp_MNs_cross_hist), linewidth=1.5)\n",
    "                # plot mean histogram over all MNs\n",
    "                mean_exp_cross_hist = np.nanmean(np.array(temp_list_of_exp_MNs_cross_hist), axis=0)\n",
    "                # plt.plot(cross_hist_time, mean_exp_cross_hist,\n",
    "                #             color=colors_dict[muscle_pair_i], alpha=1, linewidth=3)\n",
    "            plt.title(f\"Experimental data\\n(n = {len(temp_list_of_exp_MNs_cross_hist)} MNs)\")\n",
    "            plt.xlabel(\"Time (ms)\")\n",
    "            plt.ylabel(\"Firing probability\")\n",
    "            exp_subplot_ymax = plt.ylim()[1]\n",
    "            ## SUBPLOT 2) Simulated cross-histograms from posterior estimated parameters\n",
    "            plt.subplot(2,1,2)\n",
    "            sim_subplot_ymax = 0\n",
    "            key_to_load_best_sim = f\"{subject_str}{sanitize(muscle_pair_i)}_{sanitize(intensity_i)}_simHighestLikelihood\"\n",
    "            #       # Continue further only if the key exists in the \"best_sim\" to load\n",
    "            left, right = map(str.strip, muscle_pair_i.split('<->', 1))\n",
    "            pool_pair_key = 'pool_0<->pool_0' if left == right else 'pool_0<->pool_1'\n",
    "            if key_to_load_best_sim in best_sims.keys():\n",
    "                best_sim_histograms = best_sims[key_to_load_best_sim]['Cross_histograms']['cross_histograms'][pool_pair_key]\n",
    "                hist_samples_length = len(best_sim_histograms[0][perspective_name_for_cross_hist])\n",
    "                cross_hist_time = np.linspace(start=-200, stop=200, num=hist_samples_length)\n",
    "                sum_hist_for_mean = np.zeros_like(cross_hist_time)\n",
    "                num_valid_mns = 0\n",
    "                for mn_i in best_sim_histograms.keys():\n",
    "                    # Only plot valid MNs (fit R² and spike nb meet the condtions)\n",
    "                    n_spikes_temp = best_sims[key_to_load_best_sim]['Cross_histograms'][pool_pair_key][mn_i][perspective_name_for_cross_hist]['n_spikes']\n",
    "                    r2_full_temp = best_sims[key_to_load_best_sim]['Cross_histograms'][pool_pair_key][mn_i][perspective_name_for_cross_hist]['r2_full']\n",
    "                    r2_base_temp = best_sims[key_to_load_best_sim]['Cross_histograms'][pool_pair_key][mn_i][perspective_name_for_cross_hist]['r2_base']\n",
    "                    if (n_spikes_temp < min_nb_spikes[\"simulation\"]) or (r2_full_temp < min_r2_for_overall_curve_fit[\"simulation\"]) or (r2_base_temp < min_r2_for_baseline_curve_fit[\"simulation\"]):\n",
    "                        continue\n",
    "                    sum_hist_for_mean += best_sim_histograms[mn_i][perspective_name_for_cross_hist]\n",
    "                    num_valid_mns += 1\n",
    "                    # plot each MN's histogram\n",
    "                    plt.plot(cross_hist_time, best_sim_histograms[mn_i][perspective_name_for_cross_hist],\n",
    "                            color=colors_dict[muscle_pair_i], alpha=0.1, linewidth=1) # alpha=3/len(best_sim_histograms), linewidth=2)\n",
    "                sim_subplot_ymax = plt.ylim()[1]\n",
    "                # plot mean histogram over all MNs\n",
    "                if num_valid_mns >= 1:\n",
    "                    mean_exp_cross_hist = sum_hist_for_mean / num_valid_mns\n",
    "                    # plt.plot(cross_hist_time, mean_exp_cross_hist,\n",
    "                    #             color=colors_dict[muscle_pair_i], alpha=1, linewidth=3)\n",
    "            plt.title(f\"Simulation from posterior\\n(inferred parameters /w highest likelihood)\")\n",
    "            plt.xlabel(\"Time (ms)\")\n",
    "            plt.ylabel(\"Firing probability\")\n",
    "            # Get same Y limits for both subplots\n",
    "            ylim_to_select = np.max([exp_subplot_ymax,sim_subplot_ymax])\n",
    "            plt.subplot(2,1,1)\n",
    "            # plt.axvline(x=0, ymin=0, ymax=1, color='k', linestyle='--', linewidth=1.5, alpha=0.2, zorder=-10)\n",
    "            plt.ylim(-0.0002, ylim_to_select)\n",
    "            plt.subplot(2,1,2)\n",
    "            # plt.axvline(x=0, ymin=0, ymax=1, color='k', linestyle='--', linewidth=1.5, alpha=0.2, zorder=-10)\n",
    "            plt.ylim(-0.0002, ylim_to_select)\n",
    "            # \n",
    "            plt.suptitle(f\"Subject: {subject_title}, Muscle pair: {muscle_pair_i}, Intensity: {intensity_i}%\\nCross-histogram comparisons\")\n",
    "            # plt.tight_layout()\n",
    "            plt.savefig(f\"{path_to_save_into}\\\\{re.sub(r'[^0-9A-Za-z._-]+', '', str(muscle_pair_i))}_{intensity_i}_{subject_title}_cross_histograms.png\")\n",
    "            plt.savefig(f\"{path_to_save_into}\\\\{re.sub(r'[^0-9A-Za-z._-]+', '', str(muscle_pair_i))}_{intensity_i}_{subject_title}_cross_histograms.svg\")\n",
    "            plt.show()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd6289",
   "metadata": {},
   "source": [
    "# Example display of simulated data in 2D feature-subspace\n",
    "(part of Fig. 4 C in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_grid_marginals(\n",
    "    df, xcol, ycol,\n",
    "    *,\n",
    "    # data scaling / normalization (used for binning/KDE + default axis extents)\n",
    "    x_range=None, y_range=None,            # (min, max) or None\n",
    "    # explicit display bounds (override axis limits on all three panels)\n",
    "    xlim=None, ylim=None,                  # (min, max) or None\n",
    "    # title & colors\n",
    "    main_title=None,\n",
    "    x_color=\"#FF9100\",\n",
    "    y_color=\"#0077FF\",\n",
    "    base_color=\"#808080\",\n",
    "    # per-axis color scales (list of >=2 colors). If None → [base_color, x_color]/[base_color, y_color]\n",
    "    colors_for_x_axis_colorscale=None,\n",
    "    colors_for_y_axis_colorscale=None,\n",
    "    # point color blending of per-axis colors\n",
    "    blend_space=\"rgb\",                     # \"rgb\" or \"lab\"\n",
    "    point_size=18,\n",
    "    alpha_pts=0.35,\n",
    "    # marginals control\n",
    "    marginal_mode=\"hist\",                  # \"hist\" or \"kde\"\n",
    "    bins=60,                               # used in hist mode\n",
    "    n_grid=400,                            # used in *marginal* kde mode\n",
    "    kde_boundary=\"reflect\",                # \"reflect\" or \"none\"\n",
    "    hist_max_count=None,                   # cap for BOTH marginal panels (ignored in KDE mode)\n",
    "    # overlays (legacy)\n",
    "    overlay_points=None,                   # dicts with x,y,color,label\n",
    "    overlay_vlines=None,                   # list[(x, color)] on BR (X marginal)\n",
    "    overlay_hlines=None,                   # list[(y, color)] on TL (Y marginal)\n",
    "    # global scatter KDE overlay (legacy)\n",
    "    scatter_kde=False,\n",
    "    scatter_kde_levels=(0.2, 0.5, 0.8),\n",
    "    scatter_kde_bw=None,\n",
    "    scatter_kde_grid=200,\n",
    "    scatter_kde_color=\"#222222\",\n",
    "    scatter_kde_lw=1.5,\n",
    "    scatter_kde_alpha=0.9,\n",
    "    scatter_kde_max_points=8000,\n",
    "    scatter_kde_level_alphas=None,\n",
    "    scatter_kde_alpha_min=0.3,\n",
    "    scatter_kde_alpha_max=0.95,\n",
    "\n",
    "    # ---- NEW: filtered per-condition mode ----\n",
    "    dataset_mode=\"full_dataset\",           # \"full_dataset\" | \"experimental_condition_filter\"\n",
    "    condition_filters=None,                # dict[str -> list]; matched across keys, same length lists\n",
    "    condition_colors=None,                 # list of colors, same len as lists in condition_filters\n",
    "    condition_colors_scatter_kde=None,\n",
    "    group_scatter_kde=False,               # draw per-group 2D KDE (only if experimental_condition_filter)\n",
    "    group_kde_levels=(0.25, 0.5, 0.75),\n",
    "    group_kde_alpha=0.9,\n",
    "    group_kde_lw=2.0,\n",
    "\n",
    "    # ---- NEW: overlay summary points from another DF (works in both modes) ----\n",
    "    overlay_summary_df=None,               # DataFrame containing summary xcol/ycol\n",
    "    overlay_summary_conditions=None,       # dict[str -> list] same shaping as condition_filters\n",
    "    overlay_summary_colors=None,           # list of colors (same length)\n",
    "    overlay_summary_marker=\"X\",\n",
    "    overlay_summary_size=90,\n",
    "    overlay_summary_edge=\"k\",\n",
    "    overlay_summary_lw=1.0,\n",
    "\n",
    "    # figure / IO\n",
    "    figsize=(8.4, 8.4),\n",
    "    savepath=None,\n",
    "    csv_prefix=None\n",
    "):\n",
    "    import numpy as np, pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _hex_to_rgb01(h: str):\n",
    "        h = h.strip()\n",
    "        if h.startswith(\"#\"): h = h[1:]\n",
    "        if len(h)==3: h = \"\".join([c*2 for c in h])\n",
    "        return (int(h[0:2],16)/255.0, int(h[2:4],16)/255.0, int(h[4:6],16)/255.0)\n",
    "\n",
    "    def _to_rgb01(c):\n",
    "        if isinstance(c, str): return _hex_to_rgb01(c)\n",
    "        if isinstance(c, (list, tuple)) and len(c)==3: return tuple(float(v) for v in c)\n",
    "        raise ValueError(\"Colors must be hex strings like '#AABBCC' or RGB tuples in 0..1\")\n",
    "\n",
    "    def _interp_color_list(color_list, t):\n",
    "        cols = np.array([_to_rgb01(c) for c in color_list], dtype=float)\n",
    "        N = len(cols)\n",
    "        if N < 2: raise ValueError(\"Provide at least two colors per axis colormap.\")\n",
    "        t = float(np.clip(t, 0.0, 1.0))\n",
    "        if t <= 0:  return tuple(cols[0])\n",
    "        if t >= 1:  return tuple(cols[-1])\n",
    "        pos = t * (N - 1); i0 = int(np.floor(pos)); i1 = i0 + 1; frac = pos - i0\n",
    "        return tuple((1-frac)*cols[i0] + frac*cols[i1])\n",
    "\n",
    "    def _norm_to_unit(vals, vmin=None, vmax=None):\n",
    "        vals = np.asarray(vals, float)\n",
    "        lo = np.nanmin(vals) if vmin is None else float(vmin)\n",
    "        hi = np.nanmax(vals) if vmax is None else float(vmax)\n",
    "        if hi <= lo: return np.zeros_like(vals), (lo, hi)\n",
    "        return ((vals - lo) / (hi - lo)), (lo, hi)\n",
    "\n",
    "    def _gaussian_kde_1d(x, grid, bw=None):\n",
    "        x = np.asarray(x, float); x = x[np.isfinite(x)]\n",
    "        n = x.size\n",
    "        if n < 2: return np.zeros_like(grid, dtype=float)\n",
    "        std = np.std(x, ddof=1)\n",
    "        if std <= 0:\n",
    "            mu = float(x[0]); s = (grid.max() - grid.min()) * 1e-3 or 1e-6\n",
    "            return np.exp(-0.5*((grid-mu)/s)**2)/(s*np.sqrt(2*np.pi))\n",
    "        if bw is None: bw = std * n**(-1/5)  # Scott\n",
    "        z = (grid[:,None] - x[None,:]) / bw\n",
    "        dens = np.exp(-0.5*z*z).sum(axis=1) / (n * bw * np.sqrt(2*np.pi))\n",
    "        return dens\n",
    "\n",
    "    def _kde_bounded_reflect(x, grid, lo, hi, bw=None):\n",
    "        x = np.asarray(x, float); x = x[np.isfinite(x)]\n",
    "        if x.size == 0: return np.zeros_like(grid)\n",
    "        x_aug = np.concatenate([x, 2*lo - x, 2*hi - x])  # reflect at bounds\n",
    "        dens = _gaussian_kde_1d(x_aug, grid, bw=bw)\n",
    "        area = np.trapz(dens, grid)\n",
    "        if area > 0: dens = dens / area\n",
    "        return dens\n",
    "\n",
    "    def _save_pairplot_grid_csvs(basepath_no_ext, *, mode, x_label, y_label,\n",
    "                                 x=None, y=None,\n",
    "                                 x_grid=None, x_kde=None,\n",
    "                                 y_grid=None, y_kde=None,\n",
    "                                 x_hist=None, y_hist=None,\n",
    "                                 overlay_points=None):\n",
    "        if x is not None and y is not None:\n",
    "            pd.DataFrame({x_label: x, y_label: y}).to_csv(basepath_no_ext + \"_scatter.csv\", index=False)\n",
    "        if mode == \"kde\":\n",
    "            if x_grid is not None and x_kde is not None:\n",
    "                pd.DataFrame({\"x\": x_grid, \"density\": x_kde}).to_csv(basepath_no_ext + \"_kde_x.csv\", index=False)\n",
    "            if y_grid is not None and y_kde is not None:\n",
    "                pd.DataFrame({\"y\": y_grid, \"density\": y_kde}).to_csv(basepath_no_ext + \"_kde_y.csv\", index=False)\n",
    "        elif mode == \"hist\":\n",
    "            if x_hist is not None:\n",
    "                counts, edges = x_hist\n",
    "                pd.DataFrame({\"bin_left\": edges[:-1], \"bin_right\": edges[1:], \"count\": counts.astype(float)}).to_csv(\n",
    "                    basepath_no_ext + \"_hist_x.csv\", index=False\n",
    "                )\n",
    "            if y_hist is not None:\n",
    "                counts, edges = y_hist\n",
    "                pd.DataFrame({\"bin_left\": edges[:-1], \"bin_right\": edges[1:], \"count\": counts.astype(float)}).to_csv(\n",
    "                    basepath_no_ext + \"_hist_y.csv\", index=False\n",
    "                )\n",
    "        if overlay_points:\n",
    "            pd.DataFrame([{\n",
    "                x_label: float(p[\"x\"]), y_label: float(p[\"y\"]),\n",
    "                \"label\": p.get(\"label\",\"\"), \"color\": p.get(\"color\",\"\")\n",
    "            } for p in overlay_points]).to_csv(basepath_no_ext + \"_overlay_points.csv\", index=False)\n",
    "\n",
    "    def _make_group_masks(df, cond_dict):\n",
    "        \"\"\"Return list of (mask, label) for each aligned condition across keys.\"\"\"\n",
    "        keys = list(cond_dict.keys())\n",
    "        lens = {k: len(cond_dict[k]) for k in keys}\n",
    "        if len(set(lens.values())) != 1:\n",
    "            raise ValueError(\"All lists in condition_filters/overlay_summary_conditions must have the same length.\")\n",
    "        L = next(iter(lens.values()))\n",
    "        out = []\n",
    "        for i in range(L):\n",
    "            m = np.ones(len(df), dtype=bool)\n",
    "            parts = []\n",
    "            for k in keys:\n",
    "                v = cond_dict[k][i]\n",
    "                m &= (df[k] == v)\n",
    "                parts.append(f\"{k}={v}\")\n",
    "            label = \", \".join(parts)\n",
    "            out.append((m, label))\n",
    "        return out\n",
    "\n",
    "    # ---------- data & ranges ----------\n",
    "    x_all = np.asarray(df[xcol], float)\n",
    "    y_all = np.asarray(df[ycol], float)\n",
    "\n",
    "    # ranges for binning/KDE\n",
    "    xn, (xmin_data, xmax_data) = _norm_to_unit(x_all, *(x_range or (None, None)))\n",
    "    yn, (ymin_data, ymax_data) = _norm_to_unit(y_all, *(y_range or (None, None)))\n",
    "\n",
    "    # visual bounds\n",
    "    xmin_plot, xmax_plot = (xlim if xlim is not None else (xmin_data, xmax_data))\n",
    "    ymin_plot, ymax_plot = (ylim if ylim is not None else (ymin_data, ymax_data))\n",
    "\n",
    "    # ---------- per-axis color scales (for FULL mode point coloring) ----------\n",
    "    if colors_for_x_axis_colorscale is None:\n",
    "        colors_for_x_axis_colorscale = [base_color, x_color]\n",
    "    if colors_for_y_axis_colorscale is None:\n",
    "        colors_for_y_axis_colorscale = [base_color, y_color]\n",
    "\n",
    "    xn_c = np.clip(np.nan_to_num(xn, nan=0.0), 0.0, 1.0)\n",
    "    yn_c = np.clip(np.nan_to_num(yn, nan=0.0), 0.0, 1.0)\n",
    "    cx = np.array([_interp_color_list(colors_for_x_axis_colorscale, t) for t in xn_c], dtype=float)\n",
    "    cy = np.array([_interp_color_list(colors_for_y_axis_colorscale, t) for t in yn_c], dtype=float)\n",
    "\n",
    "    # blend per-point colors for full dataset mode\n",
    "    def _blend_rgb(cx_arr, cy_arr, xn_c, yn_c):\n",
    "        wy = yn_c / np.clip(xn_c + yn_c, 1e-12, None)  # Y contribution\n",
    "        wx = 1.0 - wy\n",
    "        if blend_space.lower() == \"lab\":\n",
    "            try:\n",
    "                from skimage.color import rgb2lab, lab2rgb\n",
    "                cx_lab = rgb2lab(np.clip(cx_arr.reshape(-1,1,3), 0, 1)).reshape(-1,3)\n",
    "                cy_lab = rgb2lab(np.clip(cy_arr.reshape(-1,1,3), 0, 1)).reshape(-1,3)\n",
    "                lab_mix = (wx[:,None]*cx_lab + wy[:,None]*cy_lab)\n",
    "                rgb = np.clip(lab2rgb(lab_mix.reshape(-1,1,3)).reshape(-1,3), 0, 1)\n",
    "                return rgb\n",
    "            except Exception:\n",
    "                print(\"[blend_space='lab'] falling back to RGB blend.\")\n",
    "        return np.clip(wx[:,None]*cx_arr + wy[:,None]*cy_arr, 0, 1)\n",
    "\n",
    "    rgb_full = _blend_rgb(cx, cy, xn_c, yn_c)\n",
    "\n",
    "    # ---------- layout ----------\n",
    "    fig = plt.figure(figsize=figsize, constrained_layout=False)\n",
    "    gs  = GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[1, 1],\n",
    "                   wspace=0.07, hspace=0.07, figure=fig)\n",
    "    ax_tl = fig.add_subplot(gs[0, 0])  # Y marginal\n",
    "    ax_tr = fig.add_subplot(gs[0, 1])  # scatter\n",
    "    ax_bl = fig.add_subplot(gs[1, 0])  # legend\n",
    "    ax_br = fig.add_subplot(gs[1, 1])  # X marginal\n",
    "\n",
    "    # ---------- plotting helpers ----------\n",
    "    def _plot_hist(ax, data, lo, hi, color=None, label=None, vertical=False):\n",
    "        counts, edges = np.histogram(data[np.isfinite(data)], bins=bins, range=(lo, hi))\n",
    "        if vertical:\n",
    "            # standard orientation (x = centers)\n",
    "            centers = 0.5*(edges[:-1] + edges[1:])\n",
    "            widths  = (edges[1:] - edges[:-1])\n",
    "            if color is None:\n",
    "                # gradient bars (full mode)\n",
    "                t = np.clip((centers - lo) / (hi - lo + 1e-12), 0, 1)\n",
    "                bar_colors = [ _interp_color_list(colors_for_y_axis_colorscale, ti) for ti in t ]\n",
    "                ax.bar(centers, counts, width=widths, color=bar_colors, edgecolor=\"white\", linewidth=1.5, alpha=0.8, align='center')\n",
    "            else:\n",
    "                # outline steps for groups\n",
    "                ax.stairs(counts, edges, fill=False, color=color, linewidth=2.0, label=label)\n",
    "        else:\n",
    "            centers = 0.5*(edges[:-1] + edges[1:])\n",
    "            widths  = (edges[1:] - edges[:-1])\n",
    "            if color is None:\n",
    "                t = np.clip((centers - lo) / (hi - lo + 1e-12), 0, 1)\n",
    "                bar_colors = [ _interp_color_list(colors_for_x_axis_colorscale, ti) for ti in t ]\n",
    "                ax.bar(centers, counts, width=widths, color=bar_colors, edgecolor=\"white\", linewidth=1.5, alpha=0.8, align='center')\n",
    "            else:\n",
    "                ax.stairs(counts, edges, fill=False, color=color, linewidth=2.0, label=label)\n",
    "        return counts, edges\n",
    "\n",
    "    def _plot_kde(ax, data, lo, hi, color, label=None):\n",
    "        grid = np.linspace(lo, hi, n_grid)\n",
    "        if kde_boundary == \"reflect\":\n",
    "            dens = _kde_bounded_reflect(data, grid, lo, hi, bw=None)\n",
    "        else:\n",
    "            dens = _gaussian_kde_1d(data, grid, bw=None)\n",
    "            area = np.trapz(dens, grid)\n",
    "            if area > 0: dens = dens/area\n",
    "        ax.plot(grid, dens, color=color, lw=2.2, label=label)\n",
    "        return grid, dens\n",
    "\n",
    "    def _kde2d(ax, X, Y, xlim, ylim, levels, color, alpha=0.9, lw=2.0):\n",
    "        if X.size < 2: return\n",
    "        X = X[np.isfinite(X)]; Y = Y[np.isfinite(Y)]\n",
    "        if X.size < 2: return\n",
    "        gx = np.linspace(xlim[0], xlim[1], scatter_kde_grid)\n",
    "        gy = np.linspace(ylim[0], ylim[1], scatter_kde_grid)\n",
    "        XX, YY = np.meshgrid(gx, gy)\n",
    "        sx = np.std(X, ddof=1); sy = np.std(Y, ddof=1)\n",
    "        if sx <= 0 or sy <= 0:\n",
    "            sx = (xlim[1] - xlim[0]) * 1e-3 or 1e-6\n",
    "            sy = (ylim[1] - ylim[0]) * 1e-3 or 1e-6\n",
    "        n = X.size; h_scott = n ** (-1/6)\n",
    "        bx = sx * h_scott; by = sy * h_scott\n",
    "        dens = np.zeros_like(XX, dtype=float)\n",
    "        chunk = max(1, int(2e5 // XX.size))\n",
    "        for start in range(0, n, chunk):\n",
    "            end = min(n, start+chunk)\n",
    "            dx = (XX[...,None] - X[None,None,start:end]) / bx\n",
    "            dy = (YY[...,None] - Y[None,None,start:end]) / by\n",
    "            dens += np.exp(-0.5*(dx*dx + dy*dy)).sum(axis=2)\n",
    "        dens /= (n * (2*np.pi*bx*by))\n",
    "        dmin, dmax = float(np.nanmin(dens)), float(np.nanmax(dens))\n",
    "        if dmax <= dmin: return\n",
    "        dens_norm = (dens - dmin) / (dmax - dmin)\n",
    "        cs = ax.contour(XX, YY, dens_norm, levels=levels, colors=color, linewidths=lw, alpha=alpha)\n",
    "        return cs\n",
    "\n",
    "    # ---------- FULL vs FILTERED ----------\n",
    "    show_filtered = (dataset_mode == \"experimental_condition_filter\" and condition_filters is not None)\n",
    "\n",
    "    # --- TL (Y marginal) ---\n",
    "    y_hist = y_hist_grid = y_hist_dens = None\n",
    "    if not show_filtered:\n",
    "        if marginal_mode == \"kde\":\n",
    "            y_hist_grid, y_hist_dens = _plot_kde(ax_tl, y_all, ymin_data, ymax_data, color=colors_for_y_axis_colorscale[-1], label=None)\n",
    "        else:\n",
    "            y_hist = _plot_hist(ax_tl, y_all, ymin_data, ymax_data, color=None, label=None, vertical=True)\n",
    "        ax_tl.set_xlim(ymin_plot, ymax_plot)\n",
    "        if hist_max_count is not None and marginal_mode==\"hist\":\n",
    "            ax_tl.set_ylim(0, float(hist_max_count))\n",
    "        ax_tl.set_ylabel(\"count\" if marginal_mode==\"hist\" else \"density\"); ax_tl.set_xlabel(ycol)\n",
    "    else:\n",
    "        # per-group overlays\n",
    "        if condition_colors is None:\n",
    "            raise ValueError(\"Provide condition_colors matching condition_filters lengths.\")\n",
    "        group_masks = _make_group_masks(df, condition_filters)\n",
    "        for (mask, label), color in zip(group_masks, condition_colors):\n",
    "            ys = y_all[mask]\n",
    "            if ys.size == 0: continue\n",
    "            if marginal_mode == \"kde\":\n",
    "                _plot_kde(ax_tl, ys, ymin_data, ymax_data, color=color, label=label)\n",
    "            else:\n",
    "                _plot_hist(ax_tl, ys, ymin_data, ymax_data, color=color, label=label, vertical=True)\n",
    "        ax_tl.set_xlim(ymin_plot, ymax_plot)\n",
    "        if hist_max_count is not None and marginal_mode==\"hist\":\n",
    "            ax_tl.set_ylim(0, float(hist_max_count))\n",
    "        ax_tl.set_ylabel(\"count\" if marginal_mode==\"hist\" else \"density\"); ax_tl.set_xlabel(ycol)\n",
    "\n",
    "    try: ax_tl.set_box_aspect(1)\n",
    "    except: pass\n",
    "    if overlay_hlines:\n",
    "        for yv, col in overlay_hlines:\n",
    "            ax_tl.axvline(yv, color=col, lw=2.0)\n",
    "\n",
    "    # --- BR (X marginal) ---\n",
    "    x_hist = x_hist_grid = x_hist_dens = None\n",
    "    if not show_filtered:\n",
    "        if marginal_mode == \"kde\":\n",
    "            x_hist_grid, x_hist_dens = _plot_kde(ax_br, x_all, xmin_data, xmax_data, color=colors_for_x_axis_colorscale[-1], label=None)\n",
    "        else:\n",
    "            x_hist = _plot_hist(ax_br, x_all, xmin_data, xmax_data, color=None, label=None, vertical=False)\n",
    "        ax_br.set_xlim(xmin_plot, xmax_plot)\n",
    "        if hist_max_count is not None and marginal_mode==\"hist\":\n",
    "            ax_br.set_ylim(0, float(hist_max_count))\n",
    "        ax_br.set_xlabel(xcol); ax_br.set_ylabel(\"count\" if marginal_mode==\"hist\" else \"density\")\n",
    "    else:\n",
    "        if condition_colors is None:\n",
    "            raise ValueError(\"Provide condition_colors matching condition_filters lengths.\")\n",
    "        group_masks = _make_group_masks(df, condition_filters)\n",
    "        for (mask, label), color in zip(group_masks, condition_colors):\n",
    "            xs = x_all[mask]\n",
    "            if xs.size == 0: continue\n",
    "            if marginal_mode == \"kde\":\n",
    "                _plot_kde(ax_br, xs, xmin_data, xmax_data, color=color, label=label)\n",
    "            else:\n",
    "                _plot_hist(ax_br, xs, xmin_data, xmax_data, color=color, label=label, vertical=False)\n",
    "        ax_br.set_xlim(xmin_plot, xmax_plot)\n",
    "        if hist_max_count is not None and marginal_mode==\"hist\":\n",
    "            ax_br.set_ylim(0, float(hist_max_count))\n",
    "        ax_br.set_xlabel(xcol); ax_br.set_ylabel(\"count\" if marginal_mode==\"hist\" else \"density\")\n",
    "\n",
    "    try: ax_br.set_box_aspect(1)\n",
    "    except: pass\n",
    "    if overlay_vlines:\n",
    "        for xv, col in overlay_vlines:\n",
    "            ax_br.axvline(xv, color=col, lw=2.0)\n",
    "\n",
    "    # --- TR (scatter) ---\n",
    "    if not show_filtered:\n",
    "        ax_tr.scatter(x_all, y_all, s=point_size, c=rgb_full, edgecolor='none', alpha=alpha_pts)\n",
    "        # optional single global KDE (legacy)\n",
    "        if scatter_kde:\n",
    "            _kde2d(ax_tr, x_all, y_all, (xmin_plot, xmax_plot), (ymin_plot, ymax_plot),\n",
    "                   levels=scatter_kde_levels, color=scatter_kde_color,\n",
    "                   alpha=scatter_kde_alpha, lw=scatter_kde_lw)\n",
    "    else:\n",
    "        group_masks = _make_group_masks(df, condition_filters)\n",
    "        n_groups = len(group_masks)\n",
    "        # n_groups = number of condition combinations you’re plotting\n",
    "        if condition_colors_scatter_kde is None:\n",
    "            kde_cols = [\"#000000\"] * n_groups    # default = all black\n",
    "        else:\n",
    "            if len(condition_colors_scatter_kde) != n_groups:\n",
    "                raise ValueError(\"condition_colors_scatter_kde must have the same length as the number of groups.\")\n",
    "            kde_cols = list(condition_colors_scatter_kde)\n",
    "        i = -1\n",
    "        for (mask, label), color in zip(group_masks, condition_colors):\n",
    "            i += 1\n",
    "            xs = x_all[mask]; ys = y_all[mask]\n",
    "            if xs.size == 0: continue\n",
    "            ax_tr.scatter(xs, ys, s=point_size, c=[color], edgecolor='none', alpha=alpha_pts, label=label)\n",
    "            if group_scatter_kde:\n",
    "                _kde2d(ax_tr, xs, ys, (xmin_plot, xmax_plot), (ymin_plot, ymax_plot),\n",
    "                       levels=group_kde_levels, color=kde_cols[i], alpha=group_kde_alpha, lw=group_kde_lw)\n",
    "\n",
    "    ax_tr.set_xlim(xmin_plot, xmax_plot); ax_tr.set_ylim(ymin_plot, ymax_plot)\n",
    "    ax_tr.set_xlabel(xcol); ax_tr.set_ylabel(ycol)\n",
    "    try: ax_tr.set_box_aspect(1)\n",
    "    except: pass\n",
    "\n",
    "    # --- overlay summary points/lines (works in both modes) ---\n",
    "    def _overlay_summary(df_sum, conds, cols, marker, s, edge, lw):\n",
    "        masks = _make_group_masks(df_sum, conds)\n",
    "        for (m, _), col in zip(masks, cols):\n",
    "            sub = df_sum[m]\n",
    "            if sub.empty: continue\n",
    "            # scatter markers\n",
    "            ax_tr.scatter(sub[xcol], sub[ycol], s=s, marker=marker, color=col,\n",
    "                          edgecolor=edge, linewidth=lw, zorder=5)\n",
    "            # vlines and hlines for each row\n",
    "            for _, row in sub.iterrows():\n",
    "                xv = float(row[xcol]); yv = float(row[ycol])\n",
    "                ax_br.axvline(xv, color=col, lw=2.0, alpha=0.9)\n",
    "                ax_tl.axvline(yv, color=col, lw=2.0, alpha=0.9)\n",
    "\n",
    "    if overlay_summary_df is not None and overlay_summary_conditions is not None and overlay_summary_colors is not None:\n",
    "        _overlay_summary(overlay_summary_df, overlay_summary_conditions,\n",
    "                         overlay_summary_colors, overlay_summary_marker,\n",
    "                         overlay_summary_size, overlay_summary_edge, overlay_summary_lw)\n",
    "\n",
    "    # --- legend panel ---\n",
    "    ax_bl.axis(\"off\")\n",
    "    handles, labels = ax_tr.get_legend_handles_labels()\n",
    "    if handles:\n",
    "        ax_bl.legend(handles, labels, frameon=False, loc=\"center\")\n",
    "\n",
    "    if main_title:\n",
    "        fig.suptitle(main_title, y=0.98, fontsize=12)\n",
    "\n",
    "    # CSVs (unchanged: saves the full scatter data and the *single* marginal used in full mode)\n",
    "    if csv_prefix:\n",
    "        _save_pairplot_grid_csvs(\n",
    "            csv_prefix, mode=marginal_mode, x_label=xcol, y_label=ycol,\n",
    "            x=x_all, y=y_all,\n",
    "            x_grid=None, x_kde=None, y_grid=None, y_kde=None,\n",
    "            x_hist=None, y_hist=None,\n",
    "            overlay_points=overlay_points\n",
    "        )\n",
    "\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, dpi=180)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55abb6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% FIGURE : posterior predictive simulations for selected conditions\n",
    "\n",
    "FIGURE_param_x_axis = 'common_input_std'\n",
    "FIGURE_param_y_axis = 'disynpatic_inhib_connections_desired_MN_MN'\n",
    "FIGURE_observable_feature_x_axis = 'peak_height_mean'\n",
    "FIGURE_observable_feature_y_axis = 'trough_area_mean'\n",
    "FIGURE_high_freq_CI_scale_color_list = [\"#FFE96F\",\"#FF9A3B\"] # x axis (both for parameter and observed features)\n",
    "FIGURE_RI_strength_scale_color_list = [ \"#70D7FF\",  \"#323CCF\"] # y axis (both for parameter and observed features)\n",
    "FIGURE_base_color = \"#979797\"\n",
    "FIGURE_color_blend = \"additive\"\n",
    "FIGURE_scatter_point_size = 30\n",
    "FIGURE_alpha_scatter_experimental = 0.3\n",
    "FIGURE_alpha_scatter_simulation = 0.1\n",
    "colors_dict = {\n",
    "  \"VL<->VL\": \"#D62728\",\n",
    "  \"VL<->VM\": \"#FF9201\",\n",
    "  \"VM<->VL\": \"#FF9201\",\n",
    "  \"VM<->VM\": \"#FFC400\",\n",
    "  \"TA<->TA\": \"#00C71B\",\n",
    "  \"FDI<->FDI\": \"#14BFA8\",\n",
    "  \"GM<->GM\": \"#2489DC\",\n",
    "  \"GM<->SOL\": \"#7D74EC\",\n",
    "  \"SOL<->GM\": \"#7D74EC\",\n",
    "  \"SOL<->SOL\": \"#BB86ED\",\n",
    "\n",
    "  \"VL\": \"#D62728\",\n",
    "  \"VM\": \"#FFC400\",\n",
    "  \"TA\": \"#00C71B\",\n",
    "  \"FDI\": \"#14BFA8\",\n",
    "  \"GM\": \"#2489DC\",\n",
    "  \"SOL\": \"#BB86ED\",\n",
    "}\n",
    "FIGURE_sampled_experiment_example_conditions = {\"muscle_pair\": [\"GM<->GM\",\"VM<->VM\",\"FDI<->FDI\"],\n",
    "                                                \"intensity\": [40,10,40]}\n",
    "FIGURE_sampled_experiment_example_colors = [\"#2489DC\", \"#FFC400\", \"#14BFA8\"] # Select according to muscles in FIGURE_sampled_experiment_observed_features_vector_conditions\n",
    "FIGURE_sampled_experiment_example_colors_darker = [\"#1D56BE\", \"#F08800\", \"#008B79\"]\n",
    "# FIGURE_sampled_experiment_example_conditions = {\"muscle_pair\": [\"FDI<->FDI\",\"TA<->TA\",\"SOL<->SOL\",\"GM<->GM\",\"VL<->VL\",\"VM<->VM\"],\n",
    "#                                                 # \"intensity\": [10, 10, 10, 10, 10, 10]}\n",
    "#                                                 \"intensity\": [40, 40, 40, 40, 40, 40]}\n",
    "# FIGURE_sampled_experiment_example_colors = [\"#14BFA8\", \"#00C71B\", \"#BB86ED\", \"#2489DC\", \"#D62728\", \"#FFC400\", ] # Select according to muscles in FIGURE_sampled_experiment_observed_features_vector_conditions\n",
    "# FIGURE_sampled_experiment_example_colors_darker = [\"#008B79\", \"#1E7712\", \"#9936B8\", \"#1D56BE\", \"#9C0000\", \"#F08800\", ]\n",
    "FIGURE_kde_alpha_min_max = [1, 1]\n",
    "FIG_SAVE_DIR = path_to_save_into\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% FIGURE : posterior predictive simulations for selected conditions posterior predictive sims (per-condition), optional per-sim averaging\n",
    "# ----- USER TOGGLES -----\n",
    "AGGREGATE_SIM_MEANS = True   # True: average across MUs per simulation before plotting\n",
    "UNSTANDARDIZE_FOR_PLOT = True  # True: convert standardized values back to original scales for the two axes\n",
    "# Optional custom ranges for display/binning (None = auto from data below)\n",
    "X_RANGE = None     # e.g. (0, 0.6)\n",
    "Y_RANGE = None     # e.g. (0, 2.5)\n",
    "XLIM    = None     # e.g. (-0.04, 0.64)\n",
    "YLIM    = None     # e.g. (-0.10, 2.60)\n",
    "\n",
    "# ----- Axes / inputs (same names you used before) -----\n",
    "x_feat_mean = FIGURE_observable_feature_x_axis          # e.g. 'sync_height_mean'\n",
    "y_feat_mean = FIGURE_observable_feature_y_axis          # e.g. 'inhibited_by_estimation_raw_mean'\n",
    "x_feat_raw  = x_feat_mean.replace(\"_mean\",\"\")           # e.g. 'sync_height'\n",
    "y_feat_raw  = y_feat_mean.replace(\"_mean\",\"\")           # e.g. 'inhibited_by_estimation_raw'\n",
    "\n",
    "df_ppc_all   = df_obs_simulated_from_posterior_samples.copy()\n",
    "df_hl_all    = df_obs_simulated_from_posterior_sample_highest_likelihood.copy()\n",
    "df_hl_all['sim_idx'] = 0\n",
    "\n",
    "# Sanity: needed columns\n",
    "for need_col in (x_feat_raw, y_feat_raw, \"muscle_pair\", \"intensity\"):\n",
    "    if need_col not in df_ppc_all.columns:\n",
    "        raise KeyError(f\"df_obs_simulated_from_posterior_samples missing column: {need_col}\")\n",
    "    if need_col not in df_hl_all.columns:\n",
    "        raise KeyError(f\"df_obs_simulated_from_posterior_sample_highest_likelihood missing column: {need_col}\")\n",
    "\n",
    "# Ensure 'intensity' comparable (cast to int when safe)\n",
    "def _safe_cast_int_inplace(df, col=\"intensity\"):\n",
    "    if col in df.columns and pd.api.types.is_numeric_dtype(df[col]):\n",
    "        arr = df[col].to_numpy(dtype=float, copy=False)\n",
    "        if np.all(np.isfinite(arr)) and np.all(np.isclose(arr % 1, 0, atol=1e-9)):\n",
    "            df[col] = arr.astype(int)\n",
    "\n",
    "_safe_cast_int_inplace(df_ppc_all, \"intensity\")\n",
    "_safe_cast_int_inplace(df_hl_all, \"intensity\")\n",
    "\n",
    "# ----- Unstandardize (only the two axes) using exp_stats computed from PRIOR -----\n",
    "def _unstandardize_cols(df, cols, stats):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in stats:\n",
    "            raise KeyError(f\"exp_stats missing key for '{c}'\")\n",
    "        mu = float(stats[c][\"mean\"])\n",
    "        sd = float(stats[c][\"std\"])\n",
    "        df[c] = df[c] * sd + mu\n",
    "    return df\n",
    "\n",
    "if UNSTANDARDIZE_FOR_PLOT:\n",
    "    df_ppc_all = _unstandardize_cols(df_ppc_all, [x_feat_raw, y_feat_raw], exp_stats)\n",
    "    df_hl_all  = _unstandardize_cols(df_hl_all,  [x_feat_raw, y_feat_raw], exp_stats)\n",
    "\n",
    "# ----- Optional: average across MUs per simulation (for main PPC scatter) -----\n",
    "def _aggregate_per_sim(df, value_cols, prefer_group_order=None):\n",
    "    \"\"\"\n",
    "    Average value_cols per simulation. We assemble group columns from typical keys if present.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # typical keys that define a unique \"simulation\":\n",
    "    default_order = [\"subject\", \"muscle_pair\", \"intensity\", \"condition\", \"sim_idx\"]\n",
    "    if prefer_group_order is None: prefer_group_order = default_order\n",
    "    group_cols = [c for c in prefer_group_order if c in df.columns]\n",
    "    if not group_cols:\n",
    "        # fallback: at least condition/muscle_pair/intensity if available\n",
    "        for c in [\"condition\", \"muscle_pair\", \"intensity\"]:\n",
    "            if c in df.columns and c not in group_cols:\n",
    "                group_cols.append(c)\n",
    "    # average only requested value cols\n",
    "    g = df.groupby(group_cols, as_index=False)[value_cols].mean()\n",
    "    return g\n",
    "\n",
    "if AGGREGATE_SIM_MEANS:\n",
    "    df_ppc_main = _aggregate_per_sim(df_ppc_all, [x_feat_raw, y_feat_raw])\n",
    "else:\n",
    "    df_ppc_main = df_ppc_all\n",
    "\n",
    "# For overlays (highest-likelihood): collapse to a single point per condition\n",
    "df_hl_overlay = _aggregate_per_sim(df_hl_all, [x_feat_raw, y_feat_raw])\n",
    "\n",
    "# ----- Condition filters / colors -----\n",
    "cond_filters = FIGURE_sampled_experiment_example_conditions  # e.g. {\"muscle_pair\": [...], \"intensity\":[...]}\n",
    "cond_colors  = FIGURE_sampled_experiment_example_colors\n",
    "try:\n",
    "    kde_cols = FIGURE_sampled_experiment_example_colors_darker\n",
    "except NameError:\n",
    "    # simple darken fallback\n",
    "    def _darken_hex(h, factor=0.65):\n",
    "        h = h.lstrip(\"#\")\n",
    "        r,g,b = int(h[0:2],16), int(h[2:4],16), int(h[4:6],16)\n",
    "        r = max(0, min(255, int(r*factor))); g = max(0, min(255, int(g*factor))); b = max(0, min(255, int(b*factor)))\n",
    "        return f\"#{r:02X}{g:02X}{b:02X}\"\n",
    "    kde_cols = [_darken_hex(c) for c in cond_colors]\n",
    "\n",
    "# ----- Optional per-axis gradient scales (harmless here; used in \"full_dataset\" mode) -----\n",
    "kwargs_axis_scales = {}\n",
    "try:\n",
    "    kwargs_axis_scales[\"colors_for_x_axis_colorscale\"] = FIGURE_high_freq_CI_scale_color_list\n",
    "    kwargs_axis_scales[\"colors_for_y_axis_colorscale\"] = FIGURE_RI_strength_scale_color_list\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# ----- Auto ranges if not provided -----\n",
    "def _auto_range(df, col, pad=0.02):\n",
    "    a = df[col].to_numpy(dtype=float)\n",
    "    a = a[np.isfinite(a)]\n",
    "    if a.size == 0: return (0.0, 1.0)\n",
    "    lo, hi = np.min(a), np.max(a)\n",
    "    if hi <= lo:\n",
    "        return (lo - 0.5, hi + 0.5)\n",
    "    span = hi - lo\n",
    "    return (lo - pad*span, hi + pad*span)\n",
    "\n",
    "x_range = X_RANGE if X_RANGE is not None else _auto_range(df_ppc_main, x_feat_raw, pad=0.00)\n",
    "y_range = Y_RANGE if Y_RANGE is not None else _auto_range(df_ppc_main, y_feat_raw, pad=0.00)\n",
    "xlim    = XLIM    if XLIM    is not None else _auto_range(df_ppc_main, x_feat_raw, pad=0.05)\n",
    "ylim    = YLIM    if YLIM    is not None else _auto_range(df_ppc_main, y_feat_raw, pad=0.05)\n",
    "\n",
    "# ----- Build overlay DF in the format expected by pairplot (x/y columns must match main’s xcol/ycol)\n",
    "overlay_df = df_hl_overlay.rename(columns={x_feat_raw: x_feat_raw, y_feat_raw: y_feat_raw})\n",
    "\n",
    "# ----- Plot -----\n",
    "FIG_PPC_PATH  = os.path.join(FIG_SAVE_DIR, \"_posterior_predictive_checks_features_examples.svg\")\n",
    "FIG_PPC_CSVP  = os.path.join(FIG_SAVE_DIR, \"_posterior_predictive_checks_features_examples\")\n",
    "\n",
    "pairplot_grid_marginals(\n",
    "    df_ppc_main, x_feat_raw, y_feat_raw,\n",
    "    dataset_mode=\"experimental_condition_filter\",\n",
    "    main_title=(\"FIG 6 — Posterior predictive simulations\"\n",
    "                f\" ({'per-simulation means' if AGGREGATE_SIM_MEANS else 'per-MU samples'})\"),\n",
    "    condition_filters=cond_filters,\n",
    "    condition_colors=cond_colors,\n",
    "    condition_colors_scatter_kde=kde_cols,\n",
    "\n",
    "\n",
    "    # binning/KDE ranges and display limits\n",
    "    # x_range=x_range, y_range=y_range,\n",
    "    # xlim=xlim, ylim=ylim,\n",
    "    # x_range=(0, 0.7), y_range=(0, 5),        # for binning\n",
    "    # xlim=(-0.05, 0.75), ylim=(-0.2, 5.2),    # for display\n",
    "    x_range=(0, 0.6), y_range=(0, 2.5),        # for binning\n",
    "    xlim=(-0.04, 0.64), ylim=(-0.1, 2.6),    # for display\n",
    "\n",
    "    # visuals\n",
    "    point_size=FIGURE_scatter_point_size,\n",
    "    alpha_pts=FIGURE_alpha_scatter_experimental * 1.5,\n",
    "\n",
    "    # 1D KDE marginals (per group) + per-group 2D KDE on scatter\n",
    "    marginal_mode=\"kde\",\n",
    "    n_grid=120,\n",
    "    kde_boundary=\"none\",\n",
    "    group_scatter_kde=True,\n",
    "    group_kde_levels=(0.30, 0.60, 0.90),\n",
    "    group_kde_alpha=1.0,\n",
    "    group_kde_lw=1.4,\n",
    "\n",
    "    # Overlay the highest-likelihood simulation(s) as markers + v/h lines\n",
    "    overlay_summary_df=overlay_df,\n",
    "    overlay_summary_conditions=cond_filters,       # same selections\n",
    "    overlay_summary_colors=cond_colors,\n",
    "    overlay_summary_marker=\"X\",\n",
    "    overlay_summary_size=115,\n",
    "    overlay_summary_edge=\"k\",\n",
    "    overlay_summary_lw=1.0,\n",
    "\n",
    "    savepath=FIG_PPC_PATH,\n",
    "    csv_prefix=FIG_PPC_CSVP,\n",
    "    **kwargs_axis_scales\n",
    ")\n",
    "\n",
    "print(\"Saved:\", FIG_PPC_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d5633",
   "metadata": {},
   "source": [
    "# Posterior Predictive Checks in the first 2 PCs of the full feature space (4 features x 4 summary statistics = 12d space)\n",
    "### (even though lots of colinearity within the space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b85078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ----------------------- CONFIG -----------------------\n",
    "GROUP_COLS = [\"muscle_pair\", \"intensity\"]   # columns present in all dfs\n",
    "\n",
    "# Visuals\n",
    "colors_dict = {\n",
    "  \"VL<->VL\": \"#D62728\", \"VL<->VM\": \"#FF9201\", \"VM<->VL\": \"#FF9201\", \"VM<->VM\": \"#FFC400\",\n",
    "  \"TA<->TA\": \"#00C71B\", \"FDI<->FDI\": \"#14BFA8\", \"GM<->GM\": \"#2489DC\",\n",
    "  \"GM<->SOL\": \"#7D74EC\", \"SOL<->GM\": \"#7D74EC\", \"SOL<->SOL\": \"#BB86ED\",\n",
    "  \"VL\": \"#D62728\", \"VM\": \"#FFC400\", \"TA\": \"#00C71B\", \"FDI\": \"#14BFA8\",\n",
    "  \"GM\": \"#2489DC\", \"SOL\": \"#BB86ED\",\n",
    "}\n",
    "\n",
    "# Whether to compute distances in whitened space (Mahalanobis via prior Σ)\n",
    "WHITEN_FOR_METRICS = False\n",
    "\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Configure the summary stat names once so everything downstream stays in sync\n",
    "SUMMARY_STATS = [\"mean\", \"std\", \"median\", \"iqr\"]\n",
    "FEATURE_PREFIXES = [\"trough_area\", \"peak_height\", \"firing_rate\", \"IPSP_delay\"]\n",
    "\n",
    "def pick_feature_columns(df: pd.DataFrame,\n",
    "                         prefixes=FEATURE_PREFIXES,\n",
    "                         stats=SUMMARY_STATS):\n",
    "    \"\"\"Return exactly <prefix>_<stat> columns in a consistent order.\"\"\"\n",
    "    cols = []\n",
    "    for p in prefixes:\n",
    "        for s in stats:\n",
    "            name = f\"{p}_{s}\"\n",
    "            if name not in df.columns:\n",
    "                raise KeyError(f\"Expected column '{name}' not found in DataFrame.\")\n",
    "            cols.append(name)\n",
    "    return cols\n",
    "\n",
    "# Select 16-D feature columns\n",
    "feat_cols = pick_feature_columns(df_summary_obs_simulated_from_prior,\n",
    "                                 prefixes=FEATURE_PREFIXES,\n",
    "                                 stats=SUMMARY_STATS)\n",
    "\n",
    "# Optional guard:\n",
    "expected_n = len(FEATURE_PREFIXES) * len(SUMMARY_STATS)  # 4 * 4 = 16\n",
    "assert len(feat_cols) == expected_n\n",
    "\n",
    "def make_whitener_from_prior(df_prior_feats: pd.DataFrame):\n",
    "    \"\"\"Return (mean, W) so that x_whitened = (x-mean) @ W^T has identity covariance on prior.\"\"\"\n",
    "    X = df_prior_feats.to_numpy().astype(float)\n",
    "    mu = X.mean(axis=0)\n",
    "    Xc = X - mu\n",
    "    # covariance (features x features)\n",
    "    Σ = np.cov(Xc, rowvar=False)\n",
    "    # eigen-decomp\n",
    "    evals, evecs = np.linalg.eigh(Σ)\n",
    "    # guard tiny/neg eigenvalues\n",
    "    eps = 1e-12\n",
    "    evals = np.clip(evals, eps, None)\n",
    "    W = (evecs @ np.diag(1.0/np.sqrt(evals)) @ evecs.T)  # whitening transform on columns\n",
    "    return mu, W\n",
    "\n",
    "def whiten_rows(X: np.ndarray, mu: np.ndarray, W: np.ndarray):\n",
    "    \"\"\"Apply whitening to rows of X: (X - mu) @ W^T.\"\"\"\n",
    "    return (X - mu) @ W.T\n",
    "\n",
    "def rmse(a, b): return float(np.sqrt(np.mean((a - b)**2)))\n",
    "def r2_from_vectors(pred, true, baseline_mean=None):\n",
    "    \"\"\"\n",
    "    R² over features for a *single* vector comparison:\n",
    "      R² = 1 - SSE/SST, with SST vs baseline.\n",
    "    For z-scored vs prior, baseline_mean=0 is reasonable.\n",
    "    \"\"\"\n",
    "    pred = np.asarray(pred); true = np.asarray(true)\n",
    "    sse = np.sum((pred - true)**2)\n",
    "    if baseline_mean is None:\n",
    "        baseline_mean = 0.0\n",
    "    sst = np.sum((true - baseline_mean)**2)\n",
    "    return float(1.0 - sse/max(sst, 1e-12))\n",
    "\n",
    "def silverman_bandwidth_2d(X):\n",
    "    \"\"\"Silverman's rule-of-thumb bandwidth matrix factor (scalar) for 2D.\"\"\"\n",
    "    n, d = X.shape\n",
    "    # scalar bandwidth h ~ n^{-1/(d+4)}\n",
    "    h = (n ** (-1.0/(d+4.0)))\n",
    "    return h\n",
    "\n",
    "def kde2d_grid(points, bw_scale=1.0, grid_n=150, margin=0.1):\n",
    "    \"\"\"\n",
    "    Simple 2D Gaussian KDE on a grid.\n",
    "    Returns (Xgrid, Ygrid, Z) where Z is density. Uses isotropic h * cov^(1/2) scaling.\n",
    "    \"\"\"\n",
    "    X = np.asarray(points, dtype=float)\n",
    "    n, d = X.shape\n",
    "    assert d == 2 and n >= 2\n",
    "\n",
    "    xmin, ymin = X.min(axis=0)\n",
    "    xmax, ymax = X.max(axis=0)\n",
    "    dx, dy = xmax - xmin, ymax - ymin\n",
    "    # margins\n",
    "    xmin -= margin*dx; xmax += margin*dx\n",
    "    ymin -= margin*dy; ymax += margin*dy\n",
    "\n",
    "    xs = np.linspace(xmin, xmax, grid_n)\n",
    "    ys = np.linspace(ymin, ymax, grid_n)\n",
    "    Xg, Yg = np.meshgrid(xs, ys)\n",
    "\n",
    "    # covariance & bandwidth\n",
    "    Σ = np.cov(X, rowvar=False)\n",
    "    # isotropic scalar h\n",
    "    h0 = silverman_bandwidth_2d(X) * bw_scale\n",
    "    # effective covariance for kernel\n",
    "    Σk = Σ * (h0**2)\n",
    "    # precompute inverse and norm factor\n",
    "    invΣ = np.linalg.inv(Σk)\n",
    "    norm = 1.0 / (2.0*np.pi*np.sqrt(np.linalg.det(Σk)))\n",
    "\n",
    "    # evaluate density\n",
    "    Z = np.zeros_like(Xg)\n",
    "    for k in range(n):\n",
    "        diff = np.stack([Xg - X[k,0], Yg - X[k,1]], axis=-1)  # (..., 2)\n",
    "        # quadratic form\n",
    "        q = diff @ invΣ\n",
    "        q = q[...,0]*diff[...,0] + q[...,1]*diff[...,1]\n",
    "        Z += np.exp(-0.5*q)\n",
    "    Z *= (norm / n)\n",
    "    return xs, ys, Z\n",
    "\n",
    "def hdr_levels_from_density(Z, levels=(0.9, 0.6, 0.3)):\n",
    "    \"\"\"\n",
    "    Given density grid Z, return *density* cutoffs whose superlevel sets\n",
    "    contain given probability masses (highest-density regions).\n",
    "    \"\"\"\n",
    "    z = Z.ravel()\n",
    "    order = np.argsort(z)[::-1]\n",
    "    z_sorted = z[order]\n",
    "    csum = np.cumsum(z_sorted)\n",
    "    total = csum[-1]\n",
    "    # each cell area cancels out for threshold computation (uniform grid)\n",
    "    cuts = []\n",
    "    for alpha in levels:\n",
    "        idx = np.searchsorted(csum, alpha*total)\n",
    "        idx = np.clip(idx, 0, z_sorted.size-1)\n",
    "        cuts.append(z_sorted[idx])\n",
    "    return cuts  # density thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3502d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def dropna_and_report(df: pd.DataFrame, feat_cols, name: str):\n",
    "    \"\"\"Drop rows with any NaN in feat_cols and report how many were removed.\"\"\"\n",
    "    n0 = len(df)\n",
    "    df_clean = df.dropna(subset=feat_cols)\n",
    "    n1 = len(df_clean)\n",
    "    dropped = n0 - n1\n",
    "    if dropped > 0:\n",
    "        print(f\"[{name}] Dropped {dropped} row(s) with NaNs in feature columns (kept {n1}/{n0}).\")\n",
    "    else:\n",
    "        print(f\"[{name}] No NaNs found in feature columns (kept {n1}/{n0}).\")\n",
    "    return df_clean\n",
    "\n",
    "# Clean the PRIOR only (this is the training distribution for PCA)\n",
    "df_prior_clean = dropna_and_report(df_summary_obs_simulated_from_prior, feat_cols, name=\"PRIOR\")\n",
    "\n",
    "# Fit PCA on the cleaned prior (16-D features)\n",
    "X_prior = df_prior_clean[feat_cols].to_numpy()\n",
    "pca = PCA(n_components=min(X_prior.shape[0], X_prior.shape[1]), svd_solver=\"full\")\n",
    "pca.fit(X_prior)\n",
    "\n",
    "# Helper to project any dataframe onto first two PCs (no re-fitting)\n",
    "def project_to_pcs(df, feat_cols, pca_obj, n=2):\n",
    "    X = df[feat_cols].to_numpy()\n",
    "    return pca_obj.transform(X)[:, :n]\n",
    "\n",
    "# Cumulative variance explained plot (starts at 0 PCs)\n",
    "evr = pca.explained_variance_ratio_\n",
    "cum = np.concatenate([[0.0], np.cumsum(evr)])\n",
    "xs  = np.arange(cum.size)  # 0..k\n",
    "\n",
    "plt.figure(figsize=(6, 3.5))\n",
    "plt.plot(xs, cum, lw=2)\n",
    "plt.scatter(xs, cum, s=18)\n",
    "plt.xticks(xs)\n",
    "plt.ylim(0, 1.01)\n",
    "plt.xlabel(\"# Principal Components\")\n",
    "plt.ylabel(\"Cumulative variance explained\")\n",
    "plt.title(\"PCA on prior-generated simulations (NaN rows dropped)\")\n",
    "plt.grid(alpha=0.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c436d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ===========================\n",
    "# Config\n",
    "# ===========================\n",
    "EXCLUDE_PAIRS = [\"VL<->VM\",\"GM<->SOL\"]          # e.g., [\"VL<->VL\", \"GM<->SOL\"]\n",
    "GROUP_COLS = [\"muscle_pair\", \"intensity\"]       # condition identity\n",
    "assert 'feat_cols' in globals() and isinstance(feat_cols, (list, tuple)) and len(feat_cols) > 0\n",
    "\n",
    "# ===========================\n",
    "# Helpers\n",
    "# ===========================\n",
    "def rmse(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    return float(np.sqrt(np.mean((a - b)**2)))\n",
    "\n",
    "def r2(pred, true):\n",
    "    pred = np.asarray(pred, dtype=float); true = np.asarray(true, dtype=float)\n",
    "    ss_res = np.sum((pred - true)**2)\n",
    "    ss_tot = np.sum((true - true.mean())**2)\n",
    "    return float(1.0 - ss_res/ss_tot) if ss_tot > 0 else np.nan\n",
    "\n",
    "def coerce_numeric(df, cols):\n",
    "    out = df.copy()\n",
    "    out[cols] = out[cols].apply(pd.to_numeric, errors='coerce')\n",
    "    return out\n",
    "\n",
    "def clean_df(df, feat_cols, exclude_pairs):\n",
    "    out = coerce_numeric(df, feat_cols)\n",
    "    if \"muscle_pair\" in out.columns and exclude_pairs:\n",
    "        out = out[~out[\"muscle_pair\"].isin(exclude_pairs)]\n",
    "    before = len(out)\n",
    "    out = out.dropna(subset=feat_cols)\n",
    "    dropped = before - len(out)\n",
    "    if dropped > 0:\n",
    "        print(f\"[info] Dropped {dropped} rows with NaNs in features.\")\n",
    "    return out\n",
    "\n",
    "def df_to_map_many(df, feat_cols, group_cols=GROUP_COLS):\n",
    "    m = {}\n",
    "    for k, g in df.groupby(group_cols, dropna=False):\n",
    "        m[k] = g[feat_cols].to_numpy(dtype=float)\n",
    "    return m\n",
    "\n",
    "def df_to_map_one(df, feat_cols, group_cols=GROUP_COLS):\n",
    "    m = {}\n",
    "    for k, g in df.groupby(group_cols, dropna=False):\n",
    "        if len(g) != 1:\n",
    "            print(f\"[warn] Expected 1 row for {k}, found {len(g)}. Taking the first.\")\n",
    "        m[k] = g.iloc[0][feat_cols].to_numpy(dtype=float)\n",
    "    return m\n",
    "\n",
    "def fmt(mu, sd, decimals=3):\n",
    "    return f\"{mu:.{decimals}f} ± {sd:.{decimals}f}\"\n",
    "\n",
    "def med_q1_q3(arr):\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    return (float(np.nanmedian(arr)),\n",
    "            float(np.nanpercentile(arr, 25)),\n",
    "            float(np.nanpercentile(arr, 75)))\n",
    "\n",
    "def fmt_med_iqr(med, q1, q3, decimals=3):\n",
    "    return f\"{med:.{decimals}f} [{q1:.{decimals}f}–{q3:.{decimals}f}]\"\n",
    "\n",
    "def fmt_min_max(vmin, vmax, decimals=3):\n",
    "    return f\"{vmin:.{decimals}f}, {vmax:.{decimals}f}\"\n",
    "\n",
    "def pool_concat(list_of_arrays):\n",
    "    if len(list_of_arrays) == 0:\n",
    "        return np.array([], dtype=float)\n",
    "    return np.concatenate(list_of_arrays, axis=0)\n",
    "\n",
    "# ===========================\n",
    "# Clean inputs\n",
    "# ===========================\n",
    "df_post  = clean_df(df_summary_obs_simulated_from_posterior,      feat_cols, EXCLUDE_PAIRS)\n",
    "df_best  = clean_df(df_summary_obs_simulated_highest_likelihood,  feat_cols, EXCLUDE_PAIRS)\n",
    "df_exp   = clean_df(df_summary_obs_experiment,                    feat_cols, EXCLUDE_PAIRS)\n",
    "\n",
    "# PRIOR: compare each experimental condition to ALL prior rows\n",
    "df_prior = coerce_numeric(df_summary_obs_simulated_from_prior, feat_cols).dropna(subset=feat_cols)\n",
    "prior_all = df_prior[feat_cols].to_numpy(dtype=float)   # (N_prior, F)\n",
    "\n",
    "# Build maps\n",
    "post_map  = df_to_map_many(df_post, feat_cols)   # many samples / condition\n",
    "best_map  = df_to_map_one(df_best, feat_cols)    # one / condition\n",
    "exp_map   = df_to_map_one(df_exp,  feat_cols)    # one / condition\n",
    "\n",
    "conditions = sorted(exp_map.keys())\n",
    "\n",
    "# ===========================\n",
    "# Per-condition metrics + pooled accumulators\n",
    "# ===========================\n",
    "rows = []\n",
    "missing = {'posterior': [], 'best': []}\n",
    "\n",
    "pooled_rmse_post = []   # arrays (sample-level) across conditions\n",
    "pooled_r2_post   = []\n",
    "pooled_rmse_prior = []\n",
    "pooled_r2_prior   = []\n",
    "pooled_rmse_oo = []\n",
    "pooled_r2_oo   = []\n",
    "pooled_rmse_best = []   # scalars (one per condition)\n",
    "pooled_r2_best   = []\n",
    "\n",
    "for cond in tqdm(conditions, desc=\"Summarizing per-condition\"):\n",
    "    y_true = exp_map[cond]              # (F,)\n",
    "\n",
    "    # ---- Posterior vs EXP\n",
    "    if cond in post_map:\n",
    "        Ypost = post_map[cond].astype(float, copy=False)  # (n_post, F)\n",
    "        diffs = Ypost - y_true[None, :]\n",
    "        rmse_post_samples = np.sqrt(np.mean(diffs**2, axis=1)).astype(float)\n",
    "        sst = np.sum((y_true - y_true.mean())**2)\n",
    "        if sst > 0:\n",
    "            r2_post_samples = (1.0 - np.sum(diffs**2, axis=1) / sst).astype(float)\n",
    "        else:\n",
    "            r2_post_samples = np.full(Ypost.shape[0], np.nan, dtype=float)\n",
    "\n",
    "        # μ±σ\n",
    "        rmse_post_mu, rmse_post_sd = float(np.nanmean(rmse_post_samples)), float(np.nanstd(rmse_post_samples, ddof=0))\n",
    "        r2_post_mu,   r2_post_sd   = float(np.nanmean(r2_post_samples)),   float(np.nanstd(r2_post_samples,   ddof=0))\n",
    "        # med [Q1–Q3]\n",
    "        rmse_post_med, rmse_post_q1, rmse_post_q3 = med_q1_q3(rmse_post_samples)\n",
    "        r2_post_med,   r2_post_q1,   r2_post_q3   = med_q1_q3(r2_post_samples)\n",
    "\n",
    "        pooled_rmse_post.append(rmse_post_samples)\n",
    "        pooled_r2_post.append(r2_post_samples)\n",
    "    else:\n",
    "        rmse_post_mu = rmse_post_sd = np.nan\n",
    "        r2_post_mu   = r2_post_sd   = np.nan\n",
    "        rmse_post_med = rmse_post_q1 = rmse_post_q3 = np.nan\n",
    "        r2_post_med   = r2_post_q1   = r2_post_q3   = np.nan\n",
    "        missing['posterior'].append(cond)\n",
    "\n",
    "    # ---- Best posterior (argmax logp) vs EXP\n",
    "    if cond in best_map:\n",
    "        y_best = best_map[cond].astype(float, copy=False)\n",
    "        rmse_best_val = rmse(y_best, y_true)\n",
    "        r2_best_val   = r2(y_best, y_true)\n",
    "        pooled_rmse_best.append(rmse_best_val)\n",
    "        pooled_r2_best.append(r2_best_val)\n",
    "        # represent as degenerate median/IQR\n",
    "        rmse_best_med = rmse_best_q1 = rmse_best_q3 = rmse_best_val\n",
    "        r2_best_med   = r2_best_q1   = r2_best_q3   = r2_best_val\n",
    "    else:\n",
    "        rmse_best_val = r2_best_val = np.nan\n",
    "        rmse_best_med = rmse_best_q1 = rmse_best_q3 = np.nan\n",
    "        r2_best_med   = r2_best_q1   = r2_best_q3   = np.nan\n",
    "        missing['best'].append(cond)\n",
    "\n",
    "    # ---- Prior vs EXP\n",
    "    diffs_pr = prior_all - y_true[None, :]\n",
    "    rmse_pr_samples = np.sqrt(np.mean(diffs_pr**2, axis=1)).astype(float)\n",
    "    sst = np.sum((y_true - y_true.mean())**2)\n",
    "    if sst > 0:\n",
    "        r2_pr_samples = (1.0 - np.sum(diffs_pr**2, axis=1) / sst).astype(float)\n",
    "    else:\n",
    "        r2_pr_samples = np.full(prior_all.shape[0], np.nan, dtype=float)\n",
    "\n",
    "    rmse_prior_mu, rmse_prior_sd = float(np.nanmean(rmse_pr_samples)), float(np.nanstd(rmse_pr_samples, ddof=0))\n",
    "    r2_prior_mu,   r2_prior_sd   = float(np.nanmean(r2_pr_samples)),   float(np.nanstd(r2_pr_samples,   ddof=0))\n",
    "    rmse_prior_med, rmse_prior_q1, rmse_prior_q3 = med_q1_q3(rmse_pr_samples)\n",
    "    r2_prior_med,   r2_prior_q1,   r2_prior_q3   = med_q1_q3(r2_pr_samples)\n",
    "\n",
    "    # ---- EXP vs other EXP\n",
    "    yjs = [exp_map[c] for c in conditions if c != cond]\n",
    "    if yjs:\n",
    "        yjs = np.vstack(yjs).astype(float, copy=False)\n",
    "        diffs_oo = yjs - y_true[None, :]\n",
    "        rmse_oo_samples = np.sqrt(np.mean(diffs_oo**2, axis=1)).astype(float)\n",
    "        if sst > 0:\n",
    "            r2_oo_samples = (1.0 - np.sum(diffs_oo**2, axis=1) / sst).astype(float)\n",
    "        else:\n",
    "            r2_oo_samples = np.full(len(yjs), np.nan, dtype=float)\n",
    "\n",
    "        rmse_oo_mu, rmse_oo_sd = float(np.nanmean(rmse_oo_samples)), float(np.nanstd(rmse_oo_samples, ddof=0))\n",
    "        r2_oo_mu,   r2_oo_sd   = float(np.nanmean(r2_oo_samples)),   float(np.nanstd(r2_oo_samples,   ddof=0))\n",
    "        rmse_oo_med, rmse_oo_q1, rmse_oo_q3 = med_q1_q3(rmse_oo_samples)\n",
    "        r2_oo_med,   r2_oo_q1,   r2_oo_q3   = med_q1_q3(r2_oo_samples)\n",
    "\n",
    "        pooled_rmse_oo.append(rmse_oo_samples)\n",
    "        pooled_r2_oo.append(r2_oo_samples)\n",
    "    else:\n",
    "        rmse_oo_mu = rmse_oo_sd = np.nan\n",
    "        r2_oo_mu   = r2_oo_sd   = np.nan\n",
    "        rmse_oo_med = rmse_oo_q1 = rmse_oo_q3 = np.nan\n",
    "        r2_oo_med   = r2_oo_q1   = r2_oo_q3   = np.nan\n",
    "\n",
    "    pair, inten = cond\n",
    "    rows.append({\n",
    "        \"muscle_pair\": pair,\n",
    "        \"intensity\": inten,\n",
    "\n",
    "        # Posterior (μ±σ and med[IQR])\n",
    "        \"RMSE_post (μ±σ)\": fmt(rmse_post_mu, rmse_post_sd),\n",
    "        \"RMSE_post (med [Q1–Q3])\": fmt_med_iqr(rmse_post_med, rmse_post_q1, rmse_post_q3),\n",
    "        \"R²_post (μ±σ)\"  : fmt(r2_post_mu,   r2_post_sd),\n",
    "        \"R²_post (med [Q1–Q3])\": fmt_med_iqr(r2_post_med, r2_post_q1, r2_post_q3),\n",
    "\n",
    "        # Best (single value + degenerate med[IQR])\n",
    "        \"RMSE_best\": f\"{rmse_best_val:.3f}\",\n",
    "        \"RMSE_best (med [Q1–Q3])\": fmt_med_iqr(rmse_best_med, rmse_best_q1, rmse_best_q3),\n",
    "        \"R²_best\":   f\"{r2_best_val:.3f}\",\n",
    "        \"R²_best (med [Q1–Q3])\": fmt_med_iqr(r2_best_med, r2_best_q1, r2_best_q3),\n",
    "\n",
    "        # Prior (μ±σ and med[IQR])\n",
    "        \"RMSE_prior (μ±σ)\": fmt(rmse_prior_mu, rmse_prior_sd),\n",
    "        \"RMSE_prior (med [Q1–Q3])\": fmt_med_iqr(rmse_prior_med, rmse_prior_q1, rmse_prior_q3),\n",
    "        \"R²_prior (μ±σ)\"  : fmt(r2_prior_mu,   r2_prior_sd),\n",
    "        \"R²_prior (med [Q1–Q3])\": fmt_med_iqr(r2_prior_med, r2_prior_q1, r2_prior_q3),\n",
    "\n",
    "        # EXP vs others (μ±σ and med[IQR])\n",
    "        \"RMSE_exp-others (μ±σ)\": fmt(rmse_oo_mu, rmse_oo_sd),\n",
    "        \"RMSE_exp-others (med [Q1–Q3])\": fmt_med_iqr(rmse_oo_med, rmse_oo_q1, rmse_oo_q3),\n",
    "        \"R²_exp-others (μ±σ)\"  : fmt(r2_oo_mu,   r2_oo_sd),\n",
    "        \"R²_exp-others (med [Q1–Q3])\": fmt_med_iqr(r2_oo_med, r2_oo_q1, r2_oo_q3),\n",
    "\n",
    "        # keep raw numbers for your \"AVERAGE\" row\n",
    "        \"_rmse_post_mu\": rmse_post_mu, \"_rmse_post_sd\": rmse_post_sd,\n",
    "        \"_r2_post_mu\":   r2_post_mu,   \"_r2_post_sd\":   r2_post_sd,\n",
    "        \"_rmse_best\":    rmse_best_val, \"_r2_best\":     r2_best_val,\n",
    "        \"_rmse_prior_mu\": rmse_prior_mu, \"_rmse_prior_sd\": rmse_prior_sd,\n",
    "        \"_r2_prior_mu\":   r2_prior_mu,   \"_r2_prior_sd\":   r2_prior_sd,\n",
    "        \"_rmse_oo_mu\": rmse_oo_mu, \"_rmse_oo_sd\": rmse_oo_sd,\n",
    "        \"_r2_oo_mu\":   r2_oo_mu,   \"_r2_oo_sd\":   r2_oo_sd,\n",
    "    })\n",
    "\n",
    "summary_table = pd.DataFrame(rows).sort_values(by=[\"muscle_pair\", \"intensity\"]).reset_index(drop=True)\n",
    "\n",
    "# ===========================\n",
    "# POOLED rows\n",
    "# ===========================\n",
    "pool_post_rmse  = pool_concat(pooled_rmse_post)\n",
    "pool_post_r2    = pool_concat(pooled_r2_post)\n",
    "pool_prior_rmse = pool_concat(pooled_rmse_prior)\n",
    "pool_prior_r2   = pool_concat(pooled_r2_prior)\n",
    "pool_oo_rmse    = pool_concat(pooled_rmse_oo)\n",
    "pool_oo_r2      = pool_concat(pooled_r2_oo)\n",
    "pool_best_rmse  = np.asarray(pooled_rmse_best, dtype=float) if pooled_rmse_best else np.array([], dtype=float)\n",
    "pool_best_r2    = np.asarray(pooled_r2_best,   dtype=float) if pooled_r2_best   else np.array([], dtype=float)\n",
    "\n",
    "# pooled means ± sd\n",
    "row_pooled = {\n",
    "    \"muscle_pair\": \"— POOLED (samples) —\",\n",
    "    \"intensity\": \"\",\n",
    "    \"RMSE_post (μ±σ)\": fmt(np.nanmean(pool_post_rmse),  np.nanstd(pool_post_rmse,  ddof=0)) if pool_post_rmse.size  else \"nan ± nan\",\n",
    "    \"R²_post (μ±σ)\"  : fmt(np.nanmean(pool_post_r2),    np.nanstd(pool_post_r2,    ddof=0)) if pool_post_r2.size    else \"nan ± nan\",\n",
    "    \"RMSE_best\"      : fmt(np.nanmean(pool_best_rmse),  np.nanstd(pool_best_rmse,  ddof=0)) if pool_best_rmse.size  else \"nan ± nan\",\n",
    "    \"R²_best\"        : fmt(np.nanmean(pool_best_r2),    np.nanstd(pool_best_r2,    ddof=0)) if pool_best_r2.size    else \"nan ± nan\",\n",
    "    \"RMSE_prior (μ±σ)\": fmt(np.nanmean(pool_prior_rmse), np.nanstd(pool_prior_rmse, ddof=0)) if pool_prior_rmse.size else \"nan ± nan\",\n",
    "    \"R²_prior (μ±σ)\"  : fmt(np.nanmean(pool_prior_r2),   np.nanstd(pool_prior_r2,   ddof=0)) if pool_prior_r2.size   else \"nan ± nan\",\n",
    "    \"RMSE_exp-others (μ±σ)\": fmt(np.nanmean(pool_oo_rmse), np.nanstd(pool_oo_rmse, ddof=0)) if pool_oo_rmse.size else \"nan ± nan\",\n",
    "    \"R²_exp-others (μ±σ)\"  : fmt(np.nanmean(pool_oo_r2),   np.nanstd(pool_oo_r2,   ddof=0)) if pool_oo_r2.size   else \"nan ± nan\",\n",
    "}\n",
    "\n",
    "# pooled med[IQR]\n",
    "row_pooled_median = {\n",
    "    \"muscle_pair\": \"— POOLED (med [Q1–Q3]) —\",\n",
    "    \"intensity\": \"\",\n",
    "    \"RMSE_post (med [Q1–Q3])\": fmt_med_iqr(*med_q1_q3(pool_post_rmse))  if pool_post_rmse.size  else \"nan [nan–nan]\",\n",
    "    \"R²_post (med [Q1–Q3])\"  : fmt_med_iqr(*med_q1_q3(pool_post_r2))    if pool_post_r2.size    else \"nan [nan–nan]\",\n",
    "    \"RMSE_best (med [Q1–Q3])\": fmt_med_iqr(*med_q1_q3(pool_best_rmse))  if pool_best_rmse.size  else \"nan [nan–nan]\",\n",
    "    \"R²_best (med [Q1–Q3])\"  : fmt_med_iqr(*med_q1_q3(pool_best_r2))    if pool_best_r2.size    else \"nan [nan–nan]\",\n",
    "    \"RMSE_prior (med [Q1–Q3])\": fmt_med_iqr(*med_q1_q3(pool_prior_rmse)) if pool_prior_rmse.size else \"nan [nan–nan]\",\n",
    "    \"R²_prior (med [Q1–Q3])\"  : fmt_med_iqr(*med_q1_q3(pool_prior_r2))   if pool_prior_r2.size   else \"nan [nan–nan]\",\n",
    "    \"RMSE_exp-others (med [Q1–Q3])\": fmt_med_iqr(*med_q1_q3(pool_oo_rmse)) if pool_oo_rmse.size else \"nan [nan–nan]\",\n",
    "    \"R²_exp-others (med [Q1–Q3])\"  : fmt_med_iqr(*med_q1_q3(pool_oo_r2))   if pool_oo_r2.size   else \"nan [nan–nan]\",\n",
    "}\n",
    "\n",
    "# pooled min/max (requested to appear below pooled means ± sd)\n",
    "row_pooled_minmax = {\n",
    "    \"muscle_pair\": \"— POOLED (min/max) —\",\n",
    "    \"intensity\": \"\",\n",
    "    \"RMSE_post (min,max)\": fmt_min_max(np.nanmin(pool_post_rmse),  np.nanmax(pool_post_rmse))   if pool_post_rmse.size  else \"nan, nan\",\n",
    "    \"R²_post (min,max)\"  : fmt_min_max(np.nanmin(pool_post_r2),    np.nanmax(pool_post_r2))     if pool_post_r2.size    else \"nan, nan\",\n",
    "    \"RMSE_best (min,max)\": fmt_min_max(np.nanmin(pool_best_rmse),  np.nanmax(pool_best_rmse))   if pool_best_rmse.size  else \"nan, nan\",\n",
    "    \"R²_best (min,max)\"  : fmt_min_max(np.nanmin(pool_best_r2),    np.nanmax(pool_best_r2))     if pool_best_r2.size    else \"nan, nan\",\n",
    "    \"RMSE_prior (min,max)\": fmt_min_max(np.nanmin(pool_prior_rmse), np.nanmax(pool_prior_rmse)) if pool_prior_rmse.size else \"nan, nan\",\n",
    "    \"R²_prior (min,max)\"  : fmt_min_max(np.nanmin(pool_prior_r2),   np.nanmax(pool_prior_r2))   if pool_prior_r2.size   else \"nan, nan\",\n",
    "    \"RMSE_exp-others (min,max)\": fmt_min_max(np.nanmin(pool_oo_rmse), np.nanmax(pool_oo_rmse)) if pool_oo_rmse.size else \"nan, nan\",\n",
    "    \"R²_exp-others (min,max)\"  : fmt_min_max(np.nanmin(pool_oo_r2),   np.nanmax(pool_oo_r2))   if pool_oo_r2.size   else \"nan, nan\",\n",
    "}\n",
    "\n",
    "summary_table = pd.concat(\n",
    "    [summary_table,\n",
    "     pd.DataFrame([row_pooled]),\n",
    "     pd.DataFrame([row_pooled_median]),\n",
    "     pd.DataFrame([row_pooled_minmax])],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# (Optional) keep your “AVERAGE” row (avg of per-condition means/SDs)\n",
    "# ===========================\n",
    "post_rmse_mu_avg = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_rmse_post_mu\"]))\n",
    "post_rmse_sd_avg = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_rmse_post_sd\"]))\n",
    "post_r2_mu_avg   = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_r2_post_mu\"]))\n",
    "post_r2_sd_avg   = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_r2_post_sd\"]))\n",
    "\n",
    "best_rmse_mu = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_rmse_best\"]))\n",
    "best_rmse_sd = float(np.nanstd(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_rmse_best\"], ddof=0))\n",
    "best_r2_mu   = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_r2_best\"]))\n",
    "best_r2_sd   = float(np.nanstd(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_r2_best\"], ddof=0))\n",
    "\n",
    "prior_rmse_mu_avg = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_rmse_prior_mu\"]))\n",
    "prior_rmse_sd_avg = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_rmse_prior_sd\"]))\n",
    "prior_r2_mu_avg   = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_r2_prior_mu\"]))\n",
    "prior_r2_sd_avg   = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_r2_prior_sd\"]))\n",
    "\n",
    "oo_rmse_mu_avg = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_rmse_oo_mu\"]))\n",
    "oo_rmse_sd_avg = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_rmse_oo_sd\"]))\n",
    "oo_r2_mu_avg   = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_r2_oo_mu\"]))\n",
    "oo_r2_sd_avg   = float(np.nanmean(summary_table.loc[summary_table[\"muscle_pair\"].str.startswith(\"—\") == False, \"_r2_oo_sd\"]))\n",
    "\n",
    "bottom_avg = {\n",
    "    \"muscle_pair\": \"— AVERAGE (per-condition means) —\",\n",
    "    \"intensity\": \"\",\n",
    "    \"RMSE_post (μ±σ)\": fmt(post_rmse_mu_avg, post_rmse_sd_avg),\n",
    "    \"R²_post (μ±σ)\"  : fmt(post_r2_mu_avg,   post_r2_sd_avg),\n",
    "    \"RMSE_best\"      : fmt(best_rmse_mu, best_rmse_sd),\n",
    "    \"R²_best\"        : fmt(best_r2_mu,   best_r2_sd),\n",
    "    \"RMSE_prior (μ±σ)\": fmt(prior_rmse_mu_avg, prior_rmse_sd_avg),\n",
    "    \"R²_prior (μ±σ)\"  : fmt(prior_r2_mu_avg,   prior_r2_sd_avg),\n",
    "    \"RMSE_exp-others (μ±σ)\": fmt(oo_rmse_mu_avg, oo_rmse_sd_avg),\n",
    "    \"R²_exp-others (μ±σ)\"  : fmt(oo_r2_mu_avg,  oo_r2_sd_avg),\n",
    "}\n",
    "\n",
    "summary_table = pd.concat([summary_table, pd.DataFrame([bottom_avg])], ignore_index=True)\n",
    "\n",
    "# Drop internal raw columns\n",
    "summary_table = summary_table[[c for c in summary_table.columns if not c.startswith(\"_\")]]\n",
    "\n",
    "summary_table.to_csv(f\"{path_to_save_into}\\\\summary_table_rmse_r2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table # check previous cell's result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108fd2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Config\n",
    "# -------------------------------------------------\n",
    "# EXCLUDE_PAIRS = []  # e.g. [\"VL<->VL\", \"GM<->SOL\"] # Already defined above\n",
    "INTENSITIES   = sorted(df_summary_obs_experiment[\"intensity\"].unique().tolist())\n",
    "\n",
    "# Appearance\n",
    "KDE_GRID_RES     = 200     # 2D KDE grid resolution\n",
    "PAD_FRAC         = 0.1    # padding around global x/y range\n",
    "LINE_ALPHA_MARG  = 0.85    # opacity for 1D marginal lines\n",
    "LINE_WIDTH_MARG  = 1.6\n",
    "CONTOUR_FILL_A   = 0.08    # area alpha at 90% mass\n",
    "MARKER_EXP_SIZE  = 80\n",
    "MARKER_HL_SIZE   = 58\n",
    "KDE_BW_SCALE       = 0.5   # >1 = smoother 2D contours, <1 = sharper\n",
    "MARGINAL_BW_SCALE  = 0.5   # >1 = smoother 1D curves, <1 = sharper\n",
    "\n",
    "# --- Config ---\n",
    "CONTOUR_PROBS   = (0.6, 0.3) # (0.90, 0.60, 0.30)   # e.g. (0.5,) or (0.8, 0.5, 0.2)\n",
    "CONTOUR_LINE_WS = (0.3, 0.6) # (0.8, 0.55, 0.35)    # alphas for the lines; will be broadcast to len(CONTOUR_PROBS)\n",
    "\n",
    "def _broadcast(vals, n):\n",
    "    # make len(vals) == n (repeat last if shorter; trim if longer; scalars allowed)\n",
    "    try:\n",
    "        v = list(vals)\n",
    "    except TypeError:\n",
    "        v = [vals]\n",
    "    if len(v) < n:\n",
    "        v = v + [v[-1]] * (n - len(v))\n",
    "    return v[:n]\n",
    "\n",
    "def kde_levels_for_mass(Z, dx, dy, probs):\n",
    "    \"\"\"\n",
    "    Return density thresholds t for which the highest-density region {Z >= t}\n",
    "    captures a probability mass 'p' for each p in probs.\n",
    "    \"\"\"\n",
    "    probs = np.atleast_1d(np.clip(probs, 0.0, 1.0))\n",
    "    z = Z.ravel()\n",
    "    order = np.argsort(z)[::-1]            # densities high -> low\n",
    "    z_sorted = z[order]\n",
    "    mass = np.cumsum(z_sorted) * dx * dy   # cumulative probability mass\n",
    "    levels = []\n",
    "    for p in probs:\n",
    "        k = np.searchsorted(mass, p, side=\"left\")\n",
    "        levels.append(z_sorted[min(k, z_sorted.size - 1)])\n",
    "    return np.array(levels, dtype=float)\n",
    "\n",
    "\n",
    "# Colors for muscle pairs\n",
    "colors_dict = {\n",
    "  \"VL<->VL\": \"#D62728\",\n",
    "  \"VL<->VM\": \"#FF9201\",\n",
    "  \"VM<->VL\": \"#FF9201\",\n",
    "  \"VM<->VM\": \"#FFC400\",\n",
    "  \"TA<->TA\": \"#00C71B\",\n",
    "  \"FDI<->FDI\": \"#14BFA8\",\n",
    "  \"GM<->GM\": \"#2489DC\",\n",
    "  \"GM<->SOL\": \"#7D74EC\",\n",
    "  \"SOL<->GM\": \"#7D74EC\",\n",
    "  \"SOL<->SOL\": \"#BB86ED\",\n",
    "}\n",
    "\n",
    "# --- Metric helpers (full 16-D space; features are z-scored to prior) ---\n",
    "def rmse(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    return float(np.sqrt(np.mean((a - b)**2)))\n",
    "\n",
    "def r2(pred, true):\n",
    "    pred = np.asarray(pred, dtype=float); true = np.asarray(true, dtype=float)\n",
    "    ss_res = np.sum((pred - true)**2)\n",
    "    ss_tot = np.sum((true - true.mean())**2)\n",
    "    return float(1.0 - ss_res/ss_tot) if ss_tot > 0 else np.nan\n",
    "\n",
    "# --- Build per-intensity dicts: dict[muscle_pair] -> np.ndarray[n_rows, 16] ---\n",
    "def dict_by_pair(df, intensity, exclude_pairs=None):\n",
    "    dfi = df[df[\"intensity\"] == intensity]\n",
    "    dfi = dfi.dropna(subset=feat_cols)  # ensure numeric\n",
    "    out = {}\n",
    "    for pair, block in dfi.groupby(\"muscle_pair\"):\n",
    "        if exclude_pairs and pair in exclude_pairs:\n",
    "            continue\n",
    "        out[pair] = block[feat_cols].to_numpy(dtype=float)\n",
    "    return out\n",
    "\n",
    "# --- 2D KDE helpers with probability-mass contours ---\n",
    "def kde2d_on_grid(points_2d, xgrid, ygrid):\n",
    "    \"\"\"Fit KDE on Nx2 points, evaluate on provided uniform (xgrid,ygrid).\"\"\"\n",
    "    pts = np.asarray(points_2d, dtype=float)\n",
    "    kde = gaussian_kde(pts.T, bw_method=KDE_BW_SCALE)  # Scott's rule\n",
    "    Xg, Yg = np.meshgrid(xgrid, ygrid)\n",
    "    Z = kde(np.vstack([Xg.ravel(), Yg.ravel()])).reshape(len(ygrid), len(xgrid))\n",
    "    # grid spacings\n",
    "    dx = (xgrid[-1] - xgrid[0]) / (len(xgrid) - 1)\n",
    "    dy = (ygrid[-1] - ygrid[0]) / (len(ygrid) - 1)\n",
    "    return Xg, Yg, Z, dx, dy\n",
    "\n",
    "def kde_levels_for_mass(Z, dx, dy, probs=(0.90, 0.60, 0.30)):\n",
    "    \"\"\"Density thresholds for superlevel sets capturing given masses.\"\"\"\n",
    "    flat = Z.ravel()\n",
    "    order = np.argsort(flat)[::-1]\n",
    "    z_sorted = flat[order]\n",
    "    mass = np.cumsum(z_sorted) * dx * dy\n",
    "    levels = []\n",
    "    for p in probs:\n",
    "        idx = np.searchsorted(mass, p, side=\"left\")\n",
    "        idx = min(max(idx, 0), z_sorted.size - 1)\n",
    "        levels.append(z_sorted[idx])\n",
    "    return levels  # same order as probs\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Plot per intensity with marginals\n",
    "# -------------------------------------------------\n",
    "for inten in INTENSITIES:\n",
    "    # Slice dicts (drop excluded pairs)\n",
    "    post_by_pair = dict_by_pair(df_summary_obs_simulated_from_posterior, inten, EXCLUDE_PAIRS)\n",
    "    exp_by_pair  = dict_by_pair(df_summary_obs_experiment,                   inten, EXCLUDE_PAIRS)\n",
    "    hl_by_pair   = dict_by_pair(df_summary_obs_simulated_highest_likelihood, inten, EXCLUDE_PAIRS)\n",
    "\n",
    "    present_pairs = set(post_by_pair.keys()) | set(exp_by_pair.keys()) | set(hl_by_pair.keys())\n",
    "    if not present_pairs:\n",
    "        print(f\"[info] intensity={inten}: nothing to plot after exclusions.\")\n",
    "        continue\n",
    "\n",
    "    # ---------- Project to 2D (PC1,PC2) and gather global extents ----------\n",
    "    proj_post = {}\n",
    "    proj_exp  = {}\n",
    "    proj_hl   = {}\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for pair in present_pairs:\n",
    "        if pair in post_by_pair:\n",
    "            Z_post = pca.transform(post_by_pair[pair])[:, :2]\n",
    "            proj_post[pair] = Z_post\n",
    "            xs.append(Z_post[:, 0]); ys.append(Z_post[:, 1])\n",
    "        if pair in exp_by_pair and exp_by_pair[pair].shape[0] >= 1:\n",
    "            z_exp = pca.transform(exp_by_pair[pair])[:, :2].reshape(-1, 2)[0]\n",
    "            proj_exp[pair] = z_exp\n",
    "            xs.append([z_exp[0]]); ys.append([z_exp[1]])\n",
    "        if pair in hl_by_pair and hl_by_pair[pair].shape[0] >= 1:\n",
    "            z_hl = pca.transform(hl_by_pair[pair])[:, :2].reshape(-1, 2)[0]\n",
    "            proj_hl[pair] = z_hl\n",
    "            xs.append([z_hl[0]]); ys.append([z_hl[1]])\n",
    "\n",
    "    # Global bounds + padding shared by main & marginals\n",
    "    if xs:\n",
    "        xmin, xmax = np.min(np.concatenate(xs)), np.max(np.concatenate(xs))\n",
    "        ymin, ymax = np.min(np.concatenate(ys)), np.max(np.concatenate(ys))\n",
    "    else:\n",
    "        xmin = xmax = ymin = ymax = 0.0\n",
    "    dx = xmax - xmin; dy = ymax - ymin\n",
    "    if dx <= 0: dx = 1.0\n",
    "    if dy <= 0: dy = 1.0\n",
    "    xmin -= PAD_FRAC * dx; xmax += PAD_FRAC * dx\n",
    "    ymin -= PAD_FRAC * dy; ymax += PAD_FRAC * dy\n",
    "\n",
    "    # Shared grids\n",
    "    xgrid = np.linspace(xmin, xmax, KDE_GRID_RES)\n",
    "    ygrid = np.linspace(ymin, ymax, KDE_GRID_RES)\n",
    "\n",
    "    # ---------- Figure with GridSpec: left marginal | main ; bottom marginal under main ----------\n",
    "    fig = plt.figure(figsize=(8.6, 7.1))\n",
    "    gs  = GridSpec(nrows=2, ncols=2, width_ratios=[1.4, 6.6], height_ratios=[6.6, 1.4],\n",
    "                   wspace=0.05, hspace=0.05)\n",
    "\n",
    "    ax_left   = fig.add_subplot(gs[0, 0])              # y-marginal (density vs y), horizontal lines\n",
    "    ax_main   = fig.add_subplot(gs[0, 1])              # main 2-D KDE + points\n",
    "    ax_bottom = fig.add_subplot(gs[1, 1], sharex=ax_main)  # x-marginal (density vs x)\n",
    "\n",
    "    # empty bottom-left cell\n",
    "    ax_blank = fig.add_subplot(gs[1, 0]); ax_blank.axis('off')\n",
    "\n",
    "    # ---------- Plot 2D KDEs and points on main ----------\n",
    "    rmse_means, rmse_sds, r2_means, r2_sds = [], [], [], []\n",
    "    rmse_hl, r2_hl = [], []\n",
    "\n",
    "    for pair in tqdm(sorted(present_pairs), desc=f\"Intensity {inten}: 2D KDEs\", leave=False):\n",
    "        col = colors_dict.get(pair, \"#444444\")\n",
    "\n",
    "        if pair in proj_post and proj_post[pair].shape[0] >= 5:\n",
    "            Z_post = proj_post[pair]\n",
    "            Xg, Yg, Zden, dxg, dyg = kde2d_on_grid(Z_post, xgrid, ygrid)\n",
    "\n",
    "            levels = kde_levels_for_mass(Zden, dxg, dyg, probs=CONTOUR_PROBS)\n",
    "            alphas = _broadcast(CONTOUR_LINE_WS, len(levels))\n",
    "\n",
    "            # Fill using the FIRST prob in CONTOUR_PROBS (e.g., 0.90 if you keep the default)\n",
    "            if len(levels):\n",
    "                ax_main.contourf(\n",
    "                    Xg, Yg, Zden,\n",
    "                    levels=[levels[0], Zden.max()],\n",
    "                    colors=[col],\n",
    "                    alpha=CONTOUR_FILL_A\n",
    "                )\n",
    "\n",
    "        # Draw one contour line per requested probability (same order as CONTOUR_PROBS)\n",
    "        for lvl, a in zip(levels, alphas):\n",
    "            ax_main.contour(Xg, Yg, Zden, levels=[lvl], colors=[col], linewidths=1.1, alpha=a)\n",
    "\n",
    "\n",
    "        # Experimental and highest-likelihood points\n",
    "        if pair in proj_exp:\n",
    "            xe, ye = proj_exp[pair]\n",
    "            ax_main.scatter(xe, ye, marker='X', s=MARKER_EXP_SIZE,\n",
    "                            facecolor=col, edgecolor='black', linewidths=0.9)\n",
    "        if pair in proj_hl:\n",
    "            xh, yh = proj_hl[pair]\n",
    "            ax_main.scatter(xh, yh, marker='o', s=MARKER_HL_SIZE,\n",
    "                            facecolor=col, edgecolor='black', linewidths=0.9)\n",
    "\n",
    "        # 16-D metrics vs EXP (posterior cloud)\n",
    "        if pair in post_by_pair and pair in exp_by_pair and exp_by_pair[pair].shape[0] >= 1:\n",
    "            X_post = post_by_pair[pair]\n",
    "            exp_vec = exp_by_pair[pair][0, :].astype(float, copy=False)\n",
    "            errs = np.sqrt(np.mean((X_post - exp_vec[None, :])**2, axis=1))\n",
    "            r2s  = [r2(X_post[i, :], exp_vec) for i in range(X_post.shape[0])]\n",
    "            rmse_means.append(float(np.mean(errs)))\n",
    "            rmse_sds.append(float(np.std(errs, ddof=0)))\n",
    "            r2_means.append(float(np.mean(r2s)))\n",
    "            r2_sds.append(float(np.std(r2s, ddof=0)))\n",
    "\n",
    "        # 16-D metrics vs EXP (highest-likelihood)\n",
    "        if (pair in hl_by_pair and pair in exp_by_pair and\n",
    "            hl_by_pair[pair].shape[0] >= 1 and exp_by_pair[pair].shape[0] >= 1):\n",
    "            hl_vec  = hl_by_pair[pair][0, :].astype(float, copy=False)\n",
    "            exp_vec = exp_by_pair[pair][0, :].astype(float, copy=False)\n",
    "            rmse_hl.append(rmse(hl_vec, exp_vec))\n",
    "            r2_hl.append(r2(hl_vec, exp_vec))\n",
    "\n",
    "    # Main axes cosmetics\n",
    "    ax_main.set_xlim(xmin, xmax)\n",
    "    ax_main.set_ylim(ymin, ymax)\n",
    "    ax_main.set_xlabel(\"PC1\")\n",
    "    ax_main.set_ylabel(\"PC2\")\n",
    "    ax_main.grid(alpha=0.25)\n",
    "\n",
    "    # ---------- Marginal 1-D KDEs ----------\n",
    "    # Bottom (PC1): density vs x\n",
    "    bottom_max = 0.0\n",
    "    for pair in sorted(present_pairs):\n",
    "        if pair not in proj_post or proj_post[pair].shape[0] < 3:\n",
    "            continue\n",
    "        col = colors_dict.get(pair, \"#444444\")\n",
    "        x_vals = proj_post[pair][:, 0]\n",
    "        kde_x  = gaussian_kde(x_vals, bw_method=MARGINAL_BW_SCALE)\n",
    "        y_dens = kde_x(xgrid)\n",
    "        bottom_max = max(bottom_max, float(np.max(y_dens)))\n",
    "        ax_bottom.plot(xgrid, y_dens, color=col, alpha=LINE_ALPHA_MARG, lw=LINE_WIDTH_MARG)\n",
    "        # Experimental vertical line\n",
    "        if pair in proj_exp:\n",
    "            xe = proj_exp[pair][0]\n",
    "            ax_bottom.axvline(xe, color=col, alpha=0.9, lw=1.6)\n",
    "\n",
    "    ax_bottom.set_xlim(xmin, xmax)\n",
    "    ax_bottom.set_ylim(0, bottom_max * 1.05 if bottom_max > 0 else 1.0)\n",
    "    ax_bottom.set_yticks([])\n",
    "    ax_bottom.grid(alpha=0.15)\n",
    "\n",
    "    # Left (PC2): density vs y (plotted horizontally)\n",
    "    left_max = 0.0\n",
    "    for pair in sorted(present_pairs):\n",
    "        if pair not in proj_post or proj_post[pair].shape[0] < 3:\n",
    "            continue\n",
    "        col = colors_dict.get(pair, \"#444444\")\n",
    "        y_vals = proj_post[pair][:, 1]\n",
    "        kde_y  = gaussian_kde(y_vals, bw_method=MARGINAL_BW_SCALE)\n",
    "        x_dens = kde_y(ygrid)\n",
    "        left_max = max(left_max, float(np.max(x_dens)))\n",
    "        ax_left.plot(x_dens, ygrid, color=col, alpha=LINE_ALPHA_MARG, lw=LINE_WIDTH_MARG)\n",
    "        # Experimental horizontal line\n",
    "        if pair in proj_exp:\n",
    "            ye = proj_exp[pair][1]\n",
    "            ax_left.axhline(ye, color=col, alpha=0.9, lw=1.6)\n",
    "\n",
    "    ax_left.set_ylim(ymin, ymax)\n",
    "    ax_left.set_xlim(0, left_max * 1.05 if left_max > 0 else 1.0)\n",
    "    ax_left.set_xticks([])\n",
    "    ax_left.invert_xaxis()  # densities grow into the left margin\n",
    "    ax_left.grid(alpha=0.15)\n",
    "\n",
    "    # ---------- Title + legend ----------\n",
    "    def μσ(a):\n",
    "        return (np.mean(a), np.std(a, ddof=0)) if len(a) else (np.nan, np.nan)\n",
    "\n",
    "    rmseμ, rmseσ = μσ(rmse_means)\n",
    "    r2μ,   r2σ   = μσ(r2_means)\n",
    "    rmse_hlμ, rmse_hlσ = μσ(rmse_hl)\n",
    "    r2_hlμ,   r2_hlσ   = μσ(r2_hl)\n",
    "\n",
    "    ax_main.set_title(\n",
    "        f\"Posterior predictive (intensity={inten})  |  \"\n",
    "        f\"RMSE_post: {rmseμ:.2f}±{rmseσ:.2f}   R²_post: {r2μ:.2f}±{r2σ:.2f}   |   \"\n",
    "        f\"RMSE_HL: {rmse_hlμ:.2f}   R²_HL: {r2_hlμ:.2f}\"\n",
    "    )\n",
    "\n",
    "    handles = [\n",
    "        Line2D([0], [0], marker='X', color='black', lw=0, markerfacecolor='white',\n",
    "               label='Experiment (filled cross)', markersize=8),\n",
    "        Line2D([0], [0], marker='o', color='black', lw=0, markerfacecolor='white',\n",
    "               label='Highest-likelihood (circle)', markersize=7),\n",
    "    ]\n",
    "    for pair in sorted(present_pairs):\n",
    "        handles.append(Line2D([0], [0], color=colors_dict.get(pair, \"#444444\"),\n",
    "                              lw=6, alpha=0.9, label=pair))\n",
    "    ax_main.legend(handles=handles, bbox_to_anchor=(1.02, 1.0), loc='upper left',\n",
    "                   frameon=False, fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapping_RI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
